{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "def load_data(file_path):\n",
    "    table = pq.read_table(file_path)\n",
    "    df = table.to_pandas()\n",
    "    return df\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def gauc(labels, preds, uids):\n",
    "    \"\"\"Calculate group auc\n",
    "    :param labels: list\n",
    "    :param predict: list\n",
    "    :param uids: list\n",
    "    >>> gauc([1,1,0,0,1], [0, 0,1,0,1], ['a', 'a','a', 'b', 'b'])\n",
    "    0.4\n",
    "    >>> gauc([1,1,0,0,1], [1,1,0,0,1], ['a', 'a','a', 'b', 'b'])\n",
    "    1.0\n",
    "    >>> gauc([1,1,1,0,0], [1,1,0,0,1], ['a', 'a','a', 'b', 'b'])\n",
    "    0.0\n",
    "    >>> gauc([1,1,1,0,1], [1,1,0,0,1], ['a', 'a','a', 'b', 'b'])\n",
    "    1.0\n",
    "    \"\"\"\n",
    "    assert len(uids) == len(labels)\n",
    "    assert len(uids) == len(preds)\n",
    "    group_score = defaultdict(lambda: [])\n",
    "    group_truth = defaultdict(lambda: [])\n",
    "    for idx, truth in enumerate(labels):\n",
    "        uid = uids[idx]\n",
    "        group_score[uid].append(preds[idx])\n",
    "        group_truth[uid].append(truth)\n",
    "\n",
    "    total_auc = 0\n",
    "    impression_total = 0\n",
    "    for user_id in group_truth:\n",
    "        if label_with_xor(group_truth[user_id]):\n",
    "            auc = roc_auc_score(np.asarray(\n",
    "                group_truth[user_id]), np.asarray(group_score[user_id]))\n",
    "            total_auc += auc * len(group_truth[user_id])\n",
    "            impression_total += len(group_truth[user_id])\n",
    "    group_auc = (float(total_auc) /\n",
    "                 impression_total) if impression_total else 0\n",
    "    group_auc = round(group_auc, 6)\n",
    "    return group_auc\n",
    "\n",
    "\n",
    "def label_with_xor(lists):\n",
    "    \"\"\"\n",
    "    >>> label_with_xor([1,1,1])\n",
    "    False\n",
    "    >>> label_with_xor([0,0,0])\n",
    "    False\n",
    "    >>> label_with_xor([0,])\n",
    "    False\n",
    "    >>> label_with_xor([1,])\n",
    "    False\n",
    "    >>> label_with_xor([0,1])\n",
    "    True\n",
    "    \"\"\"\n",
    "    if not lists:\n",
    "        return False\n",
    "    first = lists[0]\n",
    "    for i in range(1, len(lists)):\n",
    "        if lists[i] != first:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def auc(y_true, y_score):\n",
    "    '''\n",
    "    :param y_true: shape = [n_samples] or [n_samples, n_classes]\n",
    "    :param y_score: shape = [n_samples] or [n_samples, n_classes]\n",
    "    :return:\n",
    "    '''\n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "import gc\n",
    "def reduce_mem(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类别变量的统计特征\n",
    "def get_count_feat(data,cols,feature):\n",
    "    for col in cols:\n",
    "        if not isinstance(col,list):\n",
    "            data['count_{}'.format(col)] = data.groupby(col)[col].transform('count')\n",
    "            feature.append('count_{}'.format(col))\n",
    "        else:\n",
    "            data['count_{}'.format('_'.join(col))] = data.groupby(col)['index'].transform('count')\n",
    "            feature.append('count_{}'.format('_'.join(col)))\n",
    "    return data,feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_user_info(data):\n",
    "    did_list = []\n",
    "    time_list = []\n",
    "    vid_list = []\n",
    "    for ii in data.values:\n",
    "        time_vid = ii[1].replace('[','').replace(']','')\n",
    "        time_vid = time_vid.split(',')\n",
    "        if len(time_vid) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            for k,s in enumerate(time_vid):\n",
    "                if (k+1) % 2 == 1:\n",
    "                    did_list.append(ii[0].strip())\n",
    "                    time_list.append(s.strip())\n",
    "                else:\n",
    "                    vid_list.append(s.strip())\n",
    "    data = pd.DataFrame()\n",
    "    data['did'] = did_list\n",
    "    data['vid'] = vid_list\n",
    "    data['timestamp'] = time_list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p ../input/all/w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import numpy as np\n",
    "def get_word2vec_feature(seq,emb,feat,ikx,ext='',feature=[]):\n",
    "    sentence = [[str(x) for x in x] for x in seq]\n",
    "    if os.path.exists('../input/all/w2v/w2v_model_{}_{}.model'.format('_'.join(feat),ext)):\n",
    "        model = Word2Vec.load('../input/all/w2v/w2v_model_{}_{}.model'.format('_'.join(feat),ext))\n",
    "    else:\n",
    "        model = Word2Vec(sentence, size=emb, window=5, min_count=1, workers=10, iter=10, sg=1, seed=42)\n",
    "        model.save('../input/all/w2v/w2v_model_{}_{}.model'.format('_'.join(feat),ext))\n",
    "#     print('make emb_dict ing')\n",
    "#     emb_dict = {}\n",
    "#     for sent in sentence:\n",
    "#         vec = []\n",
    "#         for w in sent:\n",
    "#             if w in model:\n",
    "#                 emb_dict[w] = model[w]\n",
    "#     print('make emb_martix ing',len(emb_dict))\n",
    "#     emb_martix = []\n",
    "#     index = []\n",
    "#     idx = 0\n",
    "#     for v in emb_dict:\n",
    "#         if v not in index:\n",
    "#             if idx == 0: \n",
    "#                 print(v,emb_dict[v])\n",
    "#             idx = idx + 1\n",
    "#             index.append(v)\n",
    "#             emb_martix.append(np.array(emb_dict[v]))\n",
    "#     emb_martix = np.array(emb_martix)\n",
    "#     print('make data dataframe ing')\n",
    "#     data = pd.DataFrame()\n",
    "#     data[ikx] = index\n",
    "#     for i in range(emb):\n",
    "#         data['{}_emb_{}_{}'.format(ikx, i, ext)] = emb_martix[:,i]\n",
    "#         feature.append('{}_emb_{}_{}'.format(ikx, i, ext))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seq(data,col1,col2):\n",
    "    tmp = data.groupby(col1)[col2].apply(lambda x:list(x)).reset_index()\n",
    "    sentences = tmp[col2].values.tolist()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_watch_time(data,feature):\n",
    "    data['timestamp'] = data['timestamp'].astype(int)\n",
    "    data = data.sort_values('timestamp')\n",
    "    tmp_data = pd.DataFrame()\n",
    "    for i in [1,2,3,4,5]:\n",
    "        data['time_{}'.format(i)] = data.groupby('did')['timestamp'].shift(i)\n",
    "        data['time_{}'.format(i)] = data['timestamp'] - data['time_{}'.format(i)]\n",
    "        \n",
    "#         data['time_-{}'.format(i)] = data.groupby('did')['timestamp'].shift(-i)\n",
    "#         data['time_-{}'.format(i)] = -data['time_-{}'.format(i)] + data['timestamp']\n",
    "    \n",
    "        tmp = data.groupby('did')['time_{}'.format(i)].mean().reset_index()\n",
    "        \n",
    "        if i == 1:\n",
    "            tmp_data = tmp\n",
    "        else:\n",
    "            tmp_data = pd.merge(tmp_data,tmp,on=['did'],how='left',copy=False)\n",
    "        feature.append('time_{}'.format(i))\n",
    "#         feature.append('time_-{}_mean'.format(i))\n",
    "    return tmp_data,feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_static_feature(data,col,static_feature,feature,log):\n",
    "    tmp_1 = data.groupby(col)[static_feature].mean().add_suffix('_mean').reset_index()\n",
    "    tmp_2 = data.groupby(col)[static_feature].median().add_suffix('_median').reset_index()\n",
    "    if log == True:\n",
    "        for f in tmp_1.columns:\n",
    "            if f not in ['did']:\n",
    "                feature.append(f)\n",
    "        for f in tmp_2.columns:\n",
    "            if f not in ['did']:\n",
    "                feature.append(f)\n",
    "    tmp = pd.merge(tmp_1,tmp_2,on=[col],how='outer',copy=False)\n",
    "    return pd.merge(data,tmp,on=[col],how='outer',copy=False),feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def get_time(timeStamp):\n",
    "    timeArray = time.localtime(timeStamp)\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", timeArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_date(data):\n",
    "    tmp_timestamp_first = data.groupby(['did'])['timestamp'].min().reset_index()\n",
    "    del data['timestamp']\n",
    "    data = pd.merge(data,tmp_timestamp_first,on=['did'],how='left',copy=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p ../input/all/w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import numpy as np\n",
    "def get_w2v_feature(data,col1,col2,emb_size,ext='',feature=[]):\n",
    "    print('begin train word2vec')\n",
    "    data = data[col1 +[col2]]\n",
    "    data[col2] = data[col2].astype(str)\n",
    "    tmp = data.groupby(col1)[col2].apply(lambda x:list(x)).reset_index()\n",
    "    sentences = tmp[col2].values.tolist()\n",
    "    print(tmp.head())\n",
    "    del tmp[col2]\n",
    "    if os.path.exists('../input/all/w2v/{}_{}_feature{}.model'.format('_'.join(col1),col2,ext)):\n",
    "        model = Word2Vec.load('../input/all/w2v/{}_{}_feature{}.model'.format('_'.join(col1),col2,ext))\n",
    "    else:\n",
    "        model = Word2Vec(sentences, size=emb_size, window=10, min_count=1, sg=1, seed=42,iter=10)\n",
    "        model.save('../input/all/w2v/{}_{}_feature{}.model'.format('_'.join(col1),col2,ext))\n",
    "    emb_matrix = []\n",
    "    emb_dict = {}\n",
    "    print('begin make feature')\n",
    "    for seq in sentences:\n",
    "        vec = []\n",
    "        for w in seq:\n",
    "            if w in model:\n",
    "                vec.append(model[w])\n",
    "                emb_dict[w] = model[w]\n",
    "        if len(vec) > 0:\n",
    "            emb_matrix.append(np.mean(vec, axis=0))\n",
    "        else:\n",
    "            emb_matrix.append([0] * emb_size)\n",
    "    emb_matrix = np.array(emb_matrix)\n",
    "    for i in range(emb_size):\n",
    "        tmp['{}_{}_emb_{}{}'.format('_'.join(col1), col2, i, ext)] = emb_matrix[:, i]\n",
    "        feature.append('{}_{}_emb_{}{}'.format('_'.join(col1), col2, i,ext))\n",
    "    del model, emb_matrix, sentences\n",
    "    new_emb_martix = []\n",
    "    data_index = []\n",
    "    for v in emb_dict:\n",
    "        data_index.append(v)\n",
    "        tmp_emb = np.array(emb_dict[v])\n",
    "        new_emb_martix.append(tmp_emb)\n",
    "    new_emb_martix = np.array(new_emb_martix)\n",
    "    data = pd.DataFrame()\n",
    "    data[col2] = data_index\n",
    "    for i in range(emb_size):\n",
    "        data['{}_emb_{}_{}'.format(col2, i, ext)] = new_emb_martix[:,i]\n",
    "        feature.append('{}_emb_{}_{}'.format(col2, i, ext))\n",
    "    return tmp,feature,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '../input/eval/'\n",
    "train_path = '../input/train/part_'\n",
    "valid_path = '../input/train/part_30/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n"
     ]
    }
   ],
   "source": [
    "print('train data')\n",
    "train = pd.DataFrame()\n",
    "train_item = pd.DataFrame()\n",
    "for i in range(29,30):\n",
    "    tmp = load_data(train_path+ '{}/'.format(i) + 'context.parquet')\n",
    "    tmp_item = load_data(train_path+ '{}/'.format(i) + 'item.parquet')\n",
    "    tmp_item.rename(columns={'timestamp':'item_timestamp'},inplace=True)\n",
    "    tmp = pd.merge(tmp,tmp_item,on=['vid'],how='left',copy=False)\n",
    "    train = pd.concat([train,tmp],axis=0,sort=False)\n",
    "    del tmp,tmp_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data\n"
     ]
    }
   ],
   "source": [
    "print('test data')\n",
    "test = load_data(test_path  + 'context.parquet')\n",
    "test_item = load_data(test_path  + 'item.parquet')\n",
    "test_item.rename(columns={'timestamp':'item_timestamp'},inplace=True)\n",
    "test = pd.merge(test,test_item,on=['vid'],how='left',copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid data\n"
     ]
    }
   ],
   "source": [
    "print('valid data')\n",
    "valid = load_data(valid_path + 'context.parquet')\n",
    "valid_item = load_data(valid_path + 'item.parquet')\n",
    "valid_item.rename(columns={'timestamp':'item_timestamp'},inplace=True)\n",
    "valid = pd.merge(valid,valid_item,on=['vid'],how='left',copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_new_date(train)\n",
    "valid = get_new_date(valid)\n",
    "test = get_new_date(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['date'] = train['timestamp'].apply(lambda x:get_time(x))\n",
    "valid['date'] = valid['timestamp'].apply(lambda x:get_time(x))\n",
    "test['date'] = test['timestamp'].apply(lambda x:get_time(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "train['date'] = pd.to_datetime(train['date']) + datetime.timedelta(hours=8)\n",
    "valid['date'] = pd.to_datetime(valid['date']) + datetime.timedelta(hours=8)\n",
    "test['date'] = pd.to_datetime(test['date']) + datetime.timedelta(hours=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['day'] = train['date'].dt.day\n",
    "valid['day'] = valid['date'].dt.day\n",
    "test['day'] = test['date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hour'] = train['date'].dt.hour\n",
    "valid['hour'] = valid['date'].dt.hour\n",
    "test['hour'] = test['date'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train,valid,test],axis=0,ignore_index=True,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stars'] = data['stars'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_word2vec_feature(data['stars'].values.tolist(),8,['did','stars'],'stars',ext='8',feature=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 3265576/11844346 [04:19<10:48, 13224.70it/s]"
     ]
    }
   ],
   "source": [
    "emb_matrix = []\n",
    "for col in tqdm(data['stars'].values):\n",
    "    tmp = np.zeros(shape=(8))\n",
    "    for seq in col:\n",
    "        tmp = tmp + model[str(seq)] / len(col)\n",
    "    emb_matrix.append(tmp)\n",
    "emb_matrix = np.array(emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    data['{}_{}_{}'.format('did','stars',i)] = emb_matrix[:,i]\n",
    "    feature.append('{}_{}_{}'.format('did','stars',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feature_1,feature,total_feature_2 = get_w2v_feature(data,['did'],'vid',8,ext='8',feature=feature)\n",
    "# total_feature_1 = reduce_mem(total_feature_1)\n",
    "total_feature_2 = reduce_mem(total_feature_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feature_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feature_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feature_2['vid'] = total_feature_2['vid'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feature_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.merge(data,total_feature_1,how='left',on=['did'],copy=False)\n",
    "data = pd.merge(data,total_feature_2,how='left',on=['vid'],copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['mod', 'mf', 'aver', 'sver', 'did','vid', 'prev', 'region','cid', 'class_id','is_intact', 'second_class',\n",
    "#       ['hour','vid'],['day','vid'],\n",
    "#       ['hour','cid'],['day','cid'],\n",
    "      ]\n",
    "data,feature = get_count_feat(data,col,feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [ 'vid', 'prev','region', 'title_length', 'item_timestamp', 'cid', 'class_id','is_intact', 'second_class', 'duration', 'ctr', 'vv','hour']:\n",
    "    feature.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['timestamp_item_timestamp'] = data['timestamp'] = data['item_timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['timestamp_item_timestamp']:\n",
    "    feature.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cross_feat in [['did','vid'],['did','cid'],['did','class_id'],['did','is_intact'],['did','second_class']]:\n",
    "#     tmp = data.groupby(cross_feat[0])[cross_feat[1]].nunique().reset_index().rename(columns={cross_feat[1]:'{}_{}_nunique'.format(cross_feat[0],cross_feat[1])})\n",
    "#     data = pd.merge(data,tmp,on=cross_feat[0],how='left',copy=False)\n",
    "#     feature.append('{}_{}_nunique'.format(cross_feat[0],cross_feat[1]))\n",
    "    \n",
    "# #     tmp = data.groupby(cross_feat[1])[cross_feat[0]].nunique().reset_index().rename(columns={cross_feat[0]:'{}_{}_nunique'.format(cross_feat[1],cross_feat[0])})\n",
    "# #     data = pd.merge(data,tmp,on=cross_feat[1],how='left',copy=False)\n",
    "# #     feature.append('{}_{}_nunique'.format(cross_feat[1],cross_feat[0]))\n",
    "    \n",
    "#     del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_f in ['mod', 'mf', 'aver', 'sver']:\n",
    "    data[cat_f] = data[cat_f].astype(\"category\")\n",
    "    data[cat_f] = data[cat_f].cat.codes\n",
    "    feature.append(cat_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:train.shape[0]]\n",
    "valid = data[train.shape[0]:(train.shape[0] + valid.shape[0])]\n",
    "test = data[-test.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "params = {'num_leaves': 32, #结果对最终效果影响较大，越大值越好，太大会出现过拟合\n",
    "          'objective': 'binary', #定义的目标函数\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.1,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"verbosity\": 1,\n",
    "          'two_round':'true',\n",
    "          \"nthread\": -1,               \n",
    "          'metric': {'auc'},  \n",
    "          \"random_state\": 42, \n",
    "          }\n",
    "trn_data = lgb.Dataset(train[feature].values, label=train[target].values)\n",
    "val_data = lgb.Dataset(valid[feature].values, label=valid[target].values)\n",
    "\n",
    "clf = lgb.train(params,\n",
    "                trn_data,\n",
    "                500,\n",
    "                valid_sets=[trn_data, val_data],\n",
    "                verbose_eval=50,\n",
    "                early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predict = clf.predict(valid[feature].values, num_iteration=clf.best_iteration)\n",
    "xx_gauc = gauc(list(valid[target].values), list(valid_predict), list(valid['did'].values))\n",
    "print(xx_gauc)\n",
    "# 0.604091\n",
    "# 0.631716\n",
    "# 0.637493\n",
    "# 0.661686"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.feature_importance())\n",
    "print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_train = pd.concat([train,valid],axis=0,ignore_index=True,sort=False)\n",
    "del train,valid,trn_data,val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = lgb.Dataset(for_train[feature].values, label=for_train[target].values)\n",
    "for_clf = lgb.train(params,\n",
    "                trn_data,\n",
    "                clf.best_iteration,\n",
    "                valid_sets=[trn_data],\n",
    "                verbose_eval=50,\n",
    "                early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_predict = for_clf.predict(test[feature].values, num_iteration=for_clf.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = test[['index']]\n",
    "submit['score'] = for_predict\n",
    "submit.to_csv('./submit_{}.csv'.format(str(xx_gauc).split('.')[1]),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
