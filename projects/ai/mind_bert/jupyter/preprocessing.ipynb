{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import bert.tokenization\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 128\n",
    "\n",
    "DATA_PATH =  \"../input/toxic-multi\"\n",
    "BERT_PATH = \"../input/bert-multi\"\n",
    "BERT_PATH_SAVEDMODEL = os.path.join(BERT_PATH, \"1\")\n",
    "\n",
    "OUTPUT_PATH = \"../output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data from our first competition,\n",
    "# https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "wiki_toxic_comment_data = \"jigsaw-toxic-comment-train.csv\"\n",
    "\n",
    "wiki_toxic_comment_train = pandas.read_csv(os.path.join(\n",
    "    DATA_PATH, wiki_toxic_comment_data))\n",
    "wiki_toxic_comment_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223549"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_toxic_comment_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(bert_path=BERT_PATH_SAVEDMODEL):\n",
    "    \"\"\"Get the tokenizer for a BERT layer.\"\"\"\n",
    "    bert_layer = tf.saved_model.load(bert_path)\n",
    "    bert_layer = hub.KerasLayer(bert_layer, trainable=False)\n",
    "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    cased = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    tf.gfile = tf.io.gfile  # for bert.tokenization.load_vocab in tokenizer\n",
    "    tokenizer = bert.tokenization.FullTokenizer(vocab_file, cased)\n",
    "  \n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretty much everyone from warren county/surrounding regions was born at glens falls hospital. myself included. however, i'm not sure this qualifies an\n",
      "['pretty', 'much', 'everyone', 'from', 'war', '##ren', 'county', '/', 'surrounding', 'regions', 'was', 'born', 'at', 'g', '##lens', 'falls', 'hospital']\n",
      "[108361, 13172, 48628, 10188, 10338, 10969, 17382, 120, 27027, 21721, 10134, 11175, 10160, 175, 97681, 35017, 18141]\n"
     ]
    }
   ],
   "source": [
    "example_sentence = wiki_toxic_comment_train.iloc[37].comment_text[:150]\n",
    "print(example_sentence)\n",
    "\n",
    "example_tokens = tokenizer.tokenize(example_sentence)\n",
    "print(example_tokens[:17])\n",
    "\n",
    "example_input_ids = tokenizer.convert_tokens_to_ids(example_tokens)\n",
    "print(example_input_ids[:17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jigsaw-toxic-comment-train-processed-seqlen128.csv\n"
     ]
    }
   ],
   "source": [
    "def process_sentence(sentence, max_seq_length=SEQUENCE_LENGTH, tokenizer=tokenizer):\n",
    "    \"\"\"Helper function to prepare data for BERT. Converts sentence input examples\n",
    "    into the form ['input_word_ids', 'input_mask', 'segment_ids'].\"\"\"\n",
    "    # Tokenize, and truncate to max_seq_length if necessary.\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    if len(tokens) > max_seq_length - 2:\n",
    "        tokens = tokens[:(max_seq_length - 2)]\n",
    "\n",
    "    # Convert the tokens in the sentence to word IDs.\n",
    "    input_ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens + [\"[SEP]\"])\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    pad_length = max_seq_length - len(input_ids)\n",
    "    input_ids.extend([0] * pad_length)\n",
    "    input_mask.extend([0] * pad_length)\n",
    "\n",
    "    # We only have one input segment.\n",
    "    segment_ids = [0] * max_seq_length\n",
    "\n",
    "    return (input_ids, input_mask, segment_ids)\n",
    "\n",
    "def preprocess_and_save_dataset(unprocessed_filename, text_label='comment_text',\n",
    "                                seq_length=SEQUENCE_LENGTH, verbose=True):\n",
    "    \"\"\"Preprocess a CSV to the expected TF Dataset form for multilingual BERT,\n",
    "    and save the result.\"\"\"\n",
    "    dataframe = pandas.read_csv(os.path.join(DATA_PATH, unprocessed_filename),\n",
    "                                index_col='id')\n",
    "    processed_filename = (unprocessed_filename.rstrip('.csv') +\n",
    "                          \"-processed-seqlen{}.csv\".format(SEQUENCE_LENGTH))\n",
    "\n",
    "    pos = 0\n",
    "    start = time.time()\n",
    "\n",
    "    while pos < len(dataframe):\n",
    "        processed_df = dataframe[pos:pos + 10000].copy()\n",
    "\n",
    "        processed_df['input_word_ids'], processed_df['input_mask'], processed_df['all_segment_id'] = (\n",
    "            zip(*processed_df[text_label].apply(process_sentence)))\n",
    "\n",
    "        if pos == 0:\n",
    "            processed_df.to_csv(processed_filename, index_label='id', mode='w')\n",
    "        else:\n",
    "            processed_df.to_csv(processed_filename, index_label='id', mode='a',\n",
    "                                header=False)\n",
    "\n",
    "        if verbose:\n",
    "            print('Processed {} examples in {}'.format(\n",
    "                pos + 10000, time.time() - start))\n",
    "        pos += 10000\n",
    "    return\n",
    "  \n",
    "# Process the training dataset.\n",
    "preprocess_and_save_dataset(wiki_toxic_comment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string_list_into_ints(strlist):\n",
    "    s = tf.strings.strip(strlist)\n",
    "    s = tf.strings.substr(\n",
    "        strlist, 1, tf.strings.length(s) - 2)  # Remove parentheses around list\n",
    "    s = tf.strings.split(s, ',', maxsplit=SEQUENCE_LENGTH)\n",
    "    s = tf.strings.to_number(s, tf.int32)\n",
    "    s = tf.reshape(s, [SEQUENCE_LENGTH])  # Force shape here needed for XLA compilation (TPU)\n",
    "    return s\n",
    "\n",
    "def format_sentences(data, label='toxic', remove_language=False):\n",
    "    labels = {'labels': data.pop(label)}\n",
    "    if remove_language:\n",
    "        languages = {'language': data.pop('lang')}\n",
    "    # The remaining three items in the dict parsed from the CSV are lists of integers\n",
    "    for k,v in data.items():  # \"input_word_ids\", \"input_mask\", \"all_segment_id\"\n",
    "        data[k] = parse_string_list_into_ints(v)\n",
    "    return data, labels\n",
    "\n",
    "def make_sentence_dataset_from_csv(filename, label='toxic', language_to_filter=None):\n",
    "    # This assumes the column order label, input_word_ids, input_mask, segment_ids\n",
    "    SELECTED_COLUMNS = [label, \"input_word_ids\", \"input_mask\", \"all_segment_id\"]\n",
    "    label_default = tf.int32 if label == 'id' else tf.float32\n",
    "    COLUMN_DEFAULTS  = [label_default, tf.string, tf.string, tf.string]\n",
    "\n",
    "    if language_to_filter:\n",
    "        insert_pos = 0 if label != 'id' else 1\n",
    "        SELECTED_COLUMNS.insert(insert_pos, 'lang')\n",
    "        COLUMN_DEFAULTS.insert(insert_pos, tf.string)\n",
    "\n",
    "    preprocessed_sentences_dataset = tf.data.experimental.make_csv_dataset(\n",
    "        filename, column_defaults=COLUMN_DEFAULTS, select_columns=SELECTED_COLUMNS,\n",
    "        batch_size=1, num_epochs=1, shuffle=False)  # We'll do repeating and shuffling ourselves\n",
    "    # make_csv_dataset required a batch size, but we want to batch later\n",
    "    preprocessed_sentences_dataset = preprocessed_sentences_dataset.unbatch()\n",
    "    \n",
    "    if language_to_filter:\n",
    "        preprocessed_sentences_dataset = preprocessed_sentences_dataset.filter(\n",
    "            lambda data: tf.math.equal(data['lang'], tf.constant(language_to_filter)))\n",
    "        #preprocessed_sentences.pop('lang')\n",
    "    preprocessed_sentences_dataset = preprocessed_sentences_dataset.map(\n",
    "        lambda data: format_sentences(data, label=label,\n",
    "                                      remove_language=language_to_filter))\n",
    "\n",
    "    return preprocessed_sentences_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}