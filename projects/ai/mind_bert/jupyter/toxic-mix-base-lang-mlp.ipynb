{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-multi\t\t\t\t\t  tfrecords\r\n",
      "huggingface\t\t\t\t\t  tf-xlm-roberta-base\r\n",
      "jigsaw-multilingual-toxic-comment-classification  tf-xlm-roberta-large\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../input/tfrecords'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os  \n",
    "import traceback\n",
    "\n",
    "RECORDS_PATH = '../input/tfrecords'\n",
    "\n",
    "if os.path.exists('/kaggle'):\n",
    "  sys.path.append('/kaggle/input/gezi-melt/utils/utils')\n",
    "  sys.path.append('/kaggle/input/official')\n",
    "  from kaggle_datasets import KaggleDatasets\n",
    "  try:\n",
    "    RECORDS_PATH = KaggleDatasets().get_gcs_path('toxic-multi-tfrecords') + '/tfrecords/tfrecords'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "  except Exception:\n",
    "    print(traceback.format_exc())\n",
    "    RECORDS_PATH = '../input/toxic-multi-tfrecords/tfrecords/tfrecords'\n",
    "    pass\n",
    "!ls ../input\n",
    "RECORDS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0-dev20200417'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import official\n",
    "import gezi\n",
    "import melt\n",
    "import lele\n",
    "import husky\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from absl import app, flags\n",
    "FLAGS = flags.FLAGS\n",
    "try:\n",
    "    flags.DEFINE_string('model', None, '')\n",
    "    flags.DEFINE_bool('multi_head', False, '')\n",
    "    flags.DEFINE_string('pretrained', '../input/tf-xlm-roberta-large', '')\n",
    "    flags.DEFINE_string('pretrained2', None, '')\n",
    "    flags.DEFINE_integer('max_len', None, 'xlm 192 bert 128')\n",
    "    flags.DEFINE_bool('freeze_pretrained', False, '')\n",
    "    flags.DEFINE_bool('valid_en', False, '')\n",
    "    flags.DEFINE_alias('ve', 'valid_en')\n",
    "    flags.DEFINE_bool('test_en', None, '')\n",
    "    flags.DEFINE_bool('use_lang', False, '')\n",
    "    flags.DEFINE_bool('use_mlp', False, '')\n",
    "    flags.DEFINE_bool('use_src', False, '')\n",
    "    flags.DEFINE_bool('use_trans', False, '')\n",
    "    flags.DEFINE_string('task', 'toxic', '')\n",
    "except Exception:\n",
    "    pass\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags\n",
    "argv=['']\n",
    "FLAGS(argv)\n",
    "mark='xlm'\n",
    "FLAGS.train_input=f'{RECORDS_PATH}/{mark}/jigsaw-toxic-comment-train'\n",
    "FLAGS.valid_input=f'{RECORDS_PATH}/{mark}/validation'\n",
    "FLAGS.test_input=f'{RECORDS_PATH}/{mark}/test'\n",
    "FLAGS.valid_interval_steps=0\n",
    "FLAGS.verbose=1 \n",
    "FLAGS.num_epochs=1\n",
    "FLAGS.keras=1 \n",
    "FLAGS.buffer_size=2048\n",
    "FLAGS.learning_rate=3e-5 \n",
    "FLAGS.min_learning_rate=0.\n",
    "# FLAGS.opt_epsilon=1e-8 \n",
    "FLAGS.optimizer='bert-adamw'\n",
    "# FLAGS.optimizer='adam'\n",
    "FLAGS.metrics=['acc', 'auc'] \n",
    "FLAGS.test_names=['id', 'toxic']\n",
    "FLAGS.valid_interval_epochs=0.1\n",
    "FLAGS.test_interval_epochs=1.\n",
    "FLAGS.num_gpus=8\n",
    "FLAGS.cache=0\n",
    "FLAGS.model_dir='../working/exps/toxic-mix-lang-mlp'\n",
    "# FLAGS.ckpt_dir='../input/single-unintended'\n",
    "FLAGS.multi_head=0\n",
    "FLAGS.batch_parse=1\n",
    "FLAGS.save_model=1\n",
    "# FLAGS.pretrained = '../input/tf-xlm-roberta-large/'\n",
    "FLAGS.pretrained = '../input/tf-xlm-roberta-base/'\n",
    "FLAGS.batch_size=16 if 'large' in FLAGS.pretrained else 32\n",
    "FLAGS.debug=0\n",
    "FLAGS.sparse_to_dense=1 \n",
    "FLAGS.padding_idx=1 \n",
    "# FLAGS.buckets=190,350 \n",
    "FLAGS.buckets=[]\n",
    "FLAGS.batch_size=32 \n",
    "# FLAGS.batch_sizes=32,16,8 if not 'large' in FLAGS.pretrained else 16,8,4\n",
    "FLAGS.batch_sizes=[]\n",
    "# FLAGS.length_key='input_word_ids'\n",
    "FLAGS.max_len=192\n",
    "FLAGS.do_test=0\n",
    "\n",
    "toxic_types = ['severe_toxic', 'obscene', 'identity_hate', 'threat', 'insult']\n",
    "langs = ['en', 'es', 'it', 'tr', 'fr', 'pt', 'ru']\n",
    "srcs = ['unintended', 'toxic', 'test']\n",
    "\n",
    "BERT_GCS_PATH_SAVEDMODEL = '../input/bert-multi/bert_multi_from_tfhub'\n",
    "RECORDS_GCS_PATH = '../input/tfrecords'\n",
    "\n",
    "def init():\n",
    "  # if FLAGS.mode == 'valid' or FLAGS.mode == 'test':\n",
    "  #   FLAGS.gpus = 1\n",
    "  if FLAGS.valid_en:\n",
    "    if FLAGS.test_en is None:\n",
    "      FLAGS.test_en = True\n",
    "    FLAGS.valid_input = FLAGS.valid_input.replace('validation', 'validation-en')\n",
    "    FLAGS.train_input = FLAGS.train_input.replace('validation', 'validation-en')\n",
    "  if FLAGS.test_en:\n",
    "    FLAGS.test_input = FLAGS.test_input.replace('test', 'test-en')\n",
    "  if 'bylang' in FLAGS.valid_input:\n",
    "    if FLAGS.folds is not None:\n",
    "      FLAGS.folds = 3\n",
    "  \n",
    "  if FLAGS.max_len:\n",
    "    FLAGS.keras_loop = True\n",
    "    FLAGS.sparse_to_dense = False\n",
    "    \n",
    "  if 'pair' in FLAGS.train_input:\n",
    "    FLAGS.batch_size = int(FLAGS.batch_size / 2)\n",
    "    if FLAGS.max_len:\n",
    "      FLAGS.max_len *= 2\n",
    "\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.model_dir='../working/exps/toxic-mix-lang-mlp'\n",
    "FLAGS.save_model=1\n",
    "FLAGS.do_test=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import gezi\n",
    "logging = gezi.logging\n",
    "\n",
    "eval_langs = ['es', 'it', 'tr']\n",
    "\n",
    "def evaluate(y_true, y_pred, x):\n",
    "  if FLAGS.task == 'toxic':\n",
    "    try:\n",
    "      y_true = y_true[:,0]\n",
    "      y_pred = y_pred[:,0]\n",
    "    except Exception:\n",
    "      pass\n",
    "    if y_pred.max() > 1. or y_pred.min() < 0:\n",
    "      y_pred = gezi.sigmoid(y_pred)\n",
    "    result = OrderedDict()\n",
    "    loss = log_loss(y_true, y_pred)\n",
    "    result['loss'] = loss\n",
    "    \n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    result['auc/all'] = auc\n",
    "      \n",
    "    if 'lang' in x:\n",
    "      x['y_true'] = y_true\n",
    "      x['pred'] = y_pred\n",
    "      x['lang'] = gezi.decode(x['lang'])\n",
    "\n",
    "      df = pd.DataFrame(x) \n",
    "      df = shuffle(df)\n",
    "      gezi.pprint(pd.concat([df[df.y_true==0].sample(5), df[df.y_true==1].sample(5)]), \n",
    "                  print_fn=logging.info, desc='preds:')\n",
    "\n",
    "      for lang in eval_langs:\n",
    "        df_ = df[df.lang==lang]\n",
    "        if len(df_):\n",
    "          auc = roc_auc_score(df_.y_true, df_.pred)\n",
    "          result[f'auc/{lang}'] = auc\n",
    "  elif FLAGS.task == 'lang':\n",
    "    results = np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1)\n",
    "    result = {'acc': np.sum(results) / len(results)}\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def get_lang_id(x):\n",
    "  return tf.cast(tf.math.equal(x, 'en'), tf.int32) \\\n",
    "         + tf.cast(tf.math.equal(x, 'es'), tf.int32) * 2 \\\n",
    "         + tf.cast(tf.math.equal(x, 'it'), tf.int32) * 3 \\\n",
    "         + tf.cast(tf.math.equal(x, 'tr'), tf.int32) * 4 \\\n",
    "         + tf.cast(tf.math.equal(x, 'fr'), tf.int32) * 5 \\\n",
    "         + tf.cast(tf.math.equal(x, 'pt'), tf.int32) * 6 \\\n",
    "         + tf.cast(tf.math.equal(x, 'ru'), tf.int32) * 7 \\\n",
    "         - 1\n",
    "\n",
    "# srcs = ['unintended', 'toxic', 'test']\n",
    "def get_src_id(x):\n",
    "  return tf.cast(tf.math.equal(x, 'unintended'), tf.int32) \\\n",
    "         + tf.cast(tf.math.equal(x, 'toxic'), tf.int32) * 2 \\\n",
    "         + tf.cast(tf.math.equal(x, 'test'), tf.int32) * 3 \\\n",
    "         - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import melt\n",
    "\n",
    "class Dataset(melt.Dataset):\n",
    "  def __init__(self, subset='valid', **kwargs):\n",
    "    super(Dataset, self).__init__(subset, **kwargs)\n",
    "\n",
    "  def parse(self, example):\n",
    "    MAX_LEN = FLAGS.max_len\n",
    "    features_dict = {\n",
    "      'toxic': tf.io.FixedLenFeature([], tf.float32),\n",
    "      'id': tf.io.FixedLenFeature([], tf.string),\n",
    "      # 'comment_text': tf.io.FixedLenFeature([], tf.string), # TODO\n",
    "    }\n",
    "    features_dict['input_word_ids'] = tf.io.VarLenFeature(tf.int64) if not MAX_LEN else  tf.io.FixedLenFeature([MAX_LEN], tf.int64)\n",
    "\n",
    "    def _adds(names, dtype=None, length=None):\n",
    "      dtype_ = dtype\n",
    "      for name in names:\n",
    "        if name in self.example:\n",
    "          dtype = dtype_ or self.example[name].dtype \n",
    "          if length:\n",
    "            features_dict[name] = tf.io.FixedLenFeature([length], dtype)\n",
    "          else:\n",
    "            features_dict[name] = tf.io.FixedLenFeature([], dtype)\n",
    "\n",
    "    _adds(['lang', 'src'], tf.string)\n",
    "    _adds(['trans'], tf.int64, 1)\n",
    "\n",
    "    _adds(['input_mask', 'all_segment_id'], tf.int64, MAX_LEN)\n",
    "    \n",
    "    _adds(toxic_types)\n",
    "\n",
    "    features = self.parse_(serialized=example, features=features_dict)\n",
    "\n",
    "    features['lang_'] = tf.expand_dims(get_lang_id(features['lang']), -1)\n",
    "    features['src_'] = tf.expand_dims(get_src_id(features['src']), -1)\n",
    "\n",
    "    def _casts(names, dtype=tf.int32):\n",
    "      for name in names:\n",
    "        if name in features:\n",
    "          features[name] = tf.cast(features[name], dtype)\n",
    "\n",
    "    _casts(['input_word_ids', 'input_mask', 'all_segment_id', 'trans'])\n",
    "\n",
    "    x = features\n",
    "\n",
    "    if FLAGS.task == 'toxic':\n",
    "      y = features['toxic']\n",
    "    #     y = tf.cast(features['toxic'] > 0.5, tf.float32)\n",
    "      keys = ['toxic', *toxic_types]\n",
    "      for key in keys:\n",
    "        if key not in features:\n",
    "          features[key] = tf.zeros_like(features['toxic'])\n",
    "          \n",
    "      _casts(toxic_types, tf.float32)\n",
    "          \n",
    "      melt.append_dim(features, keys)\n",
    "\n",
    "      if FLAGS.multi_head:\n",
    "        y = tf.concat([features[key] for key in keys], 1)\n",
    "\n",
    "\n",
    "    elif FLAGS.task == 'lang':\n",
    "      y = tf.one_hot(features['lang_'], len(langs))\n",
    "\n",
    " \n",
    "    # self.padding_values = (melt.get_padding_values(features, 1), 1.)\n",
    "\n",
    "    return x, y\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "import tensorflow as tf\n",
    "\n",
    "def calc_loss(y_true, y_pred):\n",
    "  pass\n",
    "\n",
    "def focal_loss(gamma=1.5, alpha=.2):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "def get_loss_fn():\n",
    "#   return tf.compat.v1.losses.sigmoid_cross_entropy\n",
    "  return tf.keras.losses.BinaryCrossentropy()\n",
    "#   return focal_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0424 16:52:09.308653 139650726364992 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0424 16:52:09.309360 139650726364992 file_utils.py:57] TensorFlow version 2.2.0-dev20200417 available.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import transformers\n",
    "from transformers import TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.use_mlp=1\n",
    "FLAGS.use_lang=1\n",
    "FLAGS.write_valid=1\n",
    "# FLAGS.use_src=1\n",
    "# FLAGS.use_trans=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlmModel(keras.Model):\n",
    "  def __init__(self):\n",
    "    super(XlmModel, self).__init__() \n",
    "\n",
    "    pretrained = FLAGS.pretrained\n",
    "\n",
    "    with gezi.Timer(f'load xlm_model from {pretrained}', True, logging.info):\n",
    "      self.transformer = TFAutoModel.from_pretrained(pretrained)\n",
    "\n",
    "    # if FLAGS.pretrained2:\n",
    "    #   logging.info(f'load lang_model from {FLAGS.pretrained2}')\n",
    "    #   self.transformer.load_weights(FLAGS.pretrained2)\n",
    "\n",
    "    if FLAGS.freeze_pretrained:\n",
    "      self.transformer.trainable = False\n",
    "    # dims = [32]\n",
    "    # self.mlp = melt.layers.MLP(dims)\n",
    "    if FLAGS.use_lang:\n",
    "      self.lang_emb = keras.layers.Embedding(len(langs), 32, name='lang_emb')\n",
    "    if FLAGS.use_src:\n",
    "      self.src_emb = keras.layers.Embedding(len(srcs), 32, name='src_emb')\n",
    "    if FLAGS.use_trans:\n",
    "      self.trans_emb = keras.layers.Embedding(2, 32, name='trans_emb')\n",
    "    \n",
    "    if FLAGS.use_mlp:\n",
    "      self.mlp = melt.layers.MLP([256,128])\n",
    "    odim = len(toxic_types) + 1 if FLAGS.multi_head else 1\n",
    "    self.dense = keras.layers.Dense(odim, activation='sigmoid')\n",
    "\n",
    "  def call(self, input):\n",
    "    input_word_ids = input['input_word_ids']\n",
    "    # tf.print(input_word_ids)\n",
    "    # tf.print(input['id'])\n",
    "    # tf.print(input['toxic'])\n",
    "    x = self.transformer(input_word_ids)[0]\n",
    "    x = x[:, 0, :]\n",
    "    \n",
    "    others = []\n",
    "    if FLAGS.use_lang:\n",
    "      lang_ids = input['lang_']\n",
    "      x_lang = tf.squeeze(self.lang_emb(lang_ids), 1)\n",
    "      others += [x_lang]\n",
    "  \n",
    "    if FLAGS.use_src:\n",
    "      srcs = input['src_']\n",
    "      x_src = tf.squeeze(self.src_emb(srcs), 1)\n",
    "      others += [x_src]\n",
    "        \n",
    "    if FLAGS.use_trans:\n",
    "      trans = input['trans']\n",
    "      x_trans = tf.squeeze(self.trans_emb(trans), 1)\n",
    "      others += [x_trans]\n",
    "    \n",
    "    if others:\n",
    "      x = tf.concat([x, *others], -1)\n",
    "      \n",
    "    if FLAGS.use_mlp:\n",
    "      x = self.mlp(x)\n",
    "    x = self.dense(x)\n",
    "    # tf.print(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def xlm_model():\n",
    "  inputs = {\n",
    "            'input_word_ids': Input(shape=(FLAGS.max_len,), dtype=tf.int32, name=\"input_word_ids\"),\n",
    "            'lang_': Input(shape=(1,), dtype=tf.int32, name=\"lang_\"),\n",
    "            'src_': Input(shape=(1,), dtype=tf.int32, name=\"src_\"),\n",
    "            'trans': Input(shape=(1,), dtype=tf.int32, name=\"trans\"),\n",
    "            }\n",
    "  out = XlmModel()(inputs)\n",
    "\n",
    "  model = keras.Model(inputs=list(inputs.values()), outputs=out)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = xlm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 16:52:09 0:00:00 fcntl.floc with lock_file /root/.melt.lock (If hang here means other programs calling melt.init have not finished yet)\n",
      "2020-04-24 16:52:10 0:00:01 Tf dataset and Tf model train in Eager mode, keras 1, distributed:False\n",
      "2020-04-24 16:52:10 0:00:01 log_level: 20 (try --debug to show more or --log_level=(> 20) to show less(no INFO), try --verbose to show train/valid loss intervaly)\n",
      "2020-04-24 16:52:10 0:00:01 batch_size: 256 eval_batch_size: 256 batch_size_per_gpu: 32 num_gpus: 8 gpu: [0, 1, 2, 3, 4, 5, 6, 7] CUDA_VISIABLE_DEVICES=[] work_mode: train distributed: False horovod: False\n",
      "2020-04-24 16:52:25 0:00:15 model: [toxic-mix-lang-mlp] model_dir: [../working/exps/toxic-mix-lang-mlp]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "import os\n",
    "import melt\n",
    "\n",
    "fit = melt.fit\n",
    "melt.init()\n",
    "loss_fn = get_loss_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 16:52:26 0:00:16 load xlm_model from ../input/tf-xlm-roberta-base/ start\n",
      "I0424 16:52:26.191782 139650726364992 configuration_utils.py:281] loading configuration file ../input/tf-xlm-roberta-base/config.json\n",
      "I0424 16:52:26.193006 139650726364992 configuration_utils.py:319] Model config XLMRobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "I0424 16:52:26.194247 139650726364992 modeling_tf_utils.py:388] loading weights file ../input/tf-xlm-roberta-base/tf_model.h5\n",
      "2020-04-24 16:52:44 0:00:35 load xlm_model from ../input/tf-xlm-roberta-base/ duration: 18.673165798187256\n"
     ]
    }
   ],
   "source": [
    "strategy = melt.distributed.get_strategy()\n",
    "with strategy.scope():\n",
    "  model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model=model):\n",
    "  fit(model,  \n",
    "      loss_fn,\n",
    "      Dataset,\n",
    "      eval_fn=evaluate,\n",
    "      eval_keys=['lang'],\n",
    "      )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MirroredVariable:{\n",
       "  0: <tf.Variable 'xlm_model/dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       "  1: <tf.Variable 'xlm_model/dense_2/bias/replica_1:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       "  2: <tf.Variable 'xlm_model/dense_2/bias/replica_2:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       "  3: <tf.Variable 'xlm_model/dense_2/bias/replica_3:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       "  4: <tf.Variable 'xlm_model/dense_2/bias/replica_4:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       "  5: <tf.Variable 'xlm_model/dense_2/bias/replica_5:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       "  6: <tf.Variable 'xlm_model/dense_2/bias/replica_6:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       "  7: <tf.Variable 'xlm_model/dense_2/bias/replica_7:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 16:52:52 0:00:42 -------Round: 0 mode: None train_input:[jigsaw-toxic-comment-train-google-fr-cleaned] valid_input:[validation] train_dirs:[8] valid_dir: ../input/tfrecords/xlm/validation\n",
      "2020-04-24 16:52:52 0:00:42 --start_hour=jigsaw-toxic-comment-train --end_hour=validation root: ../input/tfrecords/xlm\n",
      "2020-04-24 16:52:55 0:00:45 Model: \"tf_roberta_model\"\n",
      "2020-04-24 16:52:55 0:00:45 _________________________________________________________________\n",
      "2020-04-24 16:52:55 0:00:45 Layer (type)                 Output Shape              Param #   \n",
      "2020-04-24 16:52:55 0:00:45 =================================================================\n",
      "2020-04-24 16:52:55 0:00:45 roberta (TFRobertaMainLayer) ((None, 192, 768), (None, 278043648 \n",
      "2020-04-24 16:52:55 0:00:45 =================================================================\n",
      "2020-04-24 16:52:55 0:00:45 Total params: 278,043,648\n",
      "2020-04-24 16:52:55 0:00:45 Trainable params: 278,043,648\n",
      "2020-04-24 16:52:55 0:00:45 Non-trainable params: 0\n",
      "2020-04-24 16:52:55 0:00:45 _________________________________________________________________\n",
      "2020-04-24 16:52:55 0:00:45 Model: \"xlm_model\"\n",
      "2020-04-24 16:52:55 0:00:45 _________________________________________________________________\n",
      "2020-04-24 16:52:55 0:00:45 Layer (type)                 Output Shape              Param #   \n",
      "2020-04-24 16:52:55 0:00:45 =================================================================\n",
      "2020-04-24 16:52:55 0:00:45 tf_roberta_model (TFRobertaM ((None, 192, 768), (None, 278043648 \n",
      "2020-04-24 16:52:55 0:00:45 _________________________________________________________________\n",
      "2020-04-24 16:52:55 0:00:45 lang_emb (Embedding)         (None, 1, 32)             224       \n",
      "2020-04-24 16:52:55 0:00:45 _________________________________________________________________\n",
      "2020-04-24 16:52:55 0:00:45 mlp (MLP)                    multiple                  237952    \n",
      "2020-04-24 16:52:55 0:00:45 _________________________________________________________________\n",
      "2020-04-24 16:52:55 0:00:45 dense_2 (Dense)              multiple                  129       \n",
      "2020-04-24 16:52:55 0:00:45 =================================================================\n",
      "2020-04-24 16:52:55 0:00:45 Total params: 278,281,953\n",
      "2020-04-24 16:52:55 0:00:45 Trainable params: 278,281,953\n",
      "2020-04-24 16:52:55 0:00:45 Non-trainable params: 0\n",
      "2020-04-24 16:52:55 0:00:45 _________________________________________________________________\n",
      "2020-04-24 16:52:55 0:00:45 Model: \"model\"\n",
      "2020-04-24 16:52:55 0:00:45 __________________________________________________________________________________________________\n",
      "2020-04-24 16:52:55 0:00:45 Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "2020-04-24 16:52:55 0:00:45 ==================================================================================================\n",
      "2020-04-24 16:52:55 0:00:45 input_word_ids (InputLayer)     [(None, 192)]        0                                            \n",
      "2020-04-24 16:52:55 0:00:45 __________________________________________________________________________________________________\n",
      "2020-04-24 16:52:55 0:00:45 lang_ (InputLayer)              [(None, 1)]          0                                            \n",
      "2020-04-24 16:52:55 0:00:45 __________________________________________________________________________________________________\n",
      "2020-04-24 16:52:55 0:00:45 src_ (InputLayer)               [(None, 1)]          0                                            \n",
      "2020-04-24 16:52:55 0:00:45 __________________________________________________________________________________________________\n",
      "2020-04-24 16:52:55 0:00:45 trans (InputLayer)              [(None, 1)]          0                                            \n",
      "2020-04-24 16:52:55 0:00:45 __________________________________________________________________________________________________\n",
      "2020-04-24 16:52:55 0:00:45 xlm_model (XlmModel)            (None, 1)            278281953   input_word_ids[0][0]             \n",
      "2020-04-24 16:52:55 0:00:45                                                                  lang_[0][0]                      \n",
      "2020-04-24 16:52:55 0:00:45                                                                  src_[0][0]                       \n",
      "2020-04-24 16:52:55 0:00:45                                                                  trans[0][0]                      \n",
      "2020-04-24 16:52:55 0:00:45 ==================================================================================================\n",
      "2020-04-24 16:52:55 0:00:46 Total params: 278,281,953\n",
      "2020-04-24 16:52:55 0:00:46 Trainable params: 278,281,953\n",
      "2020-04-24 16:52:55 0:00:46 Non-trainable params: 0\n",
      "2020-04-24 16:52:55 0:00:46 __________________________________________________________________________________________________\n",
      "2020-04-24 16:52:55 0:00:46 num_train_examples: 1563650 num_steps_per_epoch: 6109\n",
      "2020-04-24 16:52:55 0:00:46 num_valid_examples: 8000  \n",
      "2020-04-24 16:52:55 0:00:46 num_test_examples: 63812   \n",
      "2020-04-24 16:52:56 0:00:46 round: 0 optimizer: <husky.optimization.AdamWeightDecay object at 0x7f00585d5e10> lr: <husky.optimization.WarmUp object at 0x7f005855b2b0> init_lr: 3e-05\n",
      "2020-04-24 16:52:56 0:00:46 total_steps: 6109 num_warmup_steps: 610 end_lr: 0.0\n",
      "2020-04-24 16:52:56 0:00:46 latest ckpt to restore: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "   1/6109 [..............................] - ETA: 3s - loss: 0.8093 - acc: 0.0820 - auc: 0.3967"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 16:57:05 0:04:56 Model total training parameters is: 278281953 with initial l2: 0.016124955109827047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 609/6109 [=>............................] - ETA: 1:44:17 - loss: 0.2930 - acc: 0.8783 - auc: 0.7543"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|##########| 32/32 [00:00<00:00, 423.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " steps/s: 0.6 insts/s: 165.5 1epoch: 2.63h "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 7s 222ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 17:09:18 0:17:08 preds:\n",
      " +------+--------+----------+------------+\n",
      "|   id | lang   |   y_true |       pred |\n",
      "|------+--------+----------+------------|\n",
      "| 2951 | tr     |        0 | 0.0136119  |\n",
      "| 2914 | it     |        0 | 0.006891   |\n",
      "| 6096 | es     |        0 | 0.00192714 |\n",
      "| 4350 | es     |        0 | 0.497786   |\n",
      "| 1651 | it     |        0 | 0.110856   |\n",
      "| 1131 | tr     |        1 | 0.870933   |\n",
      "|  385 | it     |        1 | 0.312728   |\n",
      "| 6904 | it     |        1 | 0.107637   |\n",
      "| 1526 | it     |        1 | 0.142526   |\n",
      "| 4644 | es     |        1 | 0.212619   |\n",
      "+------+--------+----------+------------+\n",
      "2020-04-24 17:09:18 0:17:08 eval_step: 1 step: 610 epoch: 0.1\n",
      "2020-04-24 17:09:18 0:17:08 valid_metrics:\n",
      " +---------+-----------+----------+----------+----------+---------+\n",
      "|    loss |   auc/all |   auc/es |   auc/it |   auc/tr |   insts |\n",
      "|---------+-----------+----------+----------+----------+---------|\n",
      "| 0.30068 |  0.883743 | 0.865753 | 0.837934 |  0.94094 |    8000 |\n",
      "+---------+-----------+----------+----------+----------+---------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1219/6109 [====>.........................] - ETA: 1:35:37 - loss: 0.2240 - acc: 0.9087 - auc: 0.8684"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|##########| 32/32 [00:00<00:00, 414.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " steps/s: 0.8 insts/s: 211.8 1epoch: 2.05h "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 7s 221ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 17:21:36 0:29:27 preds:\n",
      " +------+--------+----------+-------------+\n",
      "|   id | lang   |   y_true |        pred |\n",
      "|------+--------+----------+-------------|\n",
      "| 2880 | tr     |        0 | 0.0960677   |\n",
      "|  808 | es     |        0 | 0.000508088 |\n",
      "| 4285 | tr     |        0 | 0.00136152  |\n",
      "|  884 | it     |        0 | 0.0639645   |\n",
      "| 6115 | it     |        0 | 0.00580581  |\n",
      "|  931 | tr     |        1 | 0.227404    |\n",
      "| 5122 | es     |        1 | 0.906868    |\n",
      "| 3055 | it     |        1 | 0.0175131   |\n",
      "| 6098 | es     |        1 | 0.174227    |\n",
      "| 2658 | it     |        1 | 0.471229    |\n",
      "+------+--------+----------+-------------+\n",
      "2020-04-24 17:21:36 0:29:27 eval_step: 2 step: 1220 epoch: 0.2\n",
      "2020-04-24 17:21:36 0:29:27 valid_metrics:\n",
      " +----------+-----------+----------+----------+----------+---------+\n",
      "|     loss |   auc/all |   auc/es |   auc/it |   auc/tr |   insts |\n",
      "|----------+-----------+----------+----------+----------+---------|\n",
      "| 0.256143 |  0.910806 | 0.885204 | 0.878913 | 0.960928 |    8000 |\n",
      "+----------+-----------+----------+----------+----------+---------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1829/6109 [=======>......................] - ETA: 1:24:33 - loss: 0.1918 - acc: 0.9221 - auc: 0.9073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|##########| 32/32 [00:00<00:00, 415.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " steps/s: 0.8 insts/s: 211.6 1epoch: 2.05h "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 7s 221ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 17:33:55 0:41:45 preds:\n",
      " +------+--------+----------+-------------+\n",
      "|   id | lang   |   y_true |        pred |\n",
      "|------+--------+----------+-------------|\n",
      "| 6142 | it     |        0 | 0.000326258 |\n",
      "| 2788 | es     |        0 | 0.000885213 |\n",
      "| 2705 | tr     |        0 | 0.000234272 |\n",
      "| 2933 | es     |        0 | 6.05794e-05 |\n",
      "| 7299 | tr     |        0 | 0.666078    |\n",
      "| 7801 | es     |        1 | 0.0313835   |\n",
      "| 4110 | it     |        1 | 0.442508    |\n",
      "| 3438 | it     |        1 | 0.0477879   |\n",
      "| 4349 | es     |        1 | 0.114709    |\n",
      "| 1186 | es     |        1 | 0.0592334   |\n",
      "+------+--------+----------+-------------+\n",
      "2020-04-24 17:33:55 0:41:45 eval_step: 3 step: 1830 epoch: 0.3\n",
      "2020-04-24 17:33:55 0:41:45 valid_metrics:\n",
      " +----------+-----------+----------+----------+----------+---------+\n",
      "|     loss |   auc/all |   auc/es |   auc/it |   auc/tr |   insts |\n",
      "|----------+-----------+----------+----------+----------+---------|\n",
      "| 0.283396 |  0.915391 | 0.882006 | 0.893562 | 0.965403 |    8000 |\n",
      "+----------+-----------+----------+----------+----------+---------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2439/6109 [==========>...................] - ETA: 1:12:53 - loss: 0.1730 - acc: 0.9300 - auc: 0.9268"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|##########| 32/32 [00:00<00:00, 475.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " steps/s: 0.8 insts/s: 211.5 1epoch: 2.05h "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 7s 224ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 17:46:14 0:54:05 preds:\n",
      " +------+--------+----------+-------------+\n",
      "|   id | lang   |   y_true |        pred |\n",
      "|------+--------+----------+-------------|\n",
      "| 6828 | tr     |        0 | 0.000713863 |\n",
      "| 2492 | it     |        0 | 0.503016    |\n",
      "| 2706 | it     |        0 | 0.0154451   |\n",
      "| 4232 | tr     |        0 | 0.187822    |\n",
      "|  907 | es     |        0 | 0.239681    |\n",
      "| 3491 | tr     |        1 | 0.993734    |\n",
      "| 6531 | tr     |        1 | 0.36878     |\n",
      "| 1414 | es     |        1 | 0.658977    |\n",
      "| 5032 | tr     |        1 | 0.0621293   |\n",
      "| 5224 | it     |        1 | 0.527888    |\n",
      "+------+--------+----------+-------------+\n",
      "2020-04-24 17:46:14 0:54:05 eval_step: 4 step: 2440 epoch: 0.4\n",
      "2020-04-24 17:46:14 0:54:05 valid_metrics:\n",
      " +----------+-----------+----------+----------+----------+---------+\n",
      "|     loss |   auc/all |   auc/es |   auc/it |   auc/tr |   insts |\n",
      "|----------+-----------+----------+----------+----------+---------|\n",
      "| 0.249672 |  0.917079 | 0.892457 |  0.89038 | 0.961297 |    8000 |\n",
      "+----------+-----------+----------+----------+----------+---------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3049/6109 [=============>................] - ETA: 1:01:12 - loss: 0.1630 - acc: 0.9339 - auc: 0.9363"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|##########| 32/32 [00:00<00:00, 438.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " steps/s: 0.8 insts/s: 207.3 1epoch: 2.10h "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 7s 221ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 17:58:46 1:06:37 preds:\n",
      " +------+--------+----------+-------------+\n",
      "|   id | lang   |   y_true |        pred |\n",
      "|------+--------+----------+-------------|\n",
      "| 4509 | es     |        0 | 0.0399012   |\n",
      "| 6200 | it     |        0 | 0.00915893  |\n",
      "| 1229 | it     |        0 | 0.000269819 |\n",
      "| 7656 | it     |        0 | 0.000790305 |\n",
      "| 1657 | es     |        0 | 0.000171839 |\n",
      "| 5135 | it     |        1 | 0.962326    |\n",
      "| 6108 | it     |        1 | 0.702586    |\n",
      "| 3430 | es     |        1 | 0.140795    |\n",
      "| 5024 | es     |        1 | 0.997859    |\n",
      "| 6156 | es     |        1 | 0.570214    |\n",
      "+------+--------+----------+-------------+\n",
      "2020-04-24 17:58:46 1:06:37 eval_step: 5 step: 3050 epoch: 0.5\n",
      "2020-04-24 17:58:46 1:06:37 valid_metrics:\n",
      " +----------+-----------+----------+----------+----------+---------+\n",
      "|     loss |   auc/all |   auc/es |   auc/it |   auc/tr |   insts |\n",
      "|----------+-----------+----------+----------+----------+---------|\n",
      "| 0.248899 |  0.930576 | 0.920111 | 0.895427 | 0.971899 |    8000 |\n",
      "+----------+-----------+----------+----------+----------+---------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3659/6109 [================>.............] - ETA: 49:04 - loss: 0.1542 - acc: 0.9375 - auc: 0.9437"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|##########| 32/32 [00:00<00:00, 387.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " steps/s: 0.8 insts/s: 211.5 1epoch: 2.05h "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 7s 222ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 18:11:05 1:18:56 preds:\n",
      " +------+--------+----------+------------+\n",
      "|   id | lang   |   y_true |       pred |\n",
      "|------+--------+----------+------------|\n",
      "| 7417 | es     |        0 | 0.0197993  |\n",
      "| 6159 | tr     |        0 | 0.00198044 |\n",
      "| 6806 | it     |        0 | 0.415595   |\n",
      "| 5857 | es     |        0 | 0.00635843 |\n",
      "| 7529 | it     |        0 | 0.270013   |\n",
      "|  984 | it     |        1 | 0.998465   |\n",
      "|  798 | tr     |        1 | 0.737451   |\n",
      "| 1131 | tr     |        1 | 0.246212   |\n",
      "| 7776 | it     |        1 | 0.248872   |\n",
      "| 5287 | it     |        1 | 0.0217545  |\n",
      "+------+--------+----------+------------+\n",
      "2020-04-24 18:11:05 1:18:56 eval_step: 6 step: 3660 epoch: 0.6\n",
      "2020-04-24 18:11:05 1:18:56 valid_metrics:\n",
      " +---------+-----------+----------+----------+----------+---------+\n",
      "|    loss |   auc/all |   auc/es |   auc/it |   auc/tr |   insts |\n",
      "|---------+-----------+----------+----------+----------+---------|\n",
      "| 0.24867 |  0.930684 | 0.922962 | 0.894687 | 0.973296 |    8000 |\n",
      "+---------+-----------+----------+----------+----------+---------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4269/6109 [===================>..........] - ETA: 36:54 - loss: 0.1473 - acc: 0.9403 - auc: 0.9493"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|##########| 32/32 [00:00<00:00, 410.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " steps/s: 0.8 insts/s: 211.1 1epoch: 2.06h "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 7s 221ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 18:23:23 1:31:14 preds:\n",
      " +------+--------+----------+-------------+\n",
      "|   id | lang   |   y_true |        pred |\n",
      "|------+--------+----------+-------------|\n",
      "| 5060 | tr     |        0 | 0.0912872   |\n",
      "| 5651 | tr     |        0 | 2.2799e-05  |\n",
      "| 3652 | it     |        0 | 0.019816    |\n",
      "| 4753 | tr     |        0 | 1.64323e-05 |\n",
      "| 5296 | it     |        0 | 0.00020995  |\n",
      "| 4739 | tr     |        1 | 0.738841    |\n",
      "| 4780 | tr     |        1 | 0.496445    |\n",
      "| 3659 | tr     |        1 | 0.436619    |\n",
      "| 7650 | it     |        1 | 0.000501685 |\n",
      "|  485 | tr     |        1 | 0.682572    |\n",
      "+------+--------+----------+-------------+\n",
      "2020-04-24 18:23:23 1:31:14 eval_step: 7 step: 4270 epoch: 0.7\n",
      "2020-04-24 18:23:23 1:31:14 valid_metrics:\n",
      " +----------+-----------+----------+----------+----------+---------+\n",
      "|     loss |   auc/all |   auc/es |   auc/it |   auc/tr |   insts |\n",
      "|----------+-----------+----------+----------+----------+---------|\n",
      "| 0.249895 |  0.933488 | 0.923668 | 0.898252 | 0.974629 |    8000 |\n",
      "+----------+-----------+----------+----------+----------+---------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4879/6109 [======================>.......] - ETA: 24:41 - loss: 0.1418 - acc: 0.9426 - auc: 0.9535"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|##########| 32/32 [00:00<00:00, 442.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " steps/s: 0.8 insts/s: 211.8 1epoch: 2.05h "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 7s 221ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 18:35:41 1:43:31 preds:\n",
      " +------+--------+----------+-------------+\n",
      "|   id | lang   |   y_true |        pred |\n",
      "|------+--------+----------+-------------|\n",
      "| 5699 | it     |        0 | 0.0115761   |\n",
      "| 1886 | tr     |        0 | 0.000148314 |\n",
      "| 4043 | tr     |        0 | 0.000520443 |\n",
      "|  610 | it     |        0 | 0.012094    |\n",
      "| 3208 | tr     |        0 | 2.07332e-05 |\n",
      "| 2254 | es     |        1 | 0.5587      |\n",
      "| 4019 | es     |        1 | 0.137026    |\n",
      "| 7712 | es     |        1 | 0.114486    |\n",
      "| 4984 | tr     |        1 | 0.207038    |\n",
      "| 3644 | it     |        1 | 0.800406    |\n",
      "+------+--------+----------+-------------+\n",
      "2020-04-24 18:35:41 1:43:31 eval_step: 8 step: 4880 epoch: 0.8\n",
      "2020-04-24 18:35:41 1:43:31 valid_metrics:\n",
      " +----------+-----------+----------+----------+----------+---------+\n",
      "|     loss |   auc/all |   auc/es |   auc/it |   auc/tr |   insts |\n",
      "|----------+-----------+----------+----------+----------+---------|\n",
      "| 0.257072 |  0.930827 |  0.92344 | 0.895851 | 0.971241 |    8000 |\n",
      "+----------+-----------+----------+----------+----------+---------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5489/6109 [=========================>....] - ETA: 12:26 - loss: 0.1379 - acc: 0.9441 - auc: 0.9563"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|##########| 32/32 [00:00<00:00, 414.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " steps/s: 0.8 insts/s: 211.9 1epoch: 2.05h "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 7s 221ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 18:47:58 1:55:48 preds:\n",
      " +------+--------+----------+-------------+\n",
      "|   id | lang   |   y_true |        pred |\n",
      "|------+--------+----------+-------------|\n",
      "| 7658 | tr     |        0 | 0.00854888  |\n",
      "|  224 | es     |        0 | 0.127938    |\n",
      "| 3268 | tr     |        0 | 5.63025e-05 |\n",
      "| 5596 | tr     |        0 | 3.1975e-05  |\n",
      "| 4565 | it     |        0 | 0.00221246  |\n",
      "|  903 | tr     |        1 | 0.225991    |\n",
      "| 7297 | tr     |        1 | 0.229415    |\n",
      "| 3188 | es     |        1 | 0.995859    |\n",
      "| 3252 | it     |        1 | 0.980154    |\n",
      "| 7096 | it     |        1 | 0.899513    |\n",
      "+------+--------+----------+-------------+\n",
      "2020-04-24 18:47:58 1:55:48 eval_step: 9 step: 5490 epoch: 0.9\n",
      "2020-04-24 18:47:58 1:55:48 valid_metrics:\n",
      " +----------+-----------+----------+----------+----------+---------+\n",
      "|     loss |   auc/all |   auc/es |   auc/it |   auc/tr |   insts |\n",
      "|----------+-----------+----------+----------+----------+---------|\n",
      "| 0.245715 |   0.93185 | 0.923388 | 0.896974 | 0.975333 |    8000 |\n",
      "+----------+-----------+----------+----------+----------+---------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6099/6109 [============================>.] - ETA: 12s - loss: 0.1342 - acc: 0.9455 - auc: 0.9590"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|##########| 32/32 [00:00<00:00, 406.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " steps/s: 0.8 insts/s: 211.7 1epoch: 2.05h "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 7s 221ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 19:00:18 2:08:08 preds:\n",
      " +------+--------+----------+-------------+\n",
      "|   id | lang   |   y_true |        pred |\n",
      "|------+--------+----------+-------------|\n",
      "| 4708 | it     |        0 | 0.130909    |\n",
      "| 6810 | tr     |        0 | 0.00186045  |\n",
      "| 5878 | tr     |        0 | 0.00241271  |\n",
      "| 2370 | tr     |        0 | 3.68447e-05 |\n",
      "| 1605 | es     |        0 | 9.41348e-06 |\n",
      "| 3156 | tr     |        1 | 0.336015    |\n",
      "| 2867 | es     |        1 | 0.0330017   |\n",
      "| 2756 | it     |        1 | 0.00854217  |\n",
      "| 6387 | tr     |        1 | 0.904889    |\n",
      "|  992 | it     |        1 | 0.949045    |\n",
      "+------+--------+----------+-------------+\n",
      "2020-04-24 19:00:18 2:08:08 eval_step: 10 step: 6100 epoch: 1.0\n",
      "2020-04-24 19:00:18 2:08:08 valid_metrics:\n",
      " +----------+-----------+----------+----------+----------+---------+\n",
      "|     loss |   auc/all |   auc/es |   auc/it |   auc/tr |   insts |\n",
      "|----------+-----------+----------+----------+----------+---------|\n",
      "| 0.251844 |  0.931738 | 0.923296 | 0.897653 | 0.974991 |    8000 |\n",
      "+----------+-----------+----------+----------+----------+---------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6109/6109 [==============================] - 7403s 1s/step - loss: 0.1341 - acc: 0.9455 - auc: 0.9591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|##########| 250/250 [00:00<00:00, 508.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 58s 232ms/step\n"
     ]
    }
   ],
   "source": [
    "FLAGS.train_input=','.join([f'{RECORDS_PATH}/{mark}/jigsaw-toxic-comment-train',\n",
    "                           f'{RECORDS_PATH}/{mark}/jigsaw-jigsaw-unintended-bias-train',\n",
    "                           f'{RECORDS_PATH}/{mark}/jigsaw-toxic-comment-train-google-tr-cleaned',\n",
    "                           f'{RECORDS_PATH}/{mark}/jigsaw-toxic-comment-train-google-it-cleaned',\n",
    "                           f'{RECORDS_PATH}/{mark}/jigsaw-toxic-comment-train-google-es-cleaned',\n",
    "                           f'{RECORDS_PATH}/{mark}/jigsaw-toxic-comment-train-google-ru-cleaned',\n",
    "                           f'{RECORDS_PATH}/{mark}/jigsaw-toxic-comment-train-google-pt-cleaned',\n",
    "                           f'{RECORDS_PATH}/{mark}/jigsaw-toxic-comment-train-google-fr-cleaned'])\n",
    "FLAGS.learning_rate=3e-5\n",
    "FLAGS.opt_epsilon=1e-8\n",
    "FLAGS.num_epochs=1\n",
    "FLAGS.vie=0.1\n",
    "FLAGS.do_test=1\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odir = '../working/exps/toxic-mix-lang'\n",
    "# os.system(f'mkdir -p f{odir}')\n",
    "# model.save_weights(f'{odir}/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../working/exps/v1/base/infos/test_1.csv does not exist: '../working/exps/v1/base/infos/test_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e1cfe782ce94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../working/exps/v1/base/infos/test_1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../working/exps/v1/base/infos/test_1.csv does not exist: '../working/exps/v1/base/infos/test_1.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../working/exps/v1/base/infos/test_1.csv')\n",
    "df.to_csv('./submission.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "try:\n",
    "#   optimizer = gezi.get('optimizer')\n",
    "#   ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "#   manager = tf.train.CheckpointManager(ckpt, FLAGS.model_dir, max_to_keep=1, checkpoint_name='model.ckpt')\n",
    "#   manager.save()\n",
    "  model.save_weights(f'{FLAGS.model_dir}/model_weight.h5')\n",
    "  os.system(f'mkdir -p {FLAGS.model_dir}/tf-xlm-roberta-base')\n",
    "  model.layers[-1].layers[0].save_pretrained(f'{FLAGS.model_dir}/tf-xlm-roberta-base')\n",
    "except Exception:\n",
    "  print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
