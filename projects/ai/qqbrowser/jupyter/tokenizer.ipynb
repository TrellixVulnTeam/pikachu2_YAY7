{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os \n",
    "sys.path.append('..')\n",
    "sys.path.append('../../../../utils')\n",
    "sys.path.append('../../../../third')\n",
    "import glob\n",
    "import pandas as pd\n",
    "import gezi\n",
    "from gezi import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('../input/tfrecords1.train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid</th>\n",
       "      <th>title_ids</th>\n",
       "      <th>tag_id</th>\n",
       "      <th>num_frames</th>\n",
       "      <th>title_maxlen</th>\n",
       "      <th>tag_maxlen</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tag_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'1003118914939508142'</td>\n",
       "      <td>101,1599,3614,7770,7716,2227,2218,2110,6629,33...</td>\n",
       "      <td>88,220,175,583,152,4</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'1003118921129803182'</td>\n",
       "      <td>101,2157,7027,782,1914,8024,677,1329,2792,8024...</td>\n",
       "      <td>106,23,20,198,306</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>83</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'1003118925231764910'</td>\n",
       "      <td>101,6821,2767,2864,2533,4696,6589,6756,1557,80...</td>\n",
       "      <td>872,279,14,201</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>52</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'1003118938732506490'</td>\n",
       "      <td>101,2797,2833,749,8043,711,784,720,833,6821,34...</td>\n",
       "      <td>72,43,172</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'1003118948438515118'</td>\n",
       "      <td>101,872,812,6820,6381,2533,4313,7987,1408,1914...</td>\n",
       "      <td>8356,279,14,201</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>148</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      vid                                          title_ids  \\\n",
       "0  b'1003118914939508142'  101,1599,3614,7770,7716,2227,2218,2110,6629,33...   \n",
       "1  b'1003118921129803182'  101,2157,7027,782,1914,8024,677,1329,2792,8024...   \n",
       "2  b'1003118925231764910'  101,6821,2767,2864,2533,4696,6589,6756,1557,80...   \n",
       "3  b'1003118938732506490'  101,2797,2833,749,8043,711,784,720,833,6821,34...   \n",
       "4  b'1003118948438515118'  101,872,812,6820,6381,2533,4313,7987,1408,1914...   \n",
       "\n",
       "                 tag_id  num_frames  title_maxlen  tag_maxlen  title_len  \\\n",
       "0  88,220,175,583,152,4          32            32          29         57   \n",
       "1     106,23,20,198,306          11            32          29         83   \n",
       "2        872,279,14,201          15            32          29         52   \n",
       "3             72,43,172          13            32          29         52   \n",
       "4       8356,279,14,201          12            32          29        148   \n",
       "\n",
       "   tag_len  \n",
       "0       20  \n",
       "1       17  \n",
       "2       14  \n",
       "3        9  \n",
       "4       15  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = pd.read_csv('../input/tfrecords2.train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid</th>\n",
       "      <th>title_ids</th>\n",
       "      <th>tag_id</th>\n",
       "      <th>num_frames</th>\n",
       "      <th>title_maxlen</th>\n",
       "      <th>tag_maxlen</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tag_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'1003118914939508142'</td>\n",
       "      <td>101,1599,3614,7770,7716,2227,2218,2110,6629,33...</td>\n",
       "      <td>88,220,175,583,152,4</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'1003118921129803182'</td>\n",
       "      <td>101,2157,7027,782,1914,8024,677,1329,2792,8024...</td>\n",
       "      <td>106,23,20,198,306</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>83</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'1003118921129803182'</td>\n",
       "      <td>101,2157,7027,782,1914,8024,677,1329,2792,8024...</td>\n",
       "      <td>106,23,20,198,306</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>83</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'1003118925231764910'</td>\n",
       "      <td>101,6821,2767,2864,2533,4696,6589,6756,1557,80...</td>\n",
       "      <td>872,279,14,201</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>52</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'1003118925231764910'</td>\n",
       "      <td>101,6821,2767,2864,2533,4696,6589,6756,1557,80...</td>\n",
       "      <td>872,279,14,201</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>52</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      vid                                          title_ids  \\\n",
       "0  b'1003118914939508142'  101,1599,3614,7770,7716,2227,2218,2110,6629,33...   \n",
       "1  b'1003118921129803182'  101,2157,7027,782,1914,8024,677,1329,2792,8024...   \n",
       "2  b'1003118921129803182'  101,2157,7027,782,1914,8024,677,1329,2792,8024...   \n",
       "3  b'1003118925231764910'  101,6821,2767,2864,2533,4696,6589,6756,1557,80...   \n",
       "4  b'1003118925231764910'  101,6821,2767,2864,2533,4696,6589,6756,1557,80...   \n",
       "\n",
       "                 tag_id  num_frames  title_maxlen  tag_maxlen  title_len  \\\n",
       "0  88,220,175,583,152,4          32            32          29         57   \n",
       "1     106,23,20,198,306          11            32          29         83   \n",
       "2     106,23,20,198,306          11            32          29         83   \n",
       "3        872,279,14,201          15            32          29         52   \n",
       "4        872,279,14,201          15            32          29         52   \n",
       "\n",
       "   tag_len  \n",
       "0       20  \n",
       "1       17  \n",
       "2       17  \n",
       "3       14  \n",
       "4       14  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = pd.read_csv('../input/tfrecords9_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid</th>\n",
       "      <th>title_ids</th>\n",
       "      <th>tag_id</th>\n",
       "      <th>num_frames</th>\n",
       "      <th>title_maxlen</th>\n",
       "      <th>tag_maxlen</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tag_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'1003119227050319226'</td>\n",
       "      <td>101,1914,1216,5543,1147,5831,1690,1147,692,114...</td>\n",
       "      <td>52,3467</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>98</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'1003119400696182138'</td>\n",
       "      <td>101,3791,2360,2233,4197,1469,2769,4991,3070,102</td>\n",
       "      <td>9,7,13,1219,108,26</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>47</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'1003119408361610618'</td>\n",
       "      <td>101,1139,3341,4381,2207,2111,4960,4197,6206,67...</td>\n",
       "      <td>48,15,101,102</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>139</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'1003119413407319470'</td>\n",
       "      <td>101,3136,4906,741,2466,4638,1327,2482,102</td>\n",
       "      <td>2573,2301,4652</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'1003119513611890042'</td>\n",
       "      <td>101,5632,4197,5143,7831,5709,3717,3362,6028,51...</td>\n",
       "      <td>111,257,6037</td>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>52</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      vid                                          title_ids  \\\n",
       "0  b'1003119227050319226'  101,1914,1216,5543,1147,5831,1690,1147,692,114...   \n",
       "1  b'1003119400696182138'    101,3791,2360,2233,4197,1469,2769,4991,3070,102   \n",
       "2  b'1003119408361610618'  101,1139,3341,4381,2207,2111,4960,4197,6206,67...   \n",
       "3  b'1003119413407319470'          101,3136,4906,741,2466,4638,1327,2482,102   \n",
       "4  b'1003119513611890042'  101,5632,4197,5143,7831,5709,3717,3362,6028,51...   \n",
       "\n",
       "               tag_id  num_frames  title_maxlen  tag_maxlen  title_len  \\\n",
       "0             52,3467           7            32          29         98   \n",
       "1  9,7,13,1219,108,26          14            32          29         47   \n",
       "2       48,15,101,102          16            32          29        139   \n",
       "3      2573,2301,4652          14            32          29         41   \n",
       "4        111,257,6037          17            32          29         52   \n",
       "\n",
       "   tag_len  \n",
       "0        7  \n",
       "1       18  \n",
       "2       13  \n",
       "3       14  \n",
       "4       12  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5507979 , 0.70814782, 0.29090474, 0.51082761, 0.89294695])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89629309, 0.12558531, 0.20724288, 0.0514672 , 0.44080984])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5507979 , 0.70814782, 0.29090474, 0.51082761, 0.89294695])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89629309, 0.12558531, 0.20724288, 0.0514672 , 0.44080984])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('../input/chinese_L-12_H-768_A-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 872, 1962, 1557, 102]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('你好啊')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '酷路泽4000改装版，这外观效果真不赖，透露着王者的气息！'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '一言不合就开干，狮子好凶猛！'\n",
    "tokens = ['一', '言', '不', '合', '就', '开', '干', '，', '狮', '子', '好', '凶', '猛', '！']\n",
    "words = ['一言不合', '就', '开干', '，', '狮子', '好', '凶猛', '！']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 6999,\n",
       " 6662,\n",
       " 3813,\n",
       " 8442,\n",
       " 3121,\n",
       " 6163,\n",
       " 4276,\n",
       " 8024,\n",
       " 6821,\n",
       " 1912,\n",
       " 6225,\n",
       " 3126,\n",
       " 3362,\n",
       " 4696,\n",
       " 679,\n",
       " 6609,\n",
       " 8024,\n",
       " 6851,\n",
       " 7463,\n",
       " 4708,\n",
       " 4374,\n",
       " 5442,\n",
       " 4638,\n",
       " 3698,\n",
       " 2622,\n",
       " 8013,\n",
       " 102]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 酷 路 泽 4000 改 装 版 ， 这 外 观 效 果 真 不 赖 ， 透 露 着 王 者 的 气 息 ！ [SEP]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['酷',\n",
       " '路',\n",
       " '泽',\n",
       " '4000',\n",
       " '改',\n",
       " '装',\n",
       " '版',\n",
       " '，',\n",
       " '这',\n",
       " '外',\n",
       " '观',\n",
       " '效',\n",
       " '果',\n",
       " '真',\n",
       " '不',\n",
       " '赖',\n",
       " '，',\n",
       " '透',\n",
       " '露',\n",
       " '着',\n",
       " '王',\n",
       " '者',\n",
       " '的',\n",
       " '气',\n",
       " '息',\n",
       " '！']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '酷',\n",
       " '路',\n",
       " '泽',\n",
       " '4000',\n",
       " '改',\n",
       " '装',\n",
       " '版',\n",
       " '，',\n",
       " '这',\n",
       " '外',\n",
       " '观',\n",
       " '效',\n",
       " '果',\n",
       " '真',\n",
       " '不',\n",
       " '赖',\n",
       " '，',\n",
       " '透',\n",
       " '露',\n",
       " '着',\n",
       " '王',\n",
       " '者',\n",
       " '的',\n",
       " '气',\n",
       " '息',\n",
       " '！',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'CLS',\n",
       " ']',\n",
       " ' ',\n",
       " '酷',\n",
       " ' ',\n",
       " '路',\n",
       " ' ',\n",
       " '泽',\n",
       " ' ',\n",
       " '4000',\n",
       " ' ',\n",
       " '改',\n",
       " ' ',\n",
       " '装',\n",
       " ' ',\n",
       " '版',\n",
       " ' ',\n",
       " '，',\n",
       " ' ',\n",
       " '这',\n",
       " ' ',\n",
       " '外',\n",
       " ' ',\n",
       " '观',\n",
       " ' ',\n",
       " '效',\n",
       " ' ',\n",
       " '果',\n",
       " ' ',\n",
       " '真',\n",
       " ' ',\n",
       " '不',\n",
       " ' ',\n",
       " '赖',\n",
       " ' ',\n",
       " '，',\n",
       " ' ',\n",
       " '透',\n",
       " ' ',\n",
       " '露',\n",
       " ' ',\n",
       " '着',\n",
       " ' ',\n",
       " '王',\n",
       " ' ',\n",
       " '者',\n",
       " ' ',\n",
       " '的',\n",
       " ' ',\n",
       " '气',\n",
       " ' ',\n",
       " '息',\n",
       " ' ',\n",
       " '！',\n",
       " ' ',\n",
       " '[',\n",
       " 'SEP',\n",
       " ']']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(jieba.cut(tokenizer.decode(tokenizer.encode(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2769, 1355, 4385, 1599, 3614, 100, 4638, 782, 6963, 2523, 1599, 3614, 100, 8024, 2218, 3683, 1963, 2769, 3221, 6821, 3416, 4638, 8013, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (0, 0)]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text,return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tokenizer(text, return_offsets_mapping=True)\n",
    "input_ids = res.input_ids\n",
    "offsets = res.offset_mapping\n",
    "tokens = [text[s:e] for s,e in offsets]\n",
    "tokens[0] = '[CLS]'\n",
    "tokens[-1] = '[SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '我',\n",
       " '发',\n",
       " '现',\n",
       " '喜',\n",
       " '欢',\n",
       " 'IU',\n",
       " '的',\n",
       " '人',\n",
       " '都',\n",
       " '很',\n",
       " '喜',\n",
       " '欢',\n",
       " 'BLACKPINK',\n",
       " '，',\n",
       " '就',\n",
       " '比',\n",
       " '如',\n",
       " '我',\n",
       " '是',\n",
       " '这',\n",
       " '样',\n",
       " '的',\n",
       " '！',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '一言不合', '就', '开干', '，', '狮子', '好', '凶猛', '！', '[SEP]']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_ = ['[CLS]', *words, '[SEP]']\n",
    "words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1 [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2 [1, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3 [1, 2, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4 [1, 2, 0, 0, 0, 3, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "5 [1, 2, 0, 0, 0, 3, 4, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "6 [1, 2, 0, 0, 0, 3, 4, 5, 6, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "7 [1, 2, 0, 0, 0, 3, 4, 5, 6, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "8 [1, 2, 0, 0, 0, 3, 4, 5, 6, 0, 7, 8, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-befa8bcfcd8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-5bb342643995>\u001b[0m in \u001b[0;36mwords_mask\u001b[0;34m(words, tokens)\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mlen__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mlen__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlen__\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mstart_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "words_mask(words_, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2769, 1355, 4385, 1599, 3614, 100, 4638, 782, 6963, 2523, 1599, 3614, 100, 8024, 2218, 3683, 1963, 2769, 3221, 6821, 3416, 4638, 8013, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (0, 0)]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text,return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "我\n",
      "发\n",
      "现\n",
      "喜\n",
      "欢\n",
      "IU\n",
      "的\n",
      "人\n",
      "都\n",
      "很\n",
      "喜\n",
      "欢\n",
      "BLACKPINK\n",
      "，\n",
      "就\n",
      "比\n",
      "如\n",
      "我\n",
      "是\n",
      "这\n",
      "样\n",
      "的\n",
      "！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s,e in tokenizer(text,return_offsets_mapping=True).offset_mapping:\n",
    "  print(text[s:e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "na\n",
      "n\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s,e in tokenizer('nan',return_offsets_mapping=True).offset_mapping:\n",
    "  print('nan'[s:e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'return_offsets_mapping': True} not recognized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['我',\n",
       " '发',\n",
       " '现',\n",
       " '喜',\n",
       " '欢',\n",
       " 'iu',\n",
       " '的',\n",
       " '人',\n",
       " '都',\n",
       " '很',\n",
       " '喜',\n",
       " '欢',\n",
       " 'black',\n",
       " '##pin',\n",
       " '##k',\n",
       " '，',\n",
       " '就',\n",
       " '比',\n",
       " '如',\n",
       " '我',\n",
       " '是',\n",
       " '这',\n",
       " '样',\n",
       " '的',\n",
       " '！']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '我发现喜欢IU的人都很喜欢BLACKPINK，就比如我是这样的！'\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '我',\n",
       " '发',\n",
       " '现',\n",
       " '喜',\n",
       " '欢',\n",
       " 'iu',\n",
       " '的',\n",
       " '人',\n",
       " '都',\n",
       " '很',\n",
       " '喜',\n",
       " '欢',\n",
       " 'black',\n",
       " '##pin',\n",
       " '##k',\n",
       " '，',\n",
       " '就',\n",
       " '比',\n",
       " '如',\n",
       " '我',\n",
       " '是',\n",
       " '这',\n",
       " '样',\n",
       " '的',\n",
       " '！',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_mask(words, tokens=None):\n",
    "#   if tokens:\n",
    "#     words_ = []\n",
    "#     i, j = 0, 0\n",
    "#     while i < len(words):\n",
    "#       print(i, j)\n",
    "#       if len(words[i]) < len(tokens[j]):\n",
    "#         k = i + 1\n",
    "#         while k < len(words):\n",
    "#           word = ''.join(words[i:k])\n",
    "#           print(i, len(word), len(tokens[j]), word, tokens[j])\n",
    "#           if len(word) == len(tokens[j]):\n",
    "#             words_.append(word)\n",
    "#             break\n",
    "#           else:\n",
    "#             k += 1\n",
    "#         i = k\n",
    "#         j += 1\n",
    "#       else:\n",
    "#         words_.append(words[i])\n",
    "#         j += len(words[i]) - len(tokens[j]) + 1\n",
    "#         i += 1\n",
    "#       print(words_)\n",
    "#     print(words_)\n",
    "#     words = words_\n",
    "                   \n",
    "  word_lens = [len(word) for word in words]\n",
    "  total_len = sum(word_lens) if not tokens else len(tokens)\n",
    "#   print(total_len)\n",
    "  start = 0\n",
    "  group = 0\n",
    "  mask_array = [0] * total_len\n",
    "  for i, len_ in enumerate(word_lens):\n",
    "    mask_array[start] = group + 1\n",
    "    group += 1\n",
    "#     print(i, mask_array)\n",
    "    if not tokens:\n",
    "      start += len_\n",
    "    else:\n",
    "      start_ = start\n",
    "      len__ = 0\n",
    "      while len__ != len_:\n",
    "        len__ += len(tokens[start_])\n",
    "        start_ += 1\n",
    "#         print(i, start_, len__, len_)\n",
    "      start = start_\n",
    "      \n",
    "  for i in range(total_len):\n",
    "    if mask_array[i] == 0:\n",
    "      mask_array[i] = mask_array[i - 1]\n",
    "  return mask_array\n",
    "\n",
    "def fix_words_mask(mask_array):\n",
    "  delta = 0\n",
    "  for i in range(len(mask_array)):\n",
    "    if i > 1:\n",
    "      delta = mask_array[i] - mask_array[i - 1]\n",
    "      if delta > 1:\n",
    "        for j in range(i, len(mask_array)):\n",
    "          mask_array[j] -= (delta - 1)\n",
    "        break\n",
    "  return mask_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', '乘', '风', '破', '浪', '的', '姐', '姐', '黄', '晓', '明', '初', '见', '姐', '姐', '团', 'E'] 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 2, 2, 3, 4, 4, 5, 5, 5, 6, 6, 7, 7, 8, 9]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '乘风破浪的姐姐 黄晓明初见姐姐团'\n",
    "# text = 'YOU DONT HAVE TO BE SORRY TO HAVE TO BE SORRY'\n",
    "# text = 'rosé专属BGM，她跑的时候真的就像公主啊！'\n",
    "# text = 'blackpink的歌词单独读起来这么搞笑的吗？哈哈哈'\n",
    "# text = 'nan'\n",
    "max_len = 32\n",
    "last_tokens = 10\n",
    "import jieba\n",
    "words = list(jieba.cut(text))\n",
    "words = [x for x in words if x.strip()]\n",
    "res = tokenizer(text, return_offsets_mapping=True)\n",
    "input_ids = res.input_ids\n",
    "offsets = res.offset_mapping\n",
    "tokens = [text[s:e] for s,e in offsets]\n",
    "tokens[0] = 'S'\n",
    "tokens[-1] = 'E'\n",
    "print(tokens, len(tokens))\n",
    "# print(tokens[18:])\n",
    "words_ = [tokens[0], *words, tokens[-1]]\n",
    "# print(words_)\n",
    "words_mask(words_, tokens)\n",
    "# words_mask(words_)\n",
    "# gezi.words_mask(words_, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我',\n",
       " '发现',\n",
       " '喜欢',\n",
       " 'IU',\n",
       " '的',\n",
       " '人',\n",
       " '都',\n",
       " '很',\n",
       " '喜欢',\n",
       " 'BLACKPINK',\n",
       " '，',\n",
       " '就',\n",
       " '比如',\n",
       " '我',\n",
       " '是',\n",
       " '这样',\n",
       " '的',\n",
       " '！']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x) for x in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2769,\n",
       " 1355,\n",
       " 4385,\n",
       " 1599,\n",
       " 3614,\n",
       " 100,\n",
       " 4638,\n",
       " 782,\n",
       " 6963,\n",
       " 2523,\n",
       " 1599,\n",
       " 3614,\n",
       " 100,\n",
       " 8024,\n",
       " 2218,\n",
       " 3683,\n",
       " 1963,\n",
       " 2769,\n",
       " 3221,\n",
       " 6821,\n",
       " 3416,\n",
       " 4638,\n",
       " 8013,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gezi.pad(tokenizer.encode(text), max_len, 0, last_tokens=last_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1 [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2 [1, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3 [1, 2, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4 [1, 2, 0, 0, 0, 3, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0]\n",
      "5 [1, 2, 0, 0, 0, 3, 4, 0, 5, 6, 0, 0, 0, 0, 0, 0]\n",
      "6 [1, 2, 0, 0, 0, 3, 4, 0, 5, 6, 0, 7, 0, 0, 0, 0]\n",
      "7 [1, 2, 0, 0, 0, 3, 4, 0, 5, 6, 0, 7, 8, 0, 0, 0]\n",
      "8 [1, 2, 0, 0, 0, 3, 4, 0, 5, 6, 0, 7, 8, 0, 9, 0]\n",
      "9 [1, 2, 0, 0, 0, 3, 4, 0, 5, 6, 0, 7, 8, 0, 9, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_mask(['S', *words, 'E']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一言不合', '就', '开干', '，', '狮子', '好', '凶猛', '！']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一', '言', '不', '合', '就', '开', '干', '，', '狮', '子', '好', '凶', '猛', '！']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 14)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1 [1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2 [1, 0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 0]\n",
      "3 [1, 0, 0, 0, 0, 2, 0, 0, 0, 3, 4, 0, 0, 0]\n",
      "4 [1, 0, 0, 0, 0, 2, 0, 0, 0, 3, 4, 0, 5, 0]\n",
      "5 [1, 0, 0, 0, 0, 2, 0, 0, 0, 3, 4, 0, 5, 6]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-e7c7b266f84b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'[CLS]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SEP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-5bb342643995>\u001b[0m in \u001b[0;36mwords_mask\u001b[0;34m(words, tokens)\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mlen__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mlen__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlen__\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mstart_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "words_mask(['[CLS]', *words, 'SEP'], tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 15,\n",
       " 18,\n",
       " 19,\n",
       " 17]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_mask(['S', *words, 'E'], tokenizer.convert_ids_to_tokens(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 872, 1962, 1557, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('你好啊')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 872, 1962, 1557, 102]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('你好啊')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 872, 1962, 1557, 102]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('你好啊')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 872, 1962, 1557, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('你好啊')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 872, 1962, 1557, 102]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('你好啊', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel, BertConfig, TFBertModel, create_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qqbrowser.models.baseline import MultiModal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import melt as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.init_flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "x = MultiModal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.models.bert.modeling_tf_bert.TFBertMainLayer at 0x7f3eb1aaefd0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.bert.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.models.bert.modeling_tf_bert.TFBertModel at 0x7f3eb1aae6a0>,\n",
       " <qqbrowser.models.baseline.NeXtVLAD at 0x7f3eb1084240>,\n",
       " <qqbrowser.models.baseline.ConcatDenseSE at 0x7f3eb0fe8320>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f3eb0f7a470>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf_bert_model'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.layers[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.bert.layers[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.layers[0].layers[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.models.bert.modeling_tf_bert.TFBertModel at 0x7f3eb1aae6a0>,\n",
       " <qqbrowser.models.baseline.NeXtVLAD at 0x7f3eb1084240>,\n",
       " <qqbrowser.models.baseline.ConcatDenseSE at 0x7f3eb0fe8320>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f3eb0f7a470>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<qqbrowser.models.baseline.NeXtVLAD at 0x7f3eb1084240>,\n",
       " <qqbrowser.models.baseline.ConcatDenseSE at 0x7f3eb0fe8320>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f3eb0f7a470>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[layer for layer in x.layers if layer != x.bert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertModel at 0x7f3eb1aae6a0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<qqbrowser.models.baseline.NeXtVLAD at 0x7f3eb1084240>,\n",
       " <qqbrowser.models.baseline.ConcatDenseSE at 0x7f3eb0fe8320>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f3eb0f7a470>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.layers[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.matplotlib_fname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ~/data/SimHei.ttf /opt/conda/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DejaVuSans-Bold.ttf\t\tLICENSE_DEJAVU\t       STIXSizOneSymReg.ttf\r\n",
      "DejaVuSans-BoldOblique.ttf\tLICENSE_STIX\t       STIXSizThreeSymBol.ttf\r\n",
      "DejaVuSans-Oblique.ttf\t\tSTIXGeneral.ttf        STIXSizThreeSymReg.ttf\r\n",
      "DejaVuSans.ttf\t\t\tSTIXGeneralBol.ttf     STIXSizTwoSymBol.ttf\r\n",
      "DejaVuSansDisplay.ttf\t\tSTIXGeneralBolIta.ttf  STIXSizTwoSymReg.ttf\r\n",
      "DejaVuSansMono-Bold.ttf\t\tSTIXGeneralItalic.ttf  SimHei.ttf\r\n",
      "DejaVuSansMono-BoldOblique.ttf\tSTIXNonUni.ttf\t       cmb10.ttf\r\n",
      "DejaVuSansMono-Oblique.ttf\tSTIXNonUniBol.ttf      cmex10.ttf\r\n",
      "DejaVuSansMono.ttf\t\tSTIXNonUniBolIta.ttf   cmmi10.ttf\r\n",
      "DejaVuSerif-Bold.ttf\t\tSTIXNonUniIta.ttf      cmr10.ttf\r\n",
      "DejaVuSerif-BoldItalic.ttf\tSTIXSizFiveSymReg.ttf  cmss10.ttf\r\n",
      "DejaVuSerif-Italic.ttf\t\tSTIXSizFourSymBol.ttf  cmsy10.ttf\r\n",
      "DejaVuSerif.ttf\t\t\tSTIXSizFourSymReg.ttf  cmtt10.ttf\r\n",
      "DejaVuSerifDisplay.ttf\t\tSTIXSizOneSymBol.ttf\r\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/conda/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ~/.cache/matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ~/data/matplotlibrc  /opt/conda/lib/python3.6/site-packages/matplotlib/mpl-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a3b8e604c89dd738b73f629e47065937bc949a9033b2f0d810af4f206605dd3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
