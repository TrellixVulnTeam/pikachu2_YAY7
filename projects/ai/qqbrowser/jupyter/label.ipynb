{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-14 16:58:12.241948: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import sys, os \n",
    "sys.path.append('..')\n",
    "sys.path.append('../../../../utils')\n",
    "sys.path.append('../../../../third')\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gezi\n",
    "from gezi import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63614\n",
      "0.0\n",
      "0.003945413626970014\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gezi\n",
    "from gezi import Vocab\n",
    "import json\n",
    "model_name = 'macbert'\n",
    "root = '../working/offline/35/'\n",
    "model_dir = f'{root}/{model_name}'\n",
    "fold = 0\n",
    "vocab = Vocab('../input/pairwise/vid.vocab', 0)\n",
    "print(vocab.size())\n",
    "file = f'{model_dir}/pairwise/{fold}/valid.json'\n",
    "with open(file) as f:\n",
    " vid_embedding = json.load(f)\n",
    "emb_dim = 256\n",
    "embs = np.zeros((vocab.size(), emb_dim))\n",
    "print(np.mean(embs))\n",
    "for vid, emb in vid_embedding.items():\n",
    "  embs[vocab.id(vid)] = np.asarray(emb)\n",
    "print(np.mean(embs))\n",
    "np.save(f'{model_dir}/pairwise/{fold}/valid.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel, TFAutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/pretrain/word/0/tiny/bert were not used when initializing TFAlbertModel: ['predictions', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/query/bias:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/LayerNorm/gamma:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/query/kernel:0', 'tf_albert_for_masked_lm/albert/embeddings/LayerNorm/beta:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/dense/bias:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/key/kernel:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/full_layer_layer_norm/gamma:0', 'tf_albert_for_masked_lm/albert/embeddings/LayerNorm/gamma:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/ffn_output/bias:0', 'tf_albert_for_masked_lm/albert/embeddings/token_type_embeddings/embeddings:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/ffn_output/kernel:0', 'tf_albert_for_masked_lm/albert/encoder/embedding_hidden_mapping_in/kernel:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/key/bias:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/ffn/kernel:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/value/bias:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/full_layer_layer_norm/beta:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/dense/kernel:0', 'tf_albert_for_masked_lm/albert/encoder/embedding_hidden_mapping_in/bias:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/ffn/bias:0', 'tf_albert_for_masked_lm/albert/embeddings/position_embeddings/embeddings:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/value/kernel:0', 'tf_albert_for_masked_lm/albert/embeddings/word_embeddings/weight:0', 'tf_albert_for_masked_lm/albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/LayerNorm/beta:0']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFAlbertModel were not initialized from the model checkpoint at ../input/pretrain/word/0/tiny/bert and are newly initialized: ['albert/embeddings/LayerNorm/beta:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/LayerNorm/gamma:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/key/bias:0', 'albert/pooler/bias:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/ffn/kernel:0', 'albert/encoder/embedding_hidden_mapping_in/kernel:0', 'albert/pooler/kernel:0', 'albert/embeddings/LayerNorm/gamma:0', 'albert/embeddings/position_embeddings/embeddings:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/query/bias:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/full_layer_layer_norm/beta:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/ffn/bias:0', 'albert/encoder/embedding_hidden_mapping_in/bias:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/full_layer_layer_norm/gamma:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/value/bias:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/query/kernel:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/key/kernel:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/dense/bias:0', 'albert/embeddings/token_type_embeddings/embeddings:0', 'albert/embeddings/word_embeddings/weight:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/value/kernel:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/dense/kernel:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/ffn_output/kernel:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/ffn_output/bias:0', 'albert/encoder/albert_layer_groups_._0/albert_layers_._0/attention/LayerNorm/beta:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "x = TFAutoModel.from_pretrained('../input/pretrain/word/0/tiny/bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.albert.modeling_tf_albert.TFAlbertMainLayer at 0x7f5e2af2abb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/pretrain/0/bert-base-chinese/bert were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at ../input/pretrain/0/bert-base-chinese/bert and are newly initialized: ['bert/pooler/dense/bias:0', 'bert/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "x = TFAutoModel.from_pretrained('../input/pretrain/0/bert-base-chinese/bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.models.bert.modeling_tf_bert.TFBertMainLayer at 0x7f5e2ba7a5b0>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.albert.modeling_tf_albert.TFAlbertMainLayer at 0x7f5e2bada7f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04416986,  0.03421553, -0.03631344, ..., -0.03308839,\n",
       "        -0.00273736, -0.0053576 ],\n",
       "       [ 0.00611336, -0.03274676, -0.03784559, ...,  0.02179161,\n",
       "        -0.04844187,  0.0049248 ],\n",
       "       [ 0.04062833,  0.02554473,  0.01683699, ...,  0.02588887,\n",
       "        -0.00560684, -0.01232914],\n",
       "       ...,\n",
       "       [-0.03317818,  0.02776423, -0.02806441, ...,  0.02959499,\n",
       "        -0.04396443, -0.02489318],\n",
       "       [ 0.0191858 , -0.0322538 , -0.02458396, ..., -0.03428076,\n",
       "         0.02351121, -0.03878129],\n",
       "       [ 0.04030947, -0.04770916, -0.0461725 , ..., -0.04363672,\n",
       "        -0.01946393, -0.01658311]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.read_csv('../input/pairwise/label.tsv', names=['vid1', 'vid2', 'relevance'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(label.vid1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(label.vid2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(label.vid1) | set(label.vid2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l ../input/pairwise/ids.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(label.vid1) & set(label.vid2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(label.vid1) & set(label.vid2))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[label.vid1 == 1282350513519752622]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[label.vid2 == 1282350513519752622]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect_vids = set(label.vid1) & set(label.vid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids = list(set(label.vid1))\n",
    "np.random.shuffle(vids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids = [x for x in vids if x not in intersect_vids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids_list = np.array_split(vids, FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(x) for x in vids_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vids_list[-1] = np.asarray(list(vids_list[-1]) + list(intersect_vids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(x) for x in vids_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2 = label[~label.vid1.isin(intersect_vids) & ~label.vid2.isin(intersect_vids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label3 = label[label.vid1.isin(intersect_vids) | label.vid2.isin(intersect_vids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label2 = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(FOLDS):\n",
    "  d = label2[label2.vid1.isin(set(vids_list[i]))]\n",
    "  print(len(d))\n",
    "  d.to_csv(f'../input/pairwise/label_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label3.to_csv(f'../input/pairwise/label_{FOLDS}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vids_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2[label2.vid1.isin(vids_list[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(label2.vid1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = {}\n",
    "for i in range(FOLDS):\n",
    "  ds[i] = pd.read_csv(f'../input/pairwise/label_{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(x, y):\n",
    "  return len(set(list(x.vid1.values) + list(x.vid2.values)) & set(list(y.vid1.values) + list(y.vid2.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(FOLDS):\n",
    "  for j in range(FOLDS):\n",
    "      if j > i:\n",
    "        print(i, j, intersect(ds[i], ds[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect(pd.concat([ds[1],ds[2],ds[3],ds[4]]), ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.concat([ds[1],ds[2],ds[3],ds[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(dt.vid1 | dt.vid2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0][ds[0].vid1.isin(set(dt.vid1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0][ds[0].vid2.isin(set(dt.vid2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e1511501be64c09e2f1eb58c3220ebc9ce84b491d308a480caed250bbb4af51"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
