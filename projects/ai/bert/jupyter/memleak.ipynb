{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-multi\t\t\t\t\t  tf-xlm-roberta-base\r\n",
      "jigsaw-multilingual-toxic-comment-classification  tf-xlm-roberta-large\r\n",
      "tfrecords\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../input/tfrecords'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os  \n",
    "import traceback\n",
    "\n",
    "RECORDS_PATH = '../input/tfrecords'\n",
    "\n",
    "if os.path.exists('/kaggle'):\n",
    "  sys.path.append('/kaggle/input/gezi-melt/utils')\n",
    "  sys.path.append('/kaggle/input/official')\n",
    "  from kaggle_datasets import KaggleDatasets\n",
    "  try:\n",
    "    GCS_PATH = KaggleDatasets().get_gcs_path('jigsaw-multilingual-toxic-comment-classification')\n",
    "    RECORDS_PATH = KaggleDatasets().get_gcs_path('toxic-multi-tfrecords') + '/tfrecords'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "  except Exception:\n",
    "    print(traceback.format_exc())\n",
    "    RECORDS_PATH = '../input/toxic-multi-tfrecords/tfrecords'\n",
    "    pass\n",
    "!ls ../input\n",
    "RECORDS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import official\n",
    "import gezi\n",
    "import melt\n",
    "import lele\n",
    "import husky\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "from absl import app, flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('model', None, '')\n",
    "flags.DEFINE_bool('multi_head', False, '')\n",
    "flags.DEFINE_string('pretrained', '../input/tf-xlm-roberta-large', '')\n",
    "flags.DEFINE_integer('max_len', 192, 'xlm 192 bert 128')\n",
    "flags.DEFINE_bool('freeze_pretrained', False, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags\n",
    "argv=['']\n",
    "FLAGS(argv)\n",
    "mark='xlm'\n",
    "FLAGS.train_input=f'{RECORDS_PATH}/{mark}/jigsaw-toxic-comment-train'\n",
    "FLAGS.valid_input=f'{RECORDS_PATH}/{mark}/validation'\n",
    "FLAGS.test_input=f'{RECORDS_PATH}/{mark}/test'\n",
    "FLAGS.valid_interval_steps=100 \n",
    "FLAGS.verbose=1 \n",
    "FLAGS.num_epochs=1\n",
    "FLAGS.keras=1 \n",
    "FLAGS.buffer_size=2048\n",
    "FLAGS.learning_rate=1e-5 \n",
    "FLAGS.min_learning_rate=0.\n",
    "# FLAGS.opt_epsilon=1e-8 \n",
    "# FLAGS.optimizer='bert-adamw'\n",
    "FLAGS.optimizer='adam'\n",
    "FLAGS.metrics=[] \n",
    "FLAGS.test_names=['id', 'toxic']\n",
    "FLAGS.valid_interval_epochs=0.1\n",
    "FLAGS.test_interval_epochs=1.\n",
    "FLAGS.num_gpus=1\n",
    "FLAGS.cache=0\n",
    "FLAGS.model_dir='../working/exps/v1/base'\n",
    "FLAGS.multi_head=0\n",
    "FLAGS.batch_parse=1\n",
    "FLAGS.save_model=0\n",
    "FLAGS.pretrained = '../input/tf-xlm-roberta-large/'\n",
    "# FLAGS.pretrained = '../input/tf-xlm-roberta-base/'\n",
    "FLAGS.batch_size=16 if 'large' in FLAGS.pretrained else 32\n",
    "# FLAGS.batch_size=16\n",
    "FLAGS.debug=0\n",
    "\n",
    "toxic_types = ['severe_toxic', 'obscene', 'identity_hate', 'threat', 'insult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import gezi\n",
    "logging = gezi.logging\n",
    "\n",
    "langs = ['es', 'it', 'tr']\n",
    "\n",
    "def evaluate(y_true, y_pred, x):\n",
    "  try:\n",
    "    y_true = y_true[:,0]\n",
    "    y_pred = y_pred[:,0]\n",
    "  except Exception:\n",
    "    pass\n",
    "  if y_pred.max() > 1. or y_pred.min() < 0:\n",
    "    y_pred = gezi.sigmoid(y_pred)\n",
    "  result = {}\n",
    "  loss = log_loss(y_true, y_pred)\n",
    "  result['loss'] = loss\n",
    "  \n",
    "  auc = roc_auc_score(y_true, y_pred)\n",
    "  result['auc/all'] = auc\n",
    "    \n",
    "  if 'lang' in x:\n",
    "    x['y_true'] = y_true\n",
    "    x['pred'] = y_pred\n",
    "    x['lang'] = gezi.decode(x['lang'])\n",
    "\n",
    "    df = pd.DataFrame(x) \n",
    "    df = shuffle(df)\n",
    "    logging.info('\\n', df)\n",
    "\n",
    "    for lang in langs:\n",
    "      df_ = df[df.lang==lang]\n",
    "      auc = roc_auc_score(df_.y_true, df_.pred)\n",
    "      result[f'auc/{lang}'] = auc\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import melt\n",
    "\n",
    "class Dataset(melt.Dataset):\n",
    "  def __init__(self, subset='valid', **kwargs):\n",
    "    super(Dataset, self).__init__(subset, **kwargs)\n",
    "\n",
    "  def parse(self, example):\n",
    "    MAX_LEN = FLAGS.max_len\n",
    "    features_dict = {\n",
    "      'input_word_ids': tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
    "      'toxic': tf.io.FixedLenFeature([], tf.float32),\n",
    "      'id': tf.io.FixedLenFeature([], tf.string),\n",
    "      # 'comment_text': tf.io.FixedLenFeature([], tf.string), # TODO\n",
    "    }\n",
    "\n",
    "    def _adds(names, dtype=None, length=None):\n",
    "      dtype_ = dtype\n",
    "      for name in names:\n",
    "        if name in self.example:\n",
    "          dtype = dtype_ or self.example[name][0].dtype \n",
    "          if length:\n",
    "            features_dict[name] = tf.io.FixedLenFeature([length], dtype)\n",
    "          else:\n",
    "            features_dict[name] = tf.io.FixedLenFeature([], dtype)\n",
    "\n",
    "    _adds(['lang'], tf.string)\n",
    "\n",
    "    _adds(['input_mask', 'all_segment_id'], tf.int64, MAX_LEN)\n",
    "    \n",
    "    _adds(toxic_types)\n",
    "\n",
    "    features = self.parse_(serialized=example, features=features_dict)\n",
    "\n",
    "    def _casts(names, dtype=tf.int32):\n",
    "      for name in names:\n",
    "        if name in features:\n",
    "          features[name] = tf.cast(features[name], dtype)\n",
    "\n",
    "    _casts(['input_word_ids', 'input_mask', 'all_segment_id'])\n",
    "    \n",
    "    x = features\n",
    "    y = features['toxic']\n",
    "#     y = tf.cast(features['toxic'] > 0.5, tf.float32)\n",
    "    keys = ['toxic', *toxic_types]\n",
    "    for key in keys:\n",
    "      if key not in features:\n",
    "        features[key] = tf.zeros_like(features['toxic'])\n",
    "        \n",
    "    _casts(toxic_types, tf.float32)\n",
    "        \n",
    "    melt.append_dim(features, keys)\n",
    "\n",
    "    if FLAGS.multi_head:\n",
    "      y = tf.concat([features[key] for key in keys], 1)\n",
    "\n",
    "    return x, y\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "import tensorflow as tf\n",
    "\n",
    "def calc_loss(y_true, y_pred):\n",
    "  pass\n",
    "\n",
    "def focal_loss(gamma=1.5, alpha=.2):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "def get_loss_fn():\n",
    "#   return tf.compat.v1.losses.sigmoid_cross_entropy\n",
    "  return tf.keras.losses.BinaryCrossentropy()\n",
    "#   return focal_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "import melt\n",
    "import gezi \n",
    "logging = gezi.logging\n",
    "\n",
    "# class Model(keras.Model):\n",
    "#   def __init__(self):\n",
    "#     super(Model, self).__init__() \n",
    "\n",
    "#     self.bert_layer = bert_layer\n",
    "#     dims = [32]\n",
    "#     self.mlp = melt.layers.MLP(dims)\n",
    "#     odim = len(toxic_types) + 1 if FLAGS.multi_head else 1\n",
    "#     self.dense = keras.layers.Dense(odim, activation='sigmoid')\n",
    "\n",
    "#   def call(self, input):\n",
    "#     input_word_ids = input['input_word_ids']\n",
    "#     input_mask = input['input_mask']\n",
    "#     segment_ids = input['all_segment_id']\n",
    "  \n",
    "#     x, _ = self.bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "#     x = self.mlp(x)\n",
    "#     x = self.dense(x)\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0418 10:09:46.857226 140390474569536 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0418 10:09:46.858443 140390474569536 file_utils.py:57] TensorFlow version 2.2.0-dev20200411 available.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "def xlm_model():\n",
    "  pretrained = FLAGS.pretrained or XLM_PATH\n",
    "  with gezi.Timer(f'load xlm_model from {pretrained}', True, logging.info):\n",
    "    transformer = TFAutoModel.from_pretrained(pretrained)\n",
    "  if FLAGS.freeze_pretrained:\n",
    "    transformer.trainable = False\n",
    "  input_word_ids = Input(shape=(FLAGS.max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "  sequence_output = transformer(input_word_ids)[0]\n",
    "  cls_token = sequence_output[:, 0, :]\n",
    "  odim = len(toxic_types) + 1 if FLAGS.multi_head else 1\n",
    "  out = keras.layers.Dense(odim, activation='sigmoid')(cls_token)\n",
    "\n",
    "  model = keras.Model(inputs=input_word_ids, outputs=out)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = xlm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-18 10:09:47 0:00:00 fcntl.floc with lock_file /root/.melt.lock (If hang here means other programs calling melt.init have not finished yet)\n",
      "2020-04-18 10:09:47 0:00:00 Tf dataset and Tf model train in Eager mode, keras 1, distributed:False\n",
      "2020-04-18 10:09:47 0:00:00 log_level: 20 (try --debug to show more or --log_level=(> 20) to show less(no INFO), try --verbose to show train/valid loss intervaly)\n",
      "2020-04-18 10:09:47 0:00:00 batch_size: 16 eval_batch_size: 16 batch_size_per_gpu: 16 num_gpus: 1 gpu: [4] CUDA_VISIABLE_DEVICES=[] work_mode: train distributed: False horovod: False\n",
      "2020-04-18 10:09:49 0:00:02 model: [base] model_dir: [../working/exps/v1/base]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "import os\n",
    "import melt\n",
    "\n",
    "fit = melt.fit\n",
    "melt.init()\n",
    "loss_fn = get_loss_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-18 10:09:49 0:00:02 load xlm_model from ../input/tf-xlm-roberta-large/ start\n",
      "I0418 10:09:49.689640 140390474569536 configuration_utils.py:281] loading configuration file ../input/tf-xlm-roberta-large/config.json\n",
      "I0418 10:09:49.691958 140390474569536 configuration_utils.py:319] Model config XLMRobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "I0418 10:09:49.700480 140390474569536 modeling_tf_utils.py:388] loading weights file ../input/tf-xlm-roberta-large/tf_model.h5\n",
      "2020-04-18 10:10:02 0:00:15 load xlm_model from ../input/tf-xlm-roberta-large/ duration: 13.163249492645264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
      "_________________________________________________________________\n",
      "tf_roberta_model (TFRobertaM ((None, 192, 1024), (None 559890432 \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 1024)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 559,891,457\n",
      "Trainable params: 559,891,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "strategy = melt.distributed.get_strategy()\n",
    "with strategy.scope():\n",
    "  model = Model()\n",
    "try:\n",
    "  model.summary()\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('../input/toxic-multi/xlm.toxic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model=model):\n",
    "  fit(model,  \n",
    "      loss_fn,\n",
    "      Dataset,\n",
    "      eval_fn=evaluate,\n",
    "      eval_keys=['lang'],\n",
    "      )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-18 10:10:15 0:00:27 -------Round: 0 mode: None train_input:[jigsaw-unintended-bias-train] valid_input:[validation] train_dirs:[1] valid_dir: ../input/tfrecords/xlm/validation\n",
      "2020-04-18 10:10:15 0:00:27 --start_hour=jigsaw-unintended-bias-train --end_hour=validation root: ../input/tfrecords/xlm-sample1\n",
      "2020-04-18 10:10:17 0:00:30 num_train_examples: 100    \n",
      "2020-04-18 10:10:17 0:00:30 num_valid_examples: 100   \n",
      "2020-04-18 10:10:17 0:00:30 num_test_examples: 63812  \n",
      "2020-04-18 10:10:18 0:00:30 latest ckpt to restore: None\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[16,192,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/tf_roberta_model/roberta/encoder/layer_._18/intermediate/activation/Erf (defined at /home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/transformers/modeling_tf_bert.py:63) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/model/tf_roberta_model/roberta/embeddings/token_type_embeddings/embedding_lookup/Reshape/_1026]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[16,192,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/tf_roberta_model/roberta/encoder/layer_._18/intermediate/activation/Erf (defined at /home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/transformers/modeling_tf_bert.py:63) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_58859]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0a9514cd4e76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvie\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtie\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-db5e7892965f>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0meval_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0meval_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lang'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       )   \n",
      "\u001b[0;32m/home/gezi/mine/pikachu/utils/melt/apps/train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, loss_fn, Dataset, dataset, valid_dataset, valid_dataset2, num_folds, fake_run, **kwargs)\u001b[0m\n\u001b[1;32m   1675\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mhusky\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1677\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhusky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1678\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_round_grow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/mine/pikachu/utils/husky/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_fn, info, Dataset, dataset, valid_dataset, valid_dataset2, test_dataset, evaluate_fn, inference_fn, eval_fn, eval_keys, write_valid, valid_names, infer_names, infer_debug_names, valid_write_fn, infer_write_fn, valid_suffix, infer_suffix, write_streaming, optimizer, variables_list_fn, lr_params, init_fn, weights, sep, out_hook, callbacks, initial_epoch, return_info, fake_run)\u001b[0m\n\u001b[1;32m    245\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                         \u001b[0;31m# verbose=2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                       )\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    919\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2421\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2423\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[16,192,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/tf_roberta_model/roberta/encoder/layer_._18/intermediate/activation/Erf (defined at /home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/transformers/modeling_tf_bert.py:63) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/model/tf_roberta_model/roberta/embeddings/token_type_embeddings/embedding_lookup/Reshape/_1026]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[16,192,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/tf_roberta_model/roberta/encoder/layer_._18/intermediate/activation/Erf (defined at /home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/transformers/modeling_tf_bert.py:63) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_58859]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "FLAGS.train_input=f'{RECORDS_PATH}/xlm-sample1/jigsaw-unintended-bias-train'\n",
    "FLAGS.learning_rate=1e-5\n",
    "# FLAGS.opt_epsilon=1e-8\n",
    "FLAGS.num_epochs=1\n",
    "FLAGS.num_train=100\n",
    "FLAGS.num_valid=100\n",
    "FLAGS.vie=1\n",
    "FLAGS.tie=0\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.train_input=f'{RECORDS_PATH}/xlm-sample2/jigsaw-unintended-bias-train'\n",
    "FLAGS.learning_rate=1e-5\n",
    "# FLAGS.opt_epsilon=1e-8\n",
    "FLAGS.num_epochs=1\n",
    "FLAGS.num_train=100\n",
    "FLAGS.num_valid=100\n",
    "FLAGS.vie=1\n",
    "FLAGS.tie=0\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.train_input=f'{RECORDS_PATH}/{mark}/jigsaw-toxic-comment-train'\n",
    "FLAGS.learning_rate=1e-5\n",
    "# FLAGS.opt_epsilon=1e-8\n",
    "FLAGS.num_epochs=1\n",
    "FLAGS.num_train=100\n",
    "FLAGS.num_valid=100\n",
    "FLAGS.vie=1\n",
    "FLAGS.tie=0\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv\n",
    "FLAGS.train_input=FLAGS.valid_input\n",
    "FLAGS.learning_rate=1e-5\n",
    "valid_input = FLAGS.valid_input\n",
    "FLAGS.num_folds = 5\n",
    "FLAGS.vie=1.\n",
    "run()\n",
    "FLAGS.num_folds = None\n",
    "FLAGS.vie=0.1\n",
    "FLAGS.valid_input = FLAGS.train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv\n",
    "FLAGS.train_input=FLAGS.valid_input\n",
    "FLAGS.learning_rate=1e-5\n",
    "valid_input = FLAGS.valid_input\n",
    "FLAGS.num_folds = 5\n",
    "FLAGS.num_train=None\n",
    "FLAGS.vie=1.\n",
    "run()\n",
    "FLAGS.num_folds = None\n",
    "FLAGS.vie=0.1\n",
    "FLAGS.valid_input = FLAGS.train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('./xlm.toxic-uint1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAGS.train_input=FLAGS.valid_input\n",
    "# FLAGS.learning_rate=1e-5\n",
    "# # FLAGS.opt_epsilon=1e-8\n",
    "# FLAGS.num_epochs=1\n",
    "# run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('./xlm.final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with strategy.scope():\n",
    "# #   model = Model()\n",
    "# FLAGS.train_input=f'{RECORDS_GCS_PATH}/{mark}/jigsaw-toxic-comment-train,{RECORDS_GCS_PATH}/xlm-sample2/jigsaw-unintended-bias-train'\n",
    "# # FLAGS.train_input=f'{RECORDS_GCS_PATH}/xlm/jigsaw-unintended-bias-train'\n",
    "# FLAGS.learning_rate=3e-5\n",
    "# FLAGS.opt_epsilon=1e-8\n",
    "# FLAGS.num_epochs=1  \n",
    "# FLAGS.valid_interval_epochs=0.1\n",
    "# run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAGS.train_input=FLAGS.valid_input\n",
    "# FLAGS.learning_rate=3e-5\n",
    "# FLAGS.opt_epsilon=1e-8\n",
    "# FLAGS.num_epochs=1\n",
    "# FLAGS.valid_interval_epochs=0.2\n",
    "# FLAGS.optimizer='bert-adamw'\n",
    "# run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = pd.read_csv('../working/exps/v1/base/submission.csv')\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d.to_csv('submission.csv', index=False)\n",
    "# d.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
