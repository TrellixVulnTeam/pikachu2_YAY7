{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os  \n",
    "import traceback\n",
    "\n",
    "RECORDS_PATH = '../input/tfrecords'\n",
    "\n",
    "if os.path.exists('/kaggle'):\n",
    "  sys.path.append('/kaggle/input/gezi-melt/utils')\n",
    "  sys.path.append('/kaggle/input/official')\n",
    "  from kaggle_datasets import KaggleDatasets\n",
    "  try:\n",
    "    GCS_PATH = KaggleDatasets().get_gcs_path('jigsaw-multilingual-toxic-comment-classification')\n",
    "    RECORDS_PATH = KaggleDatasets().get_gcs_path('toxic-multi-tfrecords') + '/tfrecords'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "  except Exception:\n",
    "    print(traceback.format_exc())\n",
    "    RECORDS_PATH = '../input/toxic-multi-tfrecords/tfrecords'\n",
    "    pass\n",
    "!ls ../input\n",
    "RECORDS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import official\n",
    "import gezi\n",
    "import melt\n",
    "import lele\n",
    "import husky\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "from absl import app, flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('model', None, '')\n",
    "flags.DEFINE_bool('multi_head', False, '')\n",
    "flags.DEFINE_string('pretrained', '../input/tf-xlm-roberta-large', '')\n",
    "flags.DEFINE_integer('max_len', 192, 'xlm 192 bert 128')\n",
    "flags.DEFINE_bool('freeze_pretrained', False, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags\n",
    "argv=['']\n",
    "FLAGS(argv)\n",
    "mark='xlm'\n",
    "FLAGS.train_input=f'{RECORDS_PATH}/{mark}/jigsaw-toxic-comment-train'\n",
    "FLAGS.valid_input=f'{RECORDS_PATH}/{mark}/validation'\n",
    "FLAGS.test_input=f'{RECORDS_PATH}/{mark}/test'\n",
    "FLAGS.valid_interval_steps=100 \n",
    "FLAGS.verbose=1 \n",
    "FLAGS.num_epochs=1\n",
    "FLAGS.keras=1 \n",
    "FLAGS.buffer_size=2048\n",
    "FLAGS.learning_rate=1e-5 \n",
    "FLAGS.min_learning_rate=0.\n",
    "# FLAGS.opt_epsilon=1e-8 \n",
    "# FLAGS.optimizer='bert-adamw'\n",
    "FLAGS.optimizer='adam'\n",
    "FLAGS.metrics=[] \n",
    "FLAGS.test_names=['id', 'toxic']\n",
    "FLAGS.valid_interval_epochs=0.1\n",
    "FLAGS.test_interval_epochs=1.\n",
    "FLAGS.num_gpus=4\n",
    "FLAGS.cache=0\n",
    "FLAGS.model_dir='../working/exps/v1/base'\n",
    "FLAGS.multi_head=1\n",
    "FLAGS.batch_parse=1\n",
    "FLAGS.save_model=0\n",
    "# FLAGS.pretrained = '../input/tf-xlm-roberta-large/'\n",
    "FLAGS.pretrained = '../input/tf-xlm-roberta-base/'\n",
    "FLAGS.batch_size=16 if 'large' in FLAGS.pretrained else 32\n",
    "# FLAGS.batch_size=16\n",
    "FLAGS.debug=0\n",
    "\n",
    "toxic_types = ['severe_toxic', 'obscene', 'identity_hate', 'threat', 'insult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import gezi\n",
    "logging = gezi.logging\n",
    "\n",
    "langs = ['es', 'it', 'tr']\n",
    "\n",
    "def evaluate(y_true, y_pred, x):\n",
    "  try:\n",
    "    y_true = y_true[:,0]\n",
    "    y_pred = y_pred[:,0]\n",
    "  except Exception:\n",
    "    pass\n",
    "  if y_pred.max() > 1. or y_pred.min() < 0:\n",
    "    y_pred = gezi.sigmoid(y_pred)\n",
    "  result = {}\n",
    "  loss = log_loss(y_true, y_pred)\n",
    "  result['loss'] = loss\n",
    "  \n",
    "  auc = roc_auc_score(y_true, y_pred)\n",
    "  result['auc/all'] = auc\n",
    "    \n",
    "  if 'lang' in x:\n",
    "    x['y_true'] = y_true\n",
    "    x['pred'] = y_pred\n",
    "    x['lang'] = gezi.decode(x['lang'])\n",
    "\n",
    "    df = pd.DataFrame(x) \n",
    "    df = shuffle(df)\n",
    "    logging.info('\\n', df)\n",
    "\n",
    "    for lang in langs:\n",
    "      df_ = df[df.lang==lang]\n",
    "      auc = roc_auc_score(df_.y_true, df_.pred)\n",
    "      result[f'auc/{lang}'] = auc\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import melt\n",
    "\n",
    "class Dataset(melt.Dataset):\n",
    "  def __init__(self, subset='valid', **kwargs):\n",
    "    super(Dataset, self).__init__(subset, **kwargs)\n",
    "\n",
    "  def parse(self, example):\n",
    "    MAX_LEN = FLAGS.max_len\n",
    "    features_dict = {\n",
    "      'input_word_ids': tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
    "      'toxic': tf.io.FixedLenFeature([], tf.float32),\n",
    "      'id': tf.io.FixedLenFeature([], tf.string),\n",
    "      # 'comment_text': tf.io.FixedLenFeature([], tf.string), # TODO\n",
    "    }\n",
    "\n",
    "    def _adds(names, dtype=None, length=None):\n",
    "      dtype_ = dtype\n",
    "      for name in names:\n",
    "        if name in self.example:\n",
    "          dtype = dtype_ or self.example[name][0].dtype \n",
    "          if length:\n",
    "            features_dict[name] = tf.io.FixedLenFeature([length], dtype)\n",
    "          else:\n",
    "            features_dict[name] = tf.io.FixedLenFeature([], dtype)\n",
    "\n",
    "    _adds(['lang'], tf.string)\n",
    "\n",
    "    _adds(['input_mask', 'all_segment_id'], tf.int64, MAX_LEN)\n",
    "    \n",
    "    _adds(toxic_types)\n",
    "\n",
    "    features = self.parse_(serialized=example, features=features_dict)\n",
    "\n",
    "    def _casts(names, dtype=tf.int32):\n",
    "      for name in names:\n",
    "        if name in features:\n",
    "          features[name] = tf.cast(features[name], dtype)\n",
    "\n",
    "    _casts(['input_word_ids', 'input_mask', 'all_segment_id'])\n",
    "    \n",
    "    x = features\n",
    "    y = features['toxic']\n",
    "#     y = tf.cast(features['toxic'] > 0.5, tf.float32)\n",
    "    keys = ['toxic', *toxic_types]\n",
    "    for key in keys:\n",
    "      if key not in features:\n",
    "        features[key] = tf.zeros_like(features['toxic'])\n",
    "        \n",
    "    _casts(toxic_types, tf.float32)\n",
    "        \n",
    "    melt.append_dim(features, keys)\n",
    "\n",
    "    if FLAGS.multi_head:\n",
    "      y = tf.concat([features[key] for key in keys], 1)\n",
    "\n",
    "    return x, y\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "import tensorflow as tf\n",
    "\n",
    "def calc_loss(y_true, y_pred):\n",
    "  pass\n",
    "\n",
    "def focal_loss(gamma=1.5, alpha=.2):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "def get_loss_fn():\n",
    "#   return tf.compat.v1.losses.sigmoid_cross_entropy\n",
    "  return tf.keras.losses.BinaryCrossentropy()\n",
    "#   return focal_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "import melt\n",
    "import gezi \n",
    "logging = gezi.logging\n",
    "\n",
    "# class Model(keras.Model):\n",
    "#   def __init__(self):\n",
    "#     super(Model, self).__init__() \n",
    "\n",
    "#     self.bert_layer = bert_layer\n",
    "#     dims = [32]\n",
    "#     self.mlp = melt.layers.MLP(dims)\n",
    "#     odim = len(toxic_types) + 1 if FLAGS.multi_head else 1\n",
    "#     self.dense = keras.layers.Dense(odim, activation='sigmoid')\n",
    "\n",
    "#   def call(self, input):\n",
    "#     input_word_ids = input['input_word_ids']\n",
    "#     input_mask = input['input_mask']\n",
    "#     segment_ids = input['all_segment_id']\n",
    "  \n",
    "#     x, _ = self.bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "#     x = self.mlp(x)\n",
    "#     x = self.dense(x)\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "def xlm_model():\n",
    "  pretrained = FLAGS.pretrained or XLM_PATH\n",
    "  with gezi.Timer(f'load xlm_model from {pretrained}', True, logging.info):\n",
    "    transformer = TFAutoModel.from_pretrained(pretrained)\n",
    "  if FLAGS.freeze_pretrained:\n",
    "    transformer.trainable = False\n",
    "  input_word_ids = Input(shape=(FLAGS.max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "  sequence_output = transformer(input_word_ids)[0]\n",
    "  cls_token = sequence_output[:, 0, :]\n",
    "  odim = len(toxic_types) + 1 if FLAGS.multi_head else 1\n",
    "  out = keras.layers.Dense(odim, activation='sigmoid')(cls_token)\n",
    "\n",
    "  model = keras.Model(inputs=input_word_ids, outputs=out)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = xlm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "import os\n",
    "import melt\n",
    "\n",
    "fit = melt.fit\n",
    "melt.init()\n",
    "loss_fn = get_loss_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = melt.distributed.get_strategy()\n",
    "with strategy.scope():\n",
    "  model = Model()\n",
    "try:\n",
    "  model.summary()\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('../input/toxic-multi/xlm.toxic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model=model):\n",
    "  fit(model,  \n",
    "      loss_fn,\n",
    "      Dataset,\n",
    "      eval_fn=evaluate,\n",
    "      eval_keys=['lang'],\n",
    "      )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "FLAGS.train_input=f'{RECORDS_PATH}/xlm-sample1/jigsaw-unintended-bias-train'\n",
    "FLAGS.learning_rate=1e-5\n",
    "# FLAGS.opt_epsilon=1e-8\n",
    "FLAGS.num_epochs=1\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.train_input=f'{RECORDS_PATH}/{mark}/jigsaw-toxic-comment-train'\n",
    "FLAGS.learning_rate=1e-5\n",
    "# FLAGS.opt_epsilon=1e-8\n",
    "FLAGS.num_epochs=1  \n",
    "run()\n",
    "# # model.save('./xlm.toxic.3e5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv\n",
    "FLAGS.train_input=FLAGS.valid_input\n",
    "FLAGS.learning_rate=1e-5\n",
    "valid_input = FLAGS.valid_input\n",
    "FLAGS.num_folds = 5\n",
    "FLAGS.vie=1.\n",
    "run()\n",
    "FLAGS.num_folds = None\n",
    "FLAGS.vie=0.1\n",
    "FLAGS.valid_input = FLAGS.train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('./xlm.toxic-uint1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.train_input=FLAGS.valid_input\n",
    "FLAGS.learning_rate=1e-5\n",
    "# FLAGS.opt_epsilon=1e-8\n",
    "FLAGS.num_epochs=1\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('./xlm.final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with strategy.scope():\n",
    "# #   model = Model()\n",
    "# FLAGS.train_input=f'{RECORDS_GCS_PATH}/{mark}/jigsaw-toxic-comment-train,{RECORDS_GCS_PATH}/xlm-sample2/jigsaw-unintended-bias-train'\n",
    "# # FLAGS.train_input=f'{RECORDS_GCS_PATH}/xlm/jigsaw-unintended-bias-train'\n",
    "# FLAGS.learning_rate=3e-5\n",
    "# FLAGS.opt_epsilon=1e-8\n",
    "# FLAGS.num_epochs=1  \n",
    "# FLAGS.valid_interval_epochs=0.1\n",
    "# run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAGS.train_input=FLAGS.valid_input\n",
    "# FLAGS.learning_rate=3e-5\n",
    "# FLAGS.opt_epsilon=1e-8\n",
    "# FLAGS.num_epochs=1\n",
    "# FLAGS.valid_interval_epochs=0.2\n",
    "# FLAGS.optimizer='bert-adamw'\n",
    "# run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('../working/exps/v1/base/submission.csv')\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_csv('submission.csv', index=False)\n",
    "d.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
