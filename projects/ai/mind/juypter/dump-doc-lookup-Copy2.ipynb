{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gezi\n",
    "from gezi import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_names = ['did', 'cat', 'sub_cat', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = {}\n",
    "flens = {}\n",
    "fnames['title'] = ['title']\n",
    "flens['title'] = [30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130379/130379 [00:45<00:00, 2887.69it/s]\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "emb_size = 30\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 1\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title = l[0], l[3]\n",
    "    if title:\n",
    "      tokens = tokenizer.encode(title)\n",
    "    else:\n",
    "      tokens = [1]\n",
    "    lens += [len(tokens[1:-1])]\n",
    "    tokens = gezi.pad(tokens[1:-1], emb_size)\n",
    "    emb[vocab.id(did)] = np.asarray(tokens)\n",
    "emb = np.asarray(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1     0     0 ...     0     0     0]\n",
      " [    1     0     0 ...     0     0     0]\n",
      " [ 1109  3128  1130 ...     0     0     0]\n",
      " ...\n",
      " [ 9743  1192 12958 ...     0     0     0]\n",
      " [ 2289 24409   117 ...     0     0     0]\n",
      " [21166   119  3254 ...     0     0     0]]\n",
      "(130381, 30)\n"
     ]
    }
   ],
   "source": [
    "print(emb)\n",
    "print(emb.shape)\n",
    "\n",
    "np.save(f'../input/title_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tovec(sent):\n",
    "  input_ids = torch.tensor(tokenizer.encode('[CLS] ' + sent)).unsqueeze(0)  # Batch size 1\n",
    "  outputs = model(input_ids)\n",
    "  last_hidden_states = outputs[0]\n",
    "  emb = last_hidden_states[0][0]\n",
    "  return emb.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 33844/130379 [1:10:31<4:12:45,  6.37it/s] "
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "emb_size = 768\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0.] * emb_size\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title = l[0], l[3]\n",
    "    emb[vocab.id(did)] = tovec(title)\n",
    "emb = np.asarray(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.035120686613642\n",
      "2\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(lens))\n",
    "print(np.min(lens))\n",
    "print(np.max(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cat sbucat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['cat'] = ['cat', 'sub_cat']\n",
    "flens['cat'] = [1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/130379 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130379/130379 [00:01<00:00, 109229.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   1]\n",
      " [  1   1]\n",
      " [  3  40]\n",
      " ...\n",
      " [ 10 104]\n",
      " [  2   3]\n",
      " [  2  19]]\n",
      "(130381, 2)\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "\n",
    "emb_size = 2\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "cat_vocab = gezi.Vocab('../input/cat.txt')\n",
    "scat_vocab = gezi.Vocab('../input/sub_cat.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "empty = [1] * emb_size\n",
    "emb = [empty] * emb_height\n",
    "\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, cat, sub_cat = l[0], l[1], l[2]\n",
    "    data = [cat_vocab.id(cat), scat_vocab.id(sub_cat)]\n",
    "    emb[vocab.id(did)] = np.asarray(data)\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "print(emb.shape)\n",
    "\n",
    "np.save(f'../input/cat_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['abstract'] = ['abstract']\n",
    "flens['abstract'] = [50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 715/130379 [00:00<01:32, 1405.23it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "  1%|          | 847/130379 [00:00<01:34, 1377.69it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "  3%|▎         | 3850/130379 [00:03<01:49, 1151.23it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      " 31%|███       | 40592/130379 [00:35<01:19, 1136.03it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      " 32%|███▏      | 42331/130379 [00:37<01:16, 1143.94it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n",
      " 40%|███▉      | 51520/130379 [00:45<01:14, 1062.56it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      " 42%|████▏     | 54223/130379 [00:48<01:13, 1029.61it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      " 49%|████▊     | 63349/130379 [00:56<01:03, 1049.50it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      " 57%|█████▋    | 74222/130379 [01:06<00:52, 1077.23it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      " 68%|██████▊   | 88037/130379 [01:18<00:38, 1110.17it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      " 91%|█████████ | 118962/130379 [01:46<00:10, 1048.68it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (944 > 512). Running this sequence through the model will result in indexing errors\n",
      " 92%|█████████▏| 119754/130379 [01:46<00:09, 1076.03it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (882 > 512). Running this sequence through the model will result in indexing errors\n",
      " 96%|█████████▌| 125037/130379 [01:51<00:04, 1113.99it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (747 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 130379/130379 [01:56<00:00, 1120.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1     0     0 ...     0     0     0]\n",
      " [    1     0     0 ...     0     0     0]\n",
      " [ 3128  1112  1562 ...     0     0     0]\n",
      " ...\n",
      " [ 1636  1132  1103 ...     0     0     0]\n",
      " [  123   118  1795 ...  1795 27019   117]\n",
      " [ 1109  1148  1226 ...  1509  3299  1406]]\n",
      "(130381, 50)\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "emb_size = 50\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 1\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, abstract = l[0], l[4]\n",
    "    if abstract:\n",
    "      tokens = tokenizer.encode(abstract)\n",
    "    else:\n",
    "      tokens = [1]\n",
    "    lens += [len(tokens[1:-1])]\n",
    "    tokens = gezi.pad(tokens[1:-1], emb_size)\n",
    "    emb[vocab.id(did)] = np.asarray(tokens)\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "print(emb.shape)\n",
    "\n",
    "np.save(f'../input/abstract_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.402311721979764\n",
      "0\n",
      "944\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(lens))\n",
    "print(np.min(lens))\n",
    "print(np.max(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['body'] = ['body']\n",
    "flens['body'] = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/msn.json',\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "emb_size = 100\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 1\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "\n",
    "bodys = json.load(open(files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130379/130379 [00:00<00:00, 213959.13it/s]\n"
     ]
    }
   ],
   "source": [
    "nid2did = {}\n",
    "file = '../input/news/news.tsv'\n",
    "total = len(open(file).readlines())\n",
    "for line in tqdm(open(file), total=total):\n",
    "  l = line.strip().split('\\t')\n",
    "  did, url = l[0], l[5]\n",
    "  nid = url.split('/')[-1].split('.')[0]\n",
    "  nid2did[nid] = did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 9288/130379 [00:20<04:13, 477.63it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      " 31%|███       | 40685/130379 [01:28<03:10, 470.86it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      " 40%|███▉      | 51521/130379 [01:51<02:51, 460.66it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      " 42%|████▏     | 54259/130379 [01:57<02:57, 429.02it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      " 68%|██████▊   | 88063/130379 [03:09<01:26, 488.31it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 130379/130379 [04:37<00:00, 469.33it/s]\n"
     ]
    }
   ],
   "source": [
    "lens = []\n",
    "for body_ in tqdm(bodys, total=len(bodys)):\n",
    "  body = ' '.join(body_['body'])\n",
    "  body = ' '.join(body.split()[:100])\n",
    "  tokens = tokenizer.encode(body)\n",
    "  totkens = tokens[1:-1]\n",
    "  lens += [len(tokens)]\n",
    "  did = vocab.id(nid2did[body_['nid']])\n",
    "  tokens = gezi.pad(tokens[1:-1], emb_size)\n",
    "  emb[did] = np.asarray(tokens)\n",
    "#   if len(lens) > 100:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     0,     0, ...,     0,     0,     0],\n",
       "       [    1,     0,     0, ...,     0,     0,     0],\n",
       "       [ 9300, 14263,  2821, ...,  9707,  7308,  4317],\n",
       "       ...,\n",
       "       [ 2372,  1128,  1107, ...,  1170,  1105,  1256],\n",
       "       [  123,   118,  1795, ...,  4254,  2237,  1348],\n",
       "       [ 1109,  1148,  1226, ...,  8125, 24996,   117]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = np.asarray(emb)\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.96120540884651"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'../input/body_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['entity'] = ['title_entities', 'abstract_entities', 'title_entity_types', 'abstract_entity_types']\n",
    "flens['entity'] = [5, 5, 5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 331/130379 [00:00<00:02, 49564.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N82836\thealth\tmedical\tWhere to Get a Cheap Flu Shot: Walmart, CVS, Costco, and More\tThe Centers for Disease Control and Prevention recommends that everyone over the age of 6 months get a dose of the influenza vaccine. To find out where to get cheap flu shots in 2019, we compared prices at Walgreens, CVS, Rite Aid, Walmart, Target, Kroger, Safeway, Meijer, Costco, and Sam's Club.\thttps://assets.msn.com/labs/mind/AAI3QbY.html\t[{\"Label\": \"Costco\", \"Type\": \"O\", \"WikidataId\": \"Q715583\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [45], \"SurfaceForms\": [\"Costco\"]}, {\"Label\": \"CVS Pharmacy\", \"Type\": \"O\", \"WikidataId\": \"Q2078880\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [40], \"SurfaceForms\": [\"CVS\"]}, {\"Label\": \"Walmart\", \"Type\": \"O\", \"WikidataId\": \"Q483551\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [31], \"SurfaceForms\": [\"Walmart\"]}]\t[{\"Label\": \"Costco\", \"Type\": \"O\", \"WikidataId\": \"Q715583\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [274], \"SurfaceForms\": [\"Costco\"]}, {\"Label\": \"Meijer\", \"Type\": \"O\", \"WikidataId\": \"Q1917753\", \"Confidence\": 0.998, \"OccurrenceOffsets\": [266], \"SurfaceForms\": [\"Meijer\"]}, {\"Label\": \"CVS Pharmacy\", \"Type\": \"O\", \"WikidataId\": \"Q2078880\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [217], \"SurfaceForms\": [\"CVS\"]}, {\"Label\": \"Sam's Club\", \"Type\": \"O\", \"WikidataId\": \"Q1972120\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [286], \"SurfaceForms\": [\"Sam's Club\"]}, {\"Label\": \"Safeway Inc.\", \"Type\": \"O\", \"WikidataId\": \"Q1508234\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [257], \"SurfaceForms\": [\"Safeway\"]}, {\"Label\": \"Walmart\", \"Type\": \"O\", \"WikidataId\": \"Q483551\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [232], \"SurfaceForms\": [\"Walmart\"]}, {\"Label\": \"Target Corporation\", \"Type\": \"O\", \"WikidataId\": \"Q1046951\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [241], \"SurfaceForms\": [\"Target\"]}, {\"Label\": \"Rite Aid\", \"Type\": \"O\", \"WikidataId\": \"Q3433273\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [222], \"SurfaceForms\": [\"Rite Aid\"]}, {\"Label\": \"Walgreens\", \"Type\": \"O\", \"WikidataId\": \"Q1591889\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [206], \"SurfaceForms\": [\"Walgreens\"]}, {\"Label\": \"Kroger\", \"Type\": \"O\", \"WikidataId\": \"Q153417\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [249], \"SurfaceForms\": [\"Kroger\"]}, {\"Label\": \"Centers for Disease Control and Prevention\", \"Type\": \"O\", \"WikidataId\": \"Q583725\", \"Confidence\": 0.994, \"OccurrenceOffsets\": [4], \"SurfaceForms\": [\"Centers for Disease Control and Prevention\"]}]\n",
      "\n",
      "[{'Label': 'Costco', 'Type': 'O', 'WikidataId': 'Q715583', 'Confidence': 1.0, 'OccurrenceOffsets': [45], 'SurfaceForms': ['Costco']}, {'Label': 'CVS Pharmacy', 'Type': 'O', 'WikidataId': 'Q2078880', 'Confidence': 0.971, 'OccurrenceOffsets': [40], 'SurfaceForms': ['CVS']}, {'Label': 'Walmart', 'Type': 'O', 'WikidataId': 'Q483551', 'Confidence': 1.0, 'OccurrenceOffsets': [31], 'SurfaceForms': ['Walmart']}]\n",
      "[{'Label': 'Costco', 'Type': 'O', 'WikidataId': 'Q715583', 'Confidence': 1.0, 'OccurrenceOffsets': [274], 'SurfaceForms': ['Costco']}, {'Label': 'Meijer', 'Type': 'O', 'WikidataId': 'Q1917753', 'Confidence': 0.998, 'OccurrenceOffsets': [266], 'SurfaceForms': ['Meijer']}, {'Label': 'CVS Pharmacy', 'Type': 'O', 'WikidataId': 'Q2078880', 'Confidence': 0.971, 'OccurrenceOffsets': [217], 'SurfaceForms': ['CVS']}, {'Label': \"Sam's Club\", 'Type': 'O', 'WikidataId': 'Q1972120', 'Confidence': 1.0, 'OccurrenceOffsets': [286], 'SurfaceForms': [\"Sam's Club\"]}, {'Label': 'Safeway Inc.', 'Type': 'O', 'WikidataId': 'Q1508234', 'Confidence': 1.0, 'OccurrenceOffsets': [257], 'SurfaceForms': ['Safeway']}, {'Label': 'Walmart', 'Type': 'O', 'WikidataId': 'Q483551', 'Confidence': 1.0, 'OccurrenceOffsets': [232], 'SurfaceForms': ['Walmart']}, {'Label': 'Target Corporation', 'Type': 'O', 'WikidataId': 'Q1046951', 'Confidence': 1.0, 'OccurrenceOffsets': [241], 'SurfaceForms': ['Target']}, {'Label': 'Rite Aid', 'Type': 'O', 'WikidataId': 'Q3433273', 'Confidence': 1.0, 'OccurrenceOffsets': [222], 'SurfaceForms': ['Rite Aid']}, {'Label': 'Walgreens', 'Type': 'O', 'WikidataId': 'Q1591889', 'Confidence': 1.0, 'OccurrenceOffsets': [206], 'SurfaceForms': ['Walgreens']}, {'Label': 'Kroger', 'Type': 'O', 'WikidataId': 'Q153417', 'Confidence': 1.0, 'OccurrenceOffsets': [249], 'SurfaceForms': ['Kroger']}, {'Label': 'Centers for Disease Control and Prevention', 'Type': 'O', 'WikidataId': 'Q583725', 'Confidence': 0.994, 'OccurrenceOffsets': [4], 'SurfaceForms': ['Centers for Disease Control and Prevention']}]\n",
      "N82836\n",
      "health\n",
      "medical\n",
      "Where to Get a Cheap Flu Shot: Walmart, CVS, Costco, and More\n",
      "The Centers for Disease Control and Prevention recommends that everyone over the age of 6 months get a dose of the influenza vaccine. To find out where to get cheap flu shots in 2019, we compared prices at Walgreens, CVS, Rite Aid, Walmart, Target, Kroger, Safeway, Meijer, Costco, and Sam's Club.\n",
      "https://assets.msn.com/labs/mind/AAI3QbY.html\n",
      "[{\"Label\": \"Costco\", \"Type\": \"O\", \"WikidataId\": \"Q715583\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [45], \"SurfaceForms\": [\"Costco\"]}, {\"Label\": \"CVS Pharmacy\", \"Type\": \"O\", \"WikidataId\": \"Q2078880\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [40], \"SurfaceForms\": [\"CVS\"]}, {\"Label\": \"Walmart\", \"Type\": \"O\", \"WikidataId\": \"Q483551\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [31], \"SurfaceForms\": [\"Walmart\"]}]\n",
      "[{\"Label\": \"Costco\", \"Type\": \"O\", \"WikidataId\": \"Q715583\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [274], \"SurfaceForms\": [\"Costco\"]}, {\"Label\": \"Meijer\", \"Type\": \"O\", \"WikidataId\": \"Q1917753\", \"Confidence\": 0.998, \"OccurrenceOffsets\": [266], \"SurfaceForms\": [\"Meijer\"]}, {\"Label\": \"CVS Pharmacy\", \"Type\": \"O\", \"WikidataId\": \"Q2078880\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [217], \"SurfaceForms\": [\"CVS\"]}, {\"Label\": \"Sam's Club\", \"Type\": \"O\", \"WikidataId\": \"Q1972120\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [286], \"SurfaceForms\": [\"Sam's Club\"]}, {\"Label\": \"Safeway Inc.\", \"Type\": \"O\", \"WikidataId\": \"Q1508234\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [257], \"SurfaceForms\": [\"Safeway\"]}, {\"Label\": \"Walmart\", \"Type\": \"O\", \"WikidataId\": \"Q483551\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [232], \"SurfaceForms\": [\"Walmart\"]}, {\"Label\": \"Target Corporation\", \"Type\": \"O\", \"WikidataId\": \"Q1046951\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [241], \"SurfaceForms\": [\"Target\"]}, {\"Label\": \"Rite Aid\", \"Type\": \"O\", \"WikidataId\": \"Q3433273\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [222], \"SurfaceForms\": [\"Rite Aid\"]}, {\"Label\": \"Walgreens\", \"Type\": \"O\", \"WikidataId\": \"Q1591889\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [206], \"SurfaceForms\": [\"Walgreens\"]}, {\"Label\": \"Kroger\", \"Type\": \"O\", \"WikidataId\": \"Q153417\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [249], \"SurfaceForms\": [\"Kroger\"]}, {\"Label\": \"Centers for Disease Control and Prevention\", \"Type\": \"O\", \"WikidataId\": \"Q583725\", \"Confidence\": 0.994, \"OccurrenceOffsets\": [4], \"SurfaceForms\": [\"Centers for Disease Control and Prevention\"]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "# emb_size = 2\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "# emb = [empty] * emb_height\n",
    "\n",
    "title_entity_lens = []\n",
    "abstract_entity_lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title_entities, abstract_entities = l[0], l[-2], l[-1]\n",
    "    title_entities = json.loads(title_entities)\n",
    "    abstract_entities = json.loads(abstract_entities)\n",
    "    title_entity_lens +=  [len(title_entities)]\n",
    "    abstract_entity_lens += [len(abstract_entities)]\n",
    "    if len(abstract_entities) > 10:\n",
    "      print(line)\n",
    "      print(title_entities)\n",
    "      print(abstract_entities)\n",
    "      print('\\n'.join(l))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1927917839529372"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(title_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(title_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(title_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9756095690256867"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abstract_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(abstract_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(abstract_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130379/130379 [00:03<00:00, 35555.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1    0    0 ...    0    0    0]\n",
      " [   1    0    0 ...    0    0    0]\n",
      " [   1    0    0 ...    0    0    0]\n",
      " ...\n",
      " [   1    0    0 ...    0    0    0]\n",
      " [   1    0    0 ...    5    0    0]\n",
      " [1840 9673    0 ...    0    0    0]] (130381, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "# emb_size = 2\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "entity_vocab = gezi.Vocab('../input/entity.txt')\n",
    "entity_type_vocab = gezi.Vocab('../input/entity_type.txt')\n",
    "\n",
    "# [title_entities, abstract_entities, title_etypes, abstract_entities]\n",
    "# 9 and 30\n",
    "MAX_TITLE_ENTITIES = 5\n",
    "MAX_ABSTRACT_ENTITIES = 5\n",
    "\n",
    "ENTITY_LEN = MAX_TITLE_ENTITIES + MAX_ABSTRACT_ENTITIES\n",
    "emb_size = ENTITY_LEN * 2\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 1 \n",
    "empty[MAX_TITLE_ENTITIES] = 1\n",
    "emb = [empty.copy() for _ in range(emb_height)] \n",
    "\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title_entities, abstract_entities = l[0], l[-2], l[-1]\n",
    "    did = vocab.id(did)\n",
    "    title_entities = json.loads(title_entities)\n",
    "    abstract_entities = json.loads(abstract_entities)\n",
    "    for i, m in enumerate(title_entities):\n",
    "      if i >= MAX_TITLE_ENTITIES:\n",
    "        continue\n",
    "      emb[did][i] = entity_vocab.id(m['WikidataId'])\n",
    "      emb[did][ENTITY_LEN + i] = entity_type_vocab.id(m['Type'])\n",
    "    for i, m in enumerate(abstract_entities):\n",
    "      if i >= MAX_ABSTRACT_ENTITIES:\n",
    "        continue\n",
    "      emb[did][MAX_TITLE_ENTITIES + i] = entity_vocab.id(m['WikidataId'])\n",
    "      emb[did][ENTITY_LEN + MAX_TITLE_ENTITIES + i] = entity_type_vocab.id(m['Type'])     \n",
    "emb = np.asarray(emb)\n",
    "print(emb, emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../input/entity_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30629923071613196"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(emb > 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([424,   0,   0,   0,   0, 254, 713, 120, 260, 532,   2,   0,   0,\n",
       "         0,   0,   2,   2,   2,   2,   2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'sub_cat', 'title_entities', 'abstract_entities', 'title_entity_types', 'abstract_entity_types', 'title', 'abstract']\n",
      "[1, 1, 5, 5, 5, 5, 30, 50]\n",
      "102\n",
      "(130381, 2)\n",
      "(130381, 20)\n",
      "(130381, 30)\n",
      "(130381, 50)\n"
     ]
    }
   ],
   "source": [
    "keys = ['cat', 'entity', 'title', 'abstract']\n",
    "feats_ = [ ]\n",
    "flens_ = []\n",
    "\n",
    "for key in keys:\n",
    "  feats_ += fnames[key]\n",
    "  flens_ += flens[key]\n",
    "  \n",
    "print(feats_)\n",
    "print(flens_)\n",
    "print(sum(flens_))\n",
    "\n",
    "embs = []\n",
    "for key in keys:\n",
    "  emb = np.load(f'../input/{key}_lookup.npy')\n",
    "  print(emb.shape)\n",
    "  embs += [emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     1,     1, ...,     0,     0,     0],\n",
       "       [    1,     1,     1, ...,     0,     0,     0],\n",
       "       [    3,    40,     1, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [   10,   104,     1, ...,     0,     0,     0],\n",
       "       [    2,     3,     1, ...,  1795, 27019,   117],\n",
       "       [    2,    19,  1840, ...,  1509,  3299,  1406]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = np.concatenate(embs, 1)\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130381, 102)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../input/doc_lookup.npy', emb)\n",
    "np.save('../input/doc_fnames.npy', np.asarray(feats_))\n",
    "np.save('../input/doc_flens.npy', np.asarray(flens_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entity emb from wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50927 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[[ 0.04096048 -0.03556133 -0.04408021 ... -0.0022821  -0.02009994\n",
      "   0.02849957]\n",
      " [-0.04931467 -0.02747755 -0.01376152 ... -0.04337468 -0.04510694\n",
      "  -0.03300745]\n",
      " [-0.00224096 -0.0357698  -0.00430866 ... -0.03357329 -0.03051395\n",
      "   0.04724857]\n",
      " ...\n",
      " [-0.04139754  0.00033464 -0.01594218 ... -0.0262714   0.04772429\n",
      "   0.01051264]\n",
      " [ 0.00095603  0.01616765 -0.03987193 ...  0.03548088  0.0228728\n",
      "   0.00683941]\n",
      " [-0.03737675 -0.02940915  0.03667745 ...  0.00616488  0.00748581\n",
      "  -0.04651056]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 42007/50927 [00:02<00:00, 18099.00it/s]\n",
      " 65%|██████▌   | 33310/50927 [00:00<00:00, 75491.09it/s]\n",
      " 92%|█████████▏| 46807/50927 [00:00<00:00, 56981.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04096048 -0.03556133 -0.04408021 ... -0.0022821  -0.02009994\n",
      "   0.02849957]\n",
      " [-0.04931467 -0.02747755 -0.01376152 ... -0.04337468 -0.04510694\n",
      "  -0.03300745]\n",
      " [ 0.001677   -0.069251    0.00074    ... -0.05406     0.003242\n",
      "  -0.006887  ]\n",
      " ...\n",
      " [ 0.02741     0.019489    0.005972   ... -0.041304    0.007908\n",
      "   0.009783  ]\n",
      " [ 0.037157   -0.016108   -0.01267    ... -0.004157   -0.008539\n",
      "   0.050533  ]\n",
      " [ 0.040347    0.054959    0.084386   ... -0.066457   -0.03859\n",
      "   0.001724  ]]\n",
      "[[ 0.14022623 -0.12174249 -0.15090647 ... -0.00781266 -0.06881118\n",
      "   0.09756688]\n",
      " [-0.17931151 -0.09991027 -0.05003782 ... -0.15771332 -0.16401193\n",
      "  -0.12001736]\n",
      " [ 0.00335602 -0.13858546  0.00148089 ... -0.10818515  0.00648791\n",
      "  -0.0137823 ]\n",
      " ...\n",
      " [ 0.09532903  0.06778065  0.02076997 ... -0.14365087  0.02750317\n",
      "   0.03402422]\n",
      " [ 0.09275387 -0.0402099  -0.03162773 ... -0.01037699 -0.02131564\n",
      "   0.12614397]\n",
      " [ 0.12996376  0.17703121  0.27182    ... -0.214068   -0.1243042\n",
      "   0.00555326]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "vocab_file = '../input/entity.txt'\n",
    "vocab = gezi.Vocab(vocab_file)\n",
    "emb_height = vocab.size()\n",
    "\n",
    "emb_size = len(open('../input/train/entity_embedding.vec').readline().strip().split()) - 1\n",
    "print(emb_size)\n",
    "\n",
    "emb = np.random.uniform(-0.05, 0.05,(emb_height, emb_size))\n",
    "print(emb)\n",
    "\n",
    "emb = list(emb)\n",
    "\n",
    "files = [\n",
    "  '../input/train/entity_embedding.vec', \n",
    "  '../input/dev/entity_embedding.vec', \n",
    "  '../input/test/entity_embedding.vec'\n",
    "]\n",
    "\n",
    "entities = set()\n",
    "for file_ in files:\n",
    "  for line in tqdm(open(file_), total=emb_height):\n",
    "    l = line.strip().split()\n",
    "    entity, vals = l[0], l[1:]\n",
    "    if entity in entities:\n",
    "      continue\n",
    "    entities.add(entity)\n",
    "    vals = np.asarray(list(map(float, vals)))\n",
    "    #vals = normalize(np.reshape(vals, (1,-1)))\n",
    "    #vals /= np.sqrt(emb_size)\n",
    "    vals = np.reshape(vals, (-1,))\n",
    "    emb[vocab.id(entity)] = vals\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "\n",
    "#emb = normalize(emb)\n",
    "\n",
    "np.save('../input/entity_emb.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50927 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[[-0.00154117  0.03035518 -0.00589259 ... -0.04094373  0.04299281\n",
      "  -0.02494033]\n",
      " [ 0.01657999 -0.04411747  0.04258725 ... -0.0143552  -0.04014027\n",
      "  -0.02701598]\n",
      " [-0.01651694  0.01509018 -0.04305561 ...  0.00110249  0.02015254\n",
      "   0.0299422 ]\n",
      " ...\n",
      " [ 0.00573689 -0.02462167  0.00071024 ...  0.03420342  0.03679753\n",
      "   0.01625067]\n",
      " [-0.02460548 -0.04921551 -0.02206481 ...  0.04289599 -0.01137452\n",
      "  -0.02312832]\n",
      " [ 0.01571464 -0.01823292  0.04533747 ...  0.00403212  0.03232957\n",
      "   0.03336391]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 42007/50927 [00:05<00:01, 7064.90it/s]\n",
      " 65%|██████▌   | 33310/50927 [00:00<00:00, 62116.54it/s]\n",
      " 92%|█████████▏| 46807/50927 [00:01<00:00, 28778.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00154117  0.03035518 -0.00589259 ... -0.04094373  0.04299281\n",
      "  -0.02494033]\n",
      " [ 0.01657999 -0.04411747  0.04258725 ... -0.0143552  -0.04014027\n",
      "  -0.02701598]\n",
      " [ 0.00335602 -0.13858546  0.00148089 ... -0.10818515  0.00648791\n",
      "  -0.0137823 ]\n",
      " ...\n",
      " [ 0.09532903  0.06778065  0.02076997 ... -0.14365087  0.02750317\n",
      "   0.03402422]\n",
      " [ 0.09275387 -0.0402099  -0.03162773 ... -0.01037699 -0.02131564\n",
      "   0.12614397]\n",
      " [ 0.12996376  0.17703121  0.27182    ... -0.214068   -0.1243042\n",
      "   0.00555326]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "vocab_file = '../input/entity.txt'\n",
    "vocab = gezi.Vocab(vocab_file)\n",
    "emb_height = vocab.size()\n",
    "\n",
    "emb_size = len(open('../input/train/entity_embedding.vec').readline().strip().split()) - 1\n",
    "print(emb_size)\n",
    "\n",
    "emb = np.random.uniform(-0.05, 0.05,(emb_height, emb_size))\n",
    "print(emb)\n",
    "\n",
    "emb = list(emb)\n",
    "\n",
    "files = [\n",
    "  '../input/train/entity_embedding.vec', \n",
    "  '../input/dev/entity_embedding.vec', \n",
    "  '../input/test/entity_embedding.vec'\n",
    "]\n",
    "\n",
    "entities = set()\n",
    "for file_ in files:\n",
    "  for line in tqdm(open(file_), total=emb_height):\n",
    "    l = line.strip().split()\n",
    "    entity, vals = l[0], l[1:]\n",
    "    if entity in entities:\n",
    "      continue\n",
    "    entities.add(entity)\n",
    "    vals = np.asarray(list(map(float, vals)))\n",
    "    vals = normalize(np.reshape(vals, (1,-1)))\n",
    "    #vals /= np.sqrt(emb_size)\n",
    "    vals = np.reshape(vals, (-1,))\n",
    "    emb[vocab.id(entity)] = vals\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "\n",
    "#emb = normalize(emb)\n",
    "\n",
    "np.save('../input/entity_emb2.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
