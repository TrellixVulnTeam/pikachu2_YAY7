{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gezi\n",
    "from gezi import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_names = ['did', 'cat', 'sub_cat', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = {}\n",
    "flens = {}\n",
    "fnames['title'] = ['title']\n",
    "flens['title'] = [32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccde15082004ff4a7cd4ca0a0f619f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130379.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "emb_size = 32\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 101\n",
    "empty[1] = 102\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title = l[0], l[3]\n",
    "    if title:\n",
    "      tokens = tokenizer.encode(title)\n",
    "    else:\n",
    "      tokens = [101, 102]\n",
    "    lens += [len(tokens)]\n",
    "    tokens = gezi.pad(tokens, emb_size)\n",
    "    emb[vocab.id(did)] = np.asarray(tokens)\n",
    "emb = np.asarray(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101   102     0 ...     0     0     0]\n",
      " [  101   102     0 ...     0     0     0]\n",
      " [  101  1109  3128 ...     0     0     0]\n",
      " ...\n",
      " [  101  9743  1192 ...     0     0     0]\n",
      " [  101  2289 24409 ...     0     0     0]\n",
      " [  101 21166   119 ...     0     0     0]]\n",
      "(130381, 32)\n"
     ]
    }
   ],
   "source": [
    "print(emb)\n",
    "print(emb.shape)\n",
    "np.save(f'../input/title_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['title_albert'] = ['title_albert']\n",
    "flens['title_albert'] = [30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130379/130379 [00:26<00:00, 4907.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1     0     0 ...     0     0     0]\n",
      " [    1     0     0 ...     0     0     0]\n",
      " [    2    14   996 ...     0     0     0]\n",
      " ...\n",
      " [    2   378    42 ...     0     0     0]\n",
      " [    2   435 16121 ...     0     0     0]\n",
      " [    2 14481     9 ...     0     0     0]]\n",
      "(130381, 30)\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "model_name = 'albert-base-v2'\n",
    "# model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "emb_size = 30\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 1\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title = l[0], l[3]\n",
    "    if title:\n",
    "      tokens = tokenizer.encode(title)\n",
    "    else:\n",
    "      tokens = [1]\n",
    "    lens += [len(tokens)]\n",
    "    tokens = gezi.pad(tokens, emb_size)\n",
    "    emb[vocab.id(did)] = np.asarray(tokens)\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "print(emb.shape)\n",
    "np.save(f'../input/title_albert_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['title_uncased'] = ['title_uncased']\n",
    "flens['title_uncased'] = [32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6c99c860324897bafd529eeeb5cfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130379.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[  101   102     0 ...     0     0     0]\n",
      " [  101   102     0 ...     0     0     0]\n",
      " [  101  1996  2739 ...     0     0     0]\n",
      " ...\n",
      " [  101  2323  2017 ...     0     0     0]\n",
      " [  101  2374 19236 ...     0     0     0]\n",
      " [  101 19193  1012 ...     0     0     0]]\n",
      "(130381, 32)\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "# model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "emb_size = 32\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 101\n",
    "empty[1] = 102\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title = l[0], l[3]\n",
    "    if title:\n",
    "      tokens = tokenizer.encode(title)\n",
    "    else:\n",
    "      tokens = [101, 102]\n",
    "    lens += [len(tokens)]\n",
    "    tokens = gezi.pad(tokens, emb_size)\n",
    "    emb[vocab.id(did)] = np.asarray(tokens)\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "print(emb.shape)\n",
    "np.save(f'../input/title_uncased_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006849262534610635"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.asarray(lens)\n",
    "np.sum(lens > 30) / len(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75c7b97f997425180f11ef076a92e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-aa6998480cf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                     \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m                 )\n\u001b[1;32m    488\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, local_files_only)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         )\n\u001b[1;32m    256\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0ms3_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storing %s in cache at %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, user_agent)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEffectiveLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     )\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unexpected EOF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/OpenSSL/SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1813\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1814\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tovec(sent):\n",
    "  input_ids = torch.tensor(tokenizer.encode('[CLS] ' + sent)).unsqueeze(0)  # Batch size 1\n",
    "  outputs = model(input_ids)\n",
    "  last_hidden_states = outputs[0]\n",
    "  emb = last_hidden_states[0][0]\n",
    "  return emb.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/130379 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f5534a652c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0memb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtovec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-44c561a704b7>\u001b[0m in \u001b[0;36mtovec\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtovec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[CLS] '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Batch size 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "emb_size = 768\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0.] * emb_size\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title = l[0], l[3]\n",
    "    emb[vocab.id(did)] = tovec(title)\n",
    "emb = np.asarray(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cat sbucat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['cat'] = ['cat', 'sub_cat']\n",
    "flens['cat'] = [1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/130379 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130379/130379 [00:01<00:00, 109229.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   1]\n",
      " [  1   1]\n",
      " [  3  40]\n",
      " ...\n",
      " [ 10 104]\n",
      " [  2   3]\n",
      " [  2  19]]\n",
      "(130381, 2)\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "\n",
    "emb_size = 2\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "cat_vocab = gezi.Vocab('../input/cat.txt')\n",
    "scat_vocab = gezi.Vocab('../input/sub_cat.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "empty = [1] * emb_size\n",
    "emb = [empty] * emb_height\n",
    "\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, cat, sub_cat = l[0], l[1], l[2]\n",
    "    data = [cat_vocab.id(cat), scat_vocab.id(sub_cat)]\n",
    "    emb[vocab.id(did)] = np.asarray(data)\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "print(emb.shape)\n",
    "\n",
    "np.save(f'../input/cat_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['abstract'] = ['abstract']\n",
    "flens['abstract'] = [64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb8f10e87fd441a9a8d8fa330a85fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130379.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 101  102    0 ...    0    0    0]\n",
      " [ 101  102    0 ...    0    0    0]\n",
      " [ 101 3128 1112 ...    0    0    0]\n",
      " ...\n",
      " [ 101 1636 1132 ...    0    0    0]\n",
      " [ 101  123  118 ... 2468  131 1857]\n",
      " [ 101 1109 1148 ... 5489 2634  113]]\n",
      "(130381, 64)\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "emb_size = 64\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 101\n",
    "empty[1] = 102\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, abstract = l[0], l[4]\n",
    "    if abstract:\n",
    "      tokens = tokenizer.encode(abstract)\n",
    "    else:\n",
    "      tokens = [101, 102]\n",
    "    lens += [len(tokens)]\n",
    "    tokens = gezi.pad(tokens, emb_size)\n",
    "    emb[vocab.id(did)] = np.asarray(tokens)\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "print(emb.shape)\n",
    "\n",
    "np.save(f'../input/abstract_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['abstract_uncased'] = ['abstract_uncased']\n",
    "flens['abstract_uncased'] = [64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b0a636b6b64d39999a8e4ca24c4399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130379.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 101  102    0 ...    0    0    0]\n",
      " [ 101  102    0 ...    0    0    0]\n",
      " [ 101 2739 2004 ...    0    0    0]\n",
      " ...\n",
      " [ 101 2122 2024 ...    0    0    0]\n",
      " [ 101 1016 1011 ... 1024 2760 2197]\n",
      " [ 101 1996 2034 ... 6159 3028 1006]]\n",
      "(130381, 64)\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "emb_size = 64\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 101\n",
    "empty[1] = 102\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, abstract = l[0], l[4]\n",
    "    if abstract:\n",
    "      tokens = tokenizer.encode(abstract)\n",
    "    else:\n",
    "      tokens = [101, 102]\n",
    "    lens += [len(tokens)]\n",
    "    tokens = gezi.pad(tokens, emb_size)\n",
    "    emb[vocab.id(did)] = np.asarray(tokens)\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "print(emb.shape)\n",
    "\n",
    "np.save(f'../input/abstract_uncased_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.402311721979764\n",
      "0\n",
      "944\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(lens))\n",
    "print(np.min(lens))\n",
    "print(np.max(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['body'] = ['body']\n",
    "flens['body'] = [512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/msn.json',\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "emb_size = 512\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 101\n",
    "empty[1] = 102\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "\n",
    "bodys = json.load(open(files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2bfdacd6db44dfb315513696361f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130379.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nid2did = {}\n",
    "file = '../input/news/news.tsv'\n",
    "total = len(open(file).readlines())\n",
    "for line in tqdm(open(file), total=total):\n",
    "  l = line.strip().split('\\t')\n",
    "  did, url = l[0], l[5]\n",
    "  nid = url.split('/')[-1].split('.')[0]\n",
    "  nid2did[nid] = did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_ID = 0\n",
    "def padding_words(word_ids, max_len):\n",
    "  if len(word_ids) < max_len:\n",
    "    word_ids = word_ids + [PADDING_ID] * (max_len - len(word_ids))\n",
    "  return word_ids\n",
    "def regular_encode(text, tokenizer, max_len, last_tokens=0, padding=True):\n",
    "  word_ids = tokenizer.encode(text)\n",
    "\n",
    "  if len(word_ids) != max_len:\n",
    "    if len(word_ids) > max_len:\n",
    "      if last_tokens > 1:\n",
    "        word_ids = [*word_ids[:max_len - last_tokens], 102, *word_ids[-last_tokens + 1:]]\n",
    "      else:\n",
    "        word_ids = [*word_ids[:max_len - 1], 102]\n",
    "    elif padding:\n",
    "      word_ids = padding_words(word_ids, max_len)\n",
    "  \n",
    "  return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66138a4349747988e40bd6398a609e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130379.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1176 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lens = []\n",
    "for body_ in tqdm(bodys, total=len(bodys)):\n",
    "  body = ' '.join(body_['body'])\n",
    "  body = ' '.join(body.split())\n",
    "  did = vocab.id(nid2did[body_['nid']])\n",
    "  tokens = regular_encode(body, tokenizer, 512, 80)\n",
    "  emb[did] = np.asarray(tokens)\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "emb\n",
    "np.save(f'../input/body_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101   102     0 ...     0     0     0]\n",
      " [  101   102     0 ...     0     0     0]\n",
      " [  101  9300 14263 ...     0     0     0]\n",
      " ...\n",
      " [  101  2372  1128 ...  2383   119   102]\n",
      " [  101   123   118 ... 24595  1118   102]\n",
      " [  101  1109  1148 ... 24595  1118   102]]\n",
      "(130381, 512)\n"
     ]
    }
   ],
   "source": [
    "print(emb)\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['body_uncased'] = ['body_uncased']\n",
    "flens['body_uncased'] = [128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b317c4a00a44298770e61738cf83e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130379.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1120 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lens = []\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "for body_ in tqdm(bodys, total=len(bodys)):\n",
    "  body = ' '.join(body_['body'])\n",
    "  body = ' '.join(body.split())\n",
    "  did = vocab.id(nid2did[body_['nid']])\n",
    "  tokens = regular_encode(body, tokenizer, 128, 20)\n",
    "  emb[did] = np.asarray(tokens)\n",
    "#   if len(lens) > 100:\n",
    "#     break\n",
    "emb = np.asarray(emb)\n",
    "emb\n",
    "np.save(f'../input/body_uncased_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.96120540884651"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['entity'] = ['title_entities', 'abstract_entities', 'title_entity_types', 'abstract_entity_types']\n",
    "flens['entity'] = [5, 5, 5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 331/130379 [00:00<00:02, 49564.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N82836\thealth\tmedical\tWhere to Get a Cheap Flu Shot: Walmart, CVS, Costco, and More\tThe Centers for Disease Control and Prevention recommends that everyone over the age of 6 months get a dose of the influenza vaccine. To find out where to get cheap flu shots in 2019, we compared prices at Walgreens, CVS, Rite Aid, Walmart, Target, Kroger, Safeway, Meijer, Costco, and Sam's Club.\thttps://assets.msn.com/labs/mind/AAI3QbY.html\t[{\"Label\": \"Costco\", \"Type\": \"O\", \"WikidataId\": \"Q715583\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [45], \"SurfaceForms\": [\"Costco\"]}, {\"Label\": \"CVS Pharmacy\", \"Type\": \"O\", \"WikidataId\": \"Q2078880\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [40], \"SurfaceForms\": [\"CVS\"]}, {\"Label\": \"Walmart\", \"Type\": \"O\", \"WikidataId\": \"Q483551\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [31], \"SurfaceForms\": [\"Walmart\"]}]\t[{\"Label\": \"Costco\", \"Type\": \"O\", \"WikidataId\": \"Q715583\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [274], \"SurfaceForms\": [\"Costco\"]}, {\"Label\": \"Meijer\", \"Type\": \"O\", \"WikidataId\": \"Q1917753\", \"Confidence\": 0.998, \"OccurrenceOffsets\": [266], \"SurfaceForms\": [\"Meijer\"]}, {\"Label\": \"CVS Pharmacy\", \"Type\": \"O\", \"WikidataId\": \"Q2078880\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [217], \"SurfaceForms\": [\"CVS\"]}, {\"Label\": \"Sam's Club\", \"Type\": \"O\", \"WikidataId\": \"Q1972120\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [286], \"SurfaceForms\": [\"Sam's Club\"]}, {\"Label\": \"Safeway Inc.\", \"Type\": \"O\", \"WikidataId\": \"Q1508234\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [257], \"SurfaceForms\": [\"Safeway\"]}, {\"Label\": \"Walmart\", \"Type\": \"O\", \"WikidataId\": \"Q483551\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [232], \"SurfaceForms\": [\"Walmart\"]}, {\"Label\": \"Target Corporation\", \"Type\": \"O\", \"WikidataId\": \"Q1046951\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [241], \"SurfaceForms\": [\"Target\"]}, {\"Label\": \"Rite Aid\", \"Type\": \"O\", \"WikidataId\": \"Q3433273\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [222], \"SurfaceForms\": [\"Rite Aid\"]}, {\"Label\": \"Walgreens\", \"Type\": \"O\", \"WikidataId\": \"Q1591889\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [206], \"SurfaceForms\": [\"Walgreens\"]}, {\"Label\": \"Kroger\", \"Type\": \"O\", \"WikidataId\": \"Q153417\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [249], \"SurfaceForms\": [\"Kroger\"]}, {\"Label\": \"Centers for Disease Control and Prevention\", \"Type\": \"O\", \"WikidataId\": \"Q583725\", \"Confidence\": 0.994, \"OccurrenceOffsets\": [4], \"SurfaceForms\": [\"Centers for Disease Control and Prevention\"]}]\n",
      "\n",
      "[{'Label': 'Costco', 'Type': 'O', 'WikidataId': 'Q715583', 'Confidence': 1.0, 'OccurrenceOffsets': [45], 'SurfaceForms': ['Costco']}, {'Label': 'CVS Pharmacy', 'Type': 'O', 'WikidataId': 'Q2078880', 'Confidence': 0.971, 'OccurrenceOffsets': [40], 'SurfaceForms': ['CVS']}, {'Label': 'Walmart', 'Type': 'O', 'WikidataId': 'Q483551', 'Confidence': 1.0, 'OccurrenceOffsets': [31], 'SurfaceForms': ['Walmart']}]\n",
      "[{'Label': 'Costco', 'Type': 'O', 'WikidataId': 'Q715583', 'Confidence': 1.0, 'OccurrenceOffsets': [274], 'SurfaceForms': ['Costco']}, {'Label': 'Meijer', 'Type': 'O', 'WikidataId': 'Q1917753', 'Confidence': 0.998, 'OccurrenceOffsets': [266], 'SurfaceForms': ['Meijer']}, {'Label': 'CVS Pharmacy', 'Type': 'O', 'WikidataId': 'Q2078880', 'Confidence': 0.971, 'OccurrenceOffsets': [217], 'SurfaceForms': ['CVS']}, {'Label': \"Sam's Club\", 'Type': 'O', 'WikidataId': 'Q1972120', 'Confidence': 1.0, 'OccurrenceOffsets': [286], 'SurfaceForms': [\"Sam's Club\"]}, {'Label': 'Safeway Inc.', 'Type': 'O', 'WikidataId': 'Q1508234', 'Confidence': 1.0, 'OccurrenceOffsets': [257], 'SurfaceForms': ['Safeway']}, {'Label': 'Walmart', 'Type': 'O', 'WikidataId': 'Q483551', 'Confidence': 1.0, 'OccurrenceOffsets': [232], 'SurfaceForms': ['Walmart']}, {'Label': 'Target Corporation', 'Type': 'O', 'WikidataId': 'Q1046951', 'Confidence': 1.0, 'OccurrenceOffsets': [241], 'SurfaceForms': ['Target']}, {'Label': 'Rite Aid', 'Type': 'O', 'WikidataId': 'Q3433273', 'Confidence': 1.0, 'OccurrenceOffsets': [222], 'SurfaceForms': ['Rite Aid']}, {'Label': 'Walgreens', 'Type': 'O', 'WikidataId': 'Q1591889', 'Confidence': 1.0, 'OccurrenceOffsets': [206], 'SurfaceForms': ['Walgreens']}, {'Label': 'Kroger', 'Type': 'O', 'WikidataId': 'Q153417', 'Confidence': 1.0, 'OccurrenceOffsets': [249], 'SurfaceForms': ['Kroger']}, {'Label': 'Centers for Disease Control and Prevention', 'Type': 'O', 'WikidataId': 'Q583725', 'Confidence': 0.994, 'OccurrenceOffsets': [4], 'SurfaceForms': ['Centers for Disease Control and Prevention']}]\n",
      "N82836\n",
      "health\n",
      "medical\n",
      "Where to Get a Cheap Flu Shot: Walmart, CVS, Costco, and More\n",
      "The Centers for Disease Control and Prevention recommends that everyone over the age of 6 months get a dose of the influenza vaccine. To find out where to get cheap flu shots in 2019, we compared prices at Walgreens, CVS, Rite Aid, Walmart, Target, Kroger, Safeway, Meijer, Costco, and Sam's Club.\n",
      "https://assets.msn.com/labs/mind/AAI3QbY.html\n",
      "[{\"Label\": \"Costco\", \"Type\": \"O\", \"WikidataId\": \"Q715583\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [45], \"SurfaceForms\": [\"Costco\"]}, {\"Label\": \"CVS Pharmacy\", \"Type\": \"O\", \"WikidataId\": \"Q2078880\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [40], \"SurfaceForms\": [\"CVS\"]}, {\"Label\": \"Walmart\", \"Type\": \"O\", \"WikidataId\": \"Q483551\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [31], \"SurfaceForms\": [\"Walmart\"]}]\n",
      "[{\"Label\": \"Costco\", \"Type\": \"O\", \"WikidataId\": \"Q715583\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [274], \"SurfaceForms\": [\"Costco\"]}, {\"Label\": \"Meijer\", \"Type\": \"O\", \"WikidataId\": \"Q1917753\", \"Confidence\": 0.998, \"OccurrenceOffsets\": [266], \"SurfaceForms\": [\"Meijer\"]}, {\"Label\": \"CVS Pharmacy\", \"Type\": \"O\", \"WikidataId\": \"Q2078880\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [217], \"SurfaceForms\": [\"CVS\"]}, {\"Label\": \"Sam's Club\", \"Type\": \"O\", \"WikidataId\": \"Q1972120\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [286], \"SurfaceForms\": [\"Sam's Club\"]}, {\"Label\": \"Safeway Inc.\", \"Type\": \"O\", \"WikidataId\": \"Q1508234\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [257], \"SurfaceForms\": [\"Safeway\"]}, {\"Label\": \"Walmart\", \"Type\": \"O\", \"WikidataId\": \"Q483551\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [232], \"SurfaceForms\": [\"Walmart\"]}, {\"Label\": \"Target Corporation\", \"Type\": \"O\", \"WikidataId\": \"Q1046951\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [241], \"SurfaceForms\": [\"Target\"]}, {\"Label\": \"Rite Aid\", \"Type\": \"O\", \"WikidataId\": \"Q3433273\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [222], \"SurfaceForms\": [\"Rite Aid\"]}, {\"Label\": \"Walgreens\", \"Type\": \"O\", \"WikidataId\": \"Q1591889\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [206], \"SurfaceForms\": [\"Walgreens\"]}, {\"Label\": \"Kroger\", \"Type\": \"O\", \"WikidataId\": \"Q153417\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [249], \"SurfaceForms\": [\"Kroger\"]}, {\"Label\": \"Centers for Disease Control and Prevention\", \"Type\": \"O\", \"WikidataId\": \"Q583725\", \"Confidence\": 0.994, \"OccurrenceOffsets\": [4], \"SurfaceForms\": [\"Centers for Disease Control and Prevention\"]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "# emb_size = 2\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "# emb = [empty] * emb_height\n",
    "\n",
    "title_entity_lens = []\n",
    "abstract_entity_lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title_entities, abstract_entities = l[0], l[-2], l[-1]\n",
    "    title_entities = json.loads(title_entities)\n",
    "    abstract_entities = json.loads(abstract_entities)\n",
    "    title_entity_lens +=  [len(title_entities)]\n",
    "    abstract_entity_lens += [len(abstract_entities)]\n",
    "    if len(abstract_entities) > 10:\n",
    "      print(line)\n",
    "      print(title_entities)\n",
    "      print(abstract_entities)\n",
    "      print('\\n'.join(l))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1927917839529372"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(title_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(title_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(title_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9756095690256867"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abstract_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(abstract_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(abstract_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130379/130379 [00:03<00:00, 35555.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1    0    0 ...    0    0    0]\n",
      " [   1    0    0 ...    0    0    0]\n",
      " [   1    0    0 ...    0    0    0]\n",
      " ...\n",
      " [   1    0    0 ...    0    0    0]\n",
      " [   1    0    0 ...    5    0    0]\n",
      " [1840 9673    0 ...    0    0    0]] (130381, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "# emb_size = 2\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "entity_vocab = gezi.Vocab('../input/entity.txt')\n",
    "entity_type_vocab = gezi.Vocab('../input/entity_type.txt')\n",
    "\n",
    "# [title_entities, abstract_entities, title_etypes, abstract_entities]\n",
    "# 9 and 30\n",
    "MAX_TITLE_ENTITIES = 5\n",
    "MAX_ABSTRACT_ENTITIES = 5\n",
    "\n",
    "ENTITY_LEN = MAX_TITLE_ENTITIES + MAX_ABSTRACT_ENTITIES\n",
    "emb_size = ENTITY_LEN * 2\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 1 \n",
    "empty[MAX_TITLE_ENTITIES] = 1\n",
    "emb = [empty.copy() for _ in range(emb_height)] \n",
    "\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title_entities, abstract_entities = l[0], l[-2], l[-1]\n",
    "    did = vocab.id(did)\n",
    "    title_entities = json.loads(title_entities)\n",
    "    abstract_entities = json.loads(abstract_entities)\n",
    "    for i, m in enumerate(title_entities):\n",
    "      if i >= MAX_TITLE_ENTITIES:\n",
    "        continue\n",
    "      emb[did][i] = entity_vocab.id(m['WikidataId'])\n",
    "      emb[did][ENTITY_LEN + i] = entity_type_vocab.id(m['Type'])\n",
    "    for i, m in enumerate(abstract_entities):\n",
    "      if i >= MAX_ABSTRACT_ENTITIES:\n",
    "        continue\n",
    "      emb[did][MAX_TITLE_ENTITIES + i] = entity_vocab.id(m['WikidataId'])\n",
    "      emb[did][ENTITY_LEN + MAX_TITLE_ENTITIES + i] = entity_type_vocab.id(m['Type'])     \n",
    "emb = np.asarray(emb)\n",
    "print(emb, emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../input/entity_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30629923071613196"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(emb > 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([424,   0,   0,   0,   0, 254, 713, 120, 260, 532,   2,   0,   0,\n",
       "         0,   0,   2,   2,   2,   2,   2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'sub_cat', 'title', 'abstract', 'body', 'title_uncased', 'abstract_uncased', 'body_uncased']\n",
      "[1, 1, 32, 64, 512, 32, 64, 128]\n",
      "834\n",
      "(130381, 2)\n",
      "(130381, 32)\n",
      "(130381, 64)\n",
      "(130381, 512)\n",
      "(130381, 32)\n",
      "(130381, 64)\n",
      "(130381, 128)\n"
     ]
    }
   ],
   "source": [
    "# keys = ['cat', 'title', 'abstract', 'body', 'title_uncased']\n",
    "keys = ['cat', 'title', 'abstract', 'body', 'title_uncased', 'abstract_uncased', 'body_uncased']\n",
    "feats_ = [ ]\n",
    "flens_ = []\n",
    "\n",
    "for key in keys:\n",
    "  feats_ += fnames[key]\n",
    "  flens_ += flens[key]\n",
    "  \n",
    "print(feats_)\n",
    "print(flens_)\n",
    "print(sum(flens_))\n",
    "\n",
    "embs = []\n",
    "for key in keys:\n",
    "  emb = np.load(f'../input/{key}_lookup.npy')\n",
    "  print(emb.shape)\n",
    "  embs += [emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    1,  101, ...,    0,    0,    0],\n",
       "       [   1,    1,  101, ...,    0,    0,    0],\n",
       "       [   3,   40,  101, ..., 3679, 2739,  102],\n",
       "       ...,\n",
       "       [  10,  104,  101, ..., 3247, 1012,  102],\n",
       "       [   2,    3,  101, ..., 5500, 2011,  102],\n",
       "       [   2,   19,  101, ..., 5500, 2011,  102]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = np.concatenate(embs, 1)\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130381, 834)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../input/doc_lookup.npy', emb)\n",
    "np.save('../input/doc_fnames.npy', np.asarray(feats_))\n",
    "np.save('../input/doc_flens.npy', np.asarray(flens_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entity emb from wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50927 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[[ 0.04096048 -0.03556133 -0.04408021 ... -0.0022821  -0.02009994\n",
      "   0.02849957]\n",
      " [-0.04931467 -0.02747755 -0.01376152 ... -0.04337468 -0.04510694\n",
      "  -0.03300745]\n",
      " [-0.00224096 -0.0357698  -0.00430866 ... -0.03357329 -0.03051395\n",
      "   0.04724857]\n",
      " ...\n",
      " [-0.04139754  0.00033464 -0.01594218 ... -0.0262714   0.04772429\n",
      "   0.01051264]\n",
      " [ 0.00095603  0.01616765 -0.03987193 ...  0.03548088  0.0228728\n",
      "   0.00683941]\n",
      " [-0.03737675 -0.02940915  0.03667745 ...  0.00616488  0.00748581\n",
      "  -0.04651056]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 42007/50927 [00:02<00:00, 18099.00it/s]\n",
      " 65%|██████▌   | 33310/50927 [00:00<00:00, 75491.09it/s]\n",
      " 92%|█████████▏| 46807/50927 [00:00<00:00, 56981.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04096048 -0.03556133 -0.04408021 ... -0.0022821  -0.02009994\n",
      "   0.02849957]\n",
      " [-0.04931467 -0.02747755 -0.01376152 ... -0.04337468 -0.04510694\n",
      "  -0.03300745]\n",
      " [ 0.001677   -0.069251    0.00074    ... -0.05406     0.003242\n",
      "  -0.006887  ]\n",
      " ...\n",
      " [ 0.02741     0.019489    0.005972   ... -0.041304    0.007908\n",
      "   0.009783  ]\n",
      " [ 0.037157   -0.016108   -0.01267    ... -0.004157   -0.008539\n",
      "   0.050533  ]\n",
      " [ 0.040347    0.054959    0.084386   ... -0.066457   -0.03859\n",
      "   0.001724  ]]\n",
      "[[ 0.14022623 -0.12174249 -0.15090647 ... -0.00781266 -0.06881118\n",
      "   0.09756688]\n",
      " [-0.17931151 -0.09991027 -0.05003782 ... -0.15771332 -0.16401193\n",
      "  -0.12001736]\n",
      " [ 0.00335602 -0.13858546  0.00148089 ... -0.10818515  0.00648791\n",
      "  -0.0137823 ]\n",
      " ...\n",
      " [ 0.09532903  0.06778065  0.02076997 ... -0.14365087  0.02750317\n",
      "   0.03402422]\n",
      " [ 0.09275387 -0.0402099  -0.03162773 ... -0.01037699 -0.02131564\n",
      "   0.12614397]\n",
      " [ 0.12996376  0.17703121  0.27182    ... -0.214068   -0.1243042\n",
      "   0.00555326]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "vocab_file = '../input/entity.txt'\n",
    "vocab = gezi.Vocab(vocab_file)\n",
    "emb_height = vocab.size()\n",
    "\n",
    "emb_size = len(open('../input/train/entity_embedding.vec').readline().strip().split()) - 1\n",
    "print(emb_size)\n",
    "\n",
    "emb = np.random.uniform(-0.05, 0.05,(emb_height, emb_size))\n",
    "print(emb)\n",
    "\n",
    "emb = list(emb)\n",
    "\n",
    "files = [\n",
    "  '../input/train/entity_embedding.vec', \n",
    "  '../input/dev/entity_embedding.vec', \n",
    "  '../input/test/entity_embedding.vec'\n",
    "]\n",
    "\n",
    "entities = set()\n",
    "for file_ in files:\n",
    "  for line in tqdm(open(file_), total=emb_height):\n",
    "    l = line.strip().split()\n",
    "    entity, vals = l[0], l[1:]\n",
    "    if entity in entities:\n",
    "      continue\n",
    "    entities.add(entity)\n",
    "    vals = np.asarray(list(map(float, vals)))\n",
    "    #vals = normalize(np.reshape(vals, (1,-1)))\n",
    "    #vals /= np.sqrt(emb_size)\n",
    "    vals = np.reshape(vals, (-1,))\n",
    "    emb[vocab.id(entity)] = vals\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "\n",
    "#emb = normalize(emb)\n",
    "\n",
    "np.save('../input/entity_emb.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50927 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[[-0.00154117  0.03035518 -0.00589259 ... -0.04094373  0.04299281\n",
      "  -0.02494033]\n",
      " [ 0.01657999 -0.04411747  0.04258725 ... -0.0143552  -0.04014027\n",
      "  -0.02701598]\n",
      " [-0.01651694  0.01509018 -0.04305561 ...  0.00110249  0.02015254\n",
      "   0.0299422 ]\n",
      " ...\n",
      " [ 0.00573689 -0.02462167  0.00071024 ...  0.03420342  0.03679753\n",
      "   0.01625067]\n",
      " [-0.02460548 -0.04921551 -0.02206481 ...  0.04289599 -0.01137452\n",
      "  -0.02312832]\n",
      " [ 0.01571464 -0.01823292  0.04533747 ...  0.00403212  0.03232957\n",
      "   0.03336391]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 42007/50927 [00:05<00:01, 7064.90it/s]\n",
      " 65%|██████▌   | 33310/50927 [00:00<00:00, 62116.54it/s]\n",
      " 92%|█████████▏| 46807/50927 [00:01<00:00, 28778.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00154117  0.03035518 -0.00589259 ... -0.04094373  0.04299281\n",
      "  -0.02494033]\n",
      " [ 0.01657999 -0.04411747  0.04258725 ... -0.0143552  -0.04014027\n",
      "  -0.02701598]\n",
      " [ 0.00335602 -0.13858546  0.00148089 ... -0.10818515  0.00648791\n",
      "  -0.0137823 ]\n",
      " ...\n",
      " [ 0.09532903  0.06778065  0.02076997 ... -0.14365087  0.02750317\n",
      "   0.03402422]\n",
      " [ 0.09275387 -0.0402099  -0.03162773 ... -0.01037699 -0.02131564\n",
      "   0.12614397]\n",
      " [ 0.12996376  0.17703121  0.27182    ... -0.214068   -0.1243042\n",
      "   0.00555326]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "vocab_file = '../input/entity.txt'\n",
    "vocab = gezi.Vocab(vocab_file)\n",
    "emb_height = vocab.size()\n",
    "\n",
    "emb_size = len(open('../input/train/entity_embedding.vec').readline().strip().split()) - 1\n",
    "print(emb_size)\n",
    "\n",
    "emb = np.random.uniform(-0.05, 0.05,(emb_height, emb_size))\n",
    "print(emb)\n",
    "\n",
    "emb = list(emb)\n",
    "\n",
    "files = [\n",
    "  '../input/train/entity_embedding.vec', \n",
    "  '../input/dev/entity_embedding.vec', \n",
    "  '../input/test/entity_embedding.vec'\n",
    "]\n",
    "\n",
    "entities = set()\n",
    "for file_ in files:\n",
    "  for line in tqdm(open(file_), total=emb_height):\n",
    "    l = line.strip().split()\n",
    "    entity, vals = l[0], l[1:]\n",
    "    if entity in entities:\n",
    "      continue\n",
    "    entities.add(entity)\n",
    "    vals = np.asarray(list(map(float, vals)))\n",
    "    vals = normalize(np.reshape(vals, (1,-1)))\n",
    "    #vals /= np.sqrt(emb_size)\n",
    "    vals = np.reshape(vals, (-1,))\n",
    "    emb[vocab.id(entity)] = vals\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "\n",
    "#emb = normalize(emb)\n",
    "\n",
    "np.save('../input/entity_emb2.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
