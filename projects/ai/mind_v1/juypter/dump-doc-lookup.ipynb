{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gezi\n",
    "from gezi import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_names = ['did', 'cat', 'sub_cat', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = {}\n",
    "flens = {}\n",
    "fnames['title'] = ['title']\n",
    "flens['title'] = [30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130379/130379 [00:48<00:00, 2661.37it/s]\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "emb_size = 30\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 1\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title = l[0], l[3]\n",
    "    if title:\n",
    "      tokens = tokenizer.encode(title)\n",
    "    else:\n",
    "      tokens = [1]\n",
    "    lens += [len(tokens[1:-1])]\n",
    "    tokens = gezi.pad(tokens[1:-1], emb_size)\n",
    "    emb[vocab.id(did)] = np.asarray(tokens)\n",
    "emb = np.asarray(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1     0     0 ...     0     0     0]\n",
      " [    1     0     0 ...     0     0     0]\n",
      " [ 1109  3128  1130 ...     0     0     0]\n",
      " ...\n",
      " [ 9743  1192 12958 ...     0     0     0]\n",
      " [ 2289 24409   117 ...     0     0     0]\n",
      " [21166   119  3254 ...     0     0     0]]\n",
      "(130381, 30)\n"
     ]
    }
   ],
   "source": [
    "print(emb)\n",
    "print(emb.shape)\n",
    "np.save(f'../input/title_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['title_albert'] = ['title_albert']\n",
    "flens['title_albert'] = [30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130379/130379 [00:26<00:00, 4907.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1     0     0 ...     0     0     0]\n",
      " [    1     0     0 ...     0     0     0]\n",
      " [    2    14   996 ...     0     0     0]\n",
      " ...\n",
      " [    2   378    42 ...     0     0     0]\n",
      " [    2   435 16121 ...     0     0     0]\n",
      " [    2 14481     9 ...     0     0     0]]\n",
      "(130381, 30)\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "model_name = 'albert-base-v2'\n",
    "# model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "emb_size = 30\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 1\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title = l[0], l[3]\n",
    "    if title:\n",
    "      tokens = tokenizer.encode(title)\n",
    "    else:\n",
    "      tokens = [1]\n",
    "    lens += [len(tokens)]\n",
    "    tokens = gezi.pad(tokens, emb_size)\n",
    "    emb[vocab.id(did)] = np.asarray(tokens)\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "print(emb.shape)\n",
    "np.save(f'../input/title_albert_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['title_uncased'] = ['title_uncased']\n",
    "flens['title_uncased'] = [30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8aae9b90f6345dda61ff38f90bd892c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130379.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[    1     0     0 ...     0     0     0]\n",
      " [    1     0     0 ...     0     0     0]\n",
      " [  101  1996  2739 ...     0     0     0]\n",
      " ...\n",
      " [  101  2323  2017 ...     0     0     0]\n",
      " [  101  2374 19236 ...     0     0     0]\n",
      " [  101 19193  1012 ...     0     0     0]]\n",
      "(130381, 30)\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "# model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "emb_size = 30\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 1\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title = l[0], l[3]\n",
    "    if title:\n",
    "      tokens = tokenizer.encode(title)\n",
    "    else:\n",
    "      tokens = [1]\n",
    "    lens += [len(tokens)]\n",
    "    tokens = gezi.pad(tokens, emb_size)\n",
    "    emb[vocab.id(did)] = np.asarray(tokens)\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "print(emb.shape)\n",
    "np.save(f'../input/title_uncased_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009587433559085435"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.asarray(lens)\n",
    "np.sum(lens > 30) / len(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75c7b97f997425180f11ef076a92e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-aa6998480cf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                     \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m                 )\n\u001b[1;32m    488\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, local_files_only)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         )\n\u001b[1;32m    256\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0ms3_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storing %s in cache at %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, user_agent)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEffectiveLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     )\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unexpected EOF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gezi/env/anaconda3/lib/python3.6/site-packages/OpenSSL/SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1813\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1814\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tovec(sent):\n",
    "  input_ids = torch.tensor(tokenizer.encode('[CLS] ' + sent)).unsqueeze(0)  # Batch size 1\n",
    "  outputs = model(input_ids)\n",
    "  last_hidden_states = outputs[0]\n",
    "  emb = last_hidden_states[0][0]\n",
    "  return emb.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/130379 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f5534a652c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0memb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtovec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-44c561a704b7>\u001b[0m in \u001b[0;36mtovec\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtovec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[CLS] '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Batch size 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "emb_size = 768\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0.] * emb_size\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title = l[0], l[3]\n",
    "    emb[vocab.id(did)] = tovec(title)\n",
    "emb = np.asarray(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cat sbucat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['cat'] = ['cat', 'sub_cat']\n",
    "flens['cat'] = [1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/130379 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130379/130379 [00:01<00:00, 109229.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   1]\n",
      " [  1   1]\n",
      " [  3  40]\n",
      " ...\n",
      " [ 10 104]\n",
      " [  2   3]\n",
      " [  2  19]]\n",
      "(130381, 2)\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "\n",
    "emb_size = 2\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "cat_vocab = gezi.Vocab('../input/cat.txt')\n",
    "scat_vocab = gezi.Vocab('../input/sub_cat.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "empty = [1] * emb_size\n",
    "emb = [empty] * emb_height\n",
    "\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, cat, sub_cat = l[0], l[1], l[2]\n",
    "    data = [cat_vocab.id(cat), scat_vocab.id(sub_cat)]\n",
    "    emb[vocab.id(did)] = np.asarray(data)\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "print(emb.shape)\n",
    "\n",
    "np.save(f'../input/cat_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['abstract'] = ['abstract']\n",
    "flens['abstract'] = [50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 715/130379 [00:00<01:32, 1405.23it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "  1%|          | 847/130379 [00:00<01:34, 1377.69it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "  3%|▎         | 3850/130379 [00:03<01:49, 1151.23it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      " 31%|███       | 40592/130379 [00:35<01:19, 1136.03it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      " 32%|███▏      | 42331/130379 [00:37<01:16, 1143.94it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n",
      " 40%|███▉      | 51520/130379 [00:45<01:14, 1062.56it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      " 42%|████▏     | 54223/130379 [00:48<01:13, 1029.61it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      " 49%|████▊     | 63349/130379 [00:56<01:03, 1049.50it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      " 57%|█████▋    | 74222/130379 [01:06<00:52, 1077.23it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      " 68%|██████▊   | 88037/130379 [01:18<00:38, 1110.17it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      " 91%|█████████ | 118962/130379 [01:46<00:10, 1048.68it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (944 > 512). Running this sequence through the model will result in indexing errors\n",
      " 92%|█████████▏| 119754/130379 [01:46<00:09, 1076.03it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (882 > 512). Running this sequence through the model will result in indexing errors\n",
      " 96%|█████████▌| 125037/130379 [01:51<00:04, 1113.99it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (747 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 130379/130379 [01:56<00:00, 1120.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1     0     0 ...     0     0     0]\n",
      " [    1     0     0 ...     0     0     0]\n",
      " [ 3128  1112  1562 ...     0     0     0]\n",
      " ...\n",
      " [ 1636  1132  1103 ...     0     0     0]\n",
      " [  123   118  1795 ...  1795 27019   117]\n",
      " [ 1109  1148  1226 ...  1509  3299  1406]]\n",
      "(130381, 50)\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "emb_size = 50\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 1\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, abstract = l[0], l[4]\n",
    "    if abstract:\n",
    "      tokens = tokenizer.encode(abstract)\n",
    "    else:\n",
    "      tokens = [1]\n",
    "    lens += [len(tokens[1:-1])]\n",
    "    tokens = gezi.pad(tokens[1:-1], emb_size)\n",
    "    emb[vocab.id(did)] = np.asarray(tokens)\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "print(emb.shape)\n",
    "\n",
    "np.save(f'../input/abstract_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['abstract_uncased'] = ['abstract_uncased']\n",
    "flens['abstract_uncased'] = [50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcadd8aa8364985b86a321dd2dbecb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130379.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (891 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (831 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (742 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[    1     0     0 ...     0     0     0]\n",
      " [    1     0     0 ...     0     0     0]\n",
      " [ 2739  2004  2464 ...     0     0     0]\n",
      " ...\n",
      " [ 2122  2024  1996 ...     0     0     0]\n",
      " [ 1016  1011  2703 ... 25623  1010  2410]\n",
      " [ 1996  2034  2112 ...  2345  2327  2322]]\n",
      "(130381, 50)\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "emb_size = 50\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 1\n",
    "\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, abstract = l[0], l[4]\n",
    "    if abstract:\n",
    "      tokens = tokenizer.encode(abstract)\n",
    "    else:\n",
    "      tokens = [1]\n",
    "    lens += [len(tokens[1:-1])]\n",
    "    tokens = gezi.pad(tokens[1:-1], emb_size)\n",
    "    emb[vocab.id(did)] = np.asarray(tokens)\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "print(emb.shape)\n",
    "\n",
    "np.save(f'../input/abstract_uncased_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.402311721979764\n",
      "0\n",
      "944\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(lens))\n",
    "print(np.min(lens))\n",
    "print(np.max(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['body'] = ['body']\n",
    "flens['body'] = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/msn.json',\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "emb_size = 100\n",
    "\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 1\n",
    "emb = [empty] * emb_height\n",
    "lens = []\n",
    "\n",
    "bodys = json.load(open(files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c404ece79b4470d83002115388f0ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130379.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nid2did = {}\n",
    "file = '../input/news/news.tsv'\n",
    "total = len(open(file).readlines())\n",
    "for line in tqdm(open(file), total=total):\n",
    "  l = line.strip().split('\\t')\n",
    "  did, url = l[0], l[5]\n",
    "  nid = url.split('/')[-1].split('.')[0]\n",
    "  nid2did[nid] = did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 9288/130379 [00:20<04:13, 477.63it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      " 31%|███       | 40685/130379 [01:28<03:10, 470.86it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      " 40%|███▉      | 51521/130379 [01:51<02:51, 460.66it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      " 42%|████▏     | 54259/130379 [01:57<02:57, 429.02it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      " 68%|██████▊   | 88063/130379 [03:09<01:26, 488.31it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 130379/130379 [04:37<00:00, 469.33it/s]\n"
     ]
    }
   ],
   "source": [
    "lens = []\n",
    "for body_ in tqdm(bodys, total=len(bodys)):\n",
    "  body = ' '.join(body_['body'])\n",
    "  body = ' '.join(body.split()[:100])\n",
    "  tokens = tokenizer.encode(body)\n",
    "  totkens = tokens[1:-1]\n",
    "  lens += [len(tokens)]\n",
    "  did = vocab.id(nid2did[body_['nid']])\n",
    "  tokens = gezi.pad(tokens[1:-1], emb_size)\n",
    "  emb[did] = np.asarray(tokens)\n",
    "#   if len(lens) > 100:\n",
    "#     break\n",
    "emb = np.asarray(emb)\n",
    "emb\n",
    "np.save(f'../input/body_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['body_uncased'] = ['body_uncased']\n",
    "flens['body_uncased'] = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d424d0b3ec684812af674a9c014170ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130379.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lens = []\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "for body_ in tqdm(bodys, total=len(bodys)):\n",
    "  body = ' '.join(body_['body'])\n",
    "  body = ' '.join(body.split()[:100])\n",
    "  tokens = tokenizer.encode(body)\n",
    "  totkens = tokens[1:-1]\n",
    "  lens += [len(tokens)]\n",
    "  did = vocab.id(nid2did[body_['nid']])\n",
    "  tokens = gezi.pad(tokens[1:-1], emb_size)\n",
    "  emb[did] = np.asarray(tokens)\n",
    "#   if len(lens) > 100:\n",
    "#     break\n",
    "emb = np.asarray(emb)\n",
    "emb\n",
    "np.save(f'../input/body_uncased_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.96120540884651"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames['entity'] = ['title_entities', 'abstract_entities', 'title_entity_types', 'abstract_entity_types']\n",
    "flens['entity'] = [5, 5, 5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 331/130379 [00:00<00:02, 49564.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N82836\thealth\tmedical\tWhere to Get a Cheap Flu Shot: Walmart, CVS, Costco, and More\tThe Centers for Disease Control and Prevention recommends that everyone over the age of 6 months get a dose of the influenza vaccine. To find out where to get cheap flu shots in 2019, we compared prices at Walgreens, CVS, Rite Aid, Walmart, Target, Kroger, Safeway, Meijer, Costco, and Sam's Club.\thttps://assets.msn.com/labs/mind/AAI3QbY.html\t[{\"Label\": \"Costco\", \"Type\": \"O\", \"WikidataId\": \"Q715583\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [45], \"SurfaceForms\": [\"Costco\"]}, {\"Label\": \"CVS Pharmacy\", \"Type\": \"O\", \"WikidataId\": \"Q2078880\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [40], \"SurfaceForms\": [\"CVS\"]}, {\"Label\": \"Walmart\", \"Type\": \"O\", \"WikidataId\": \"Q483551\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [31], \"SurfaceForms\": [\"Walmart\"]}]\t[{\"Label\": \"Costco\", \"Type\": \"O\", \"WikidataId\": \"Q715583\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [274], \"SurfaceForms\": [\"Costco\"]}, {\"Label\": \"Meijer\", \"Type\": \"O\", \"WikidataId\": \"Q1917753\", \"Confidence\": 0.998, \"OccurrenceOffsets\": [266], \"SurfaceForms\": [\"Meijer\"]}, {\"Label\": \"CVS Pharmacy\", \"Type\": \"O\", \"WikidataId\": \"Q2078880\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [217], \"SurfaceForms\": [\"CVS\"]}, {\"Label\": \"Sam's Club\", \"Type\": \"O\", \"WikidataId\": \"Q1972120\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [286], \"SurfaceForms\": [\"Sam's Club\"]}, {\"Label\": \"Safeway Inc.\", \"Type\": \"O\", \"WikidataId\": \"Q1508234\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [257], \"SurfaceForms\": [\"Safeway\"]}, {\"Label\": \"Walmart\", \"Type\": \"O\", \"WikidataId\": \"Q483551\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [232], \"SurfaceForms\": [\"Walmart\"]}, {\"Label\": \"Target Corporation\", \"Type\": \"O\", \"WikidataId\": \"Q1046951\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [241], \"SurfaceForms\": [\"Target\"]}, {\"Label\": \"Rite Aid\", \"Type\": \"O\", \"WikidataId\": \"Q3433273\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [222], \"SurfaceForms\": [\"Rite Aid\"]}, {\"Label\": \"Walgreens\", \"Type\": \"O\", \"WikidataId\": \"Q1591889\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [206], \"SurfaceForms\": [\"Walgreens\"]}, {\"Label\": \"Kroger\", \"Type\": \"O\", \"WikidataId\": \"Q153417\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [249], \"SurfaceForms\": [\"Kroger\"]}, {\"Label\": \"Centers for Disease Control and Prevention\", \"Type\": \"O\", \"WikidataId\": \"Q583725\", \"Confidence\": 0.994, \"OccurrenceOffsets\": [4], \"SurfaceForms\": [\"Centers for Disease Control and Prevention\"]}]\n",
      "\n",
      "[{'Label': 'Costco', 'Type': 'O', 'WikidataId': 'Q715583', 'Confidence': 1.0, 'OccurrenceOffsets': [45], 'SurfaceForms': ['Costco']}, {'Label': 'CVS Pharmacy', 'Type': 'O', 'WikidataId': 'Q2078880', 'Confidence': 0.971, 'OccurrenceOffsets': [40], 'SurfaceForms': ['CVS']}, {'Label': 'Walmart', 'Type': 'O', 'WikidataId': 'Q483551', 'Confidence': 1.0, 'OccurrenceOffsets': [31], 'SurfaceForms': ['Walmart']}]\n",
      "[{'Label': 'Costco', 'Type': 'O', 'WikidataId': 'Q715583', 'Confidence': 1.0, 'OccurrenceOffsets': [274], 'SurfaceForms': ['Costco']}, {'Label': 'Meijer', 'Type': 'O', 'WikidataId': 'Q1917753', 'Confidence': 0.998, 'OccurrenceOffsets': [266], 'SurfaceForms': ['Meijer']}, {'Label': 'CVS Pharmacy', 'Type': 'O', 'WikidataId': 'Q2078880', 'Confidence': 0.971, 'OccurrenceOffsets': [217], 'SurfaceForms': ['CVS']}, {'Label': \"Sam's Club\", 'Type': 'O', 'WikidataId': 'Q1972120', 'Confidence': 1.0, 'OccurrenceOffsets': [286], 'SurfaceForms': [\"Sam's Club\"]}, {'Label': 'Safeway Inc.', 'Type': 'O', 'WikidataId': 'Q1508234', 'Confidence': 1.0, 'OccurrenceOffsets': [257], 'SurfaceForms': ['Safeway']}, {'Label': 'Walmart', 'Type': 'O', 'WikidataId': 'Q483551', 'Confidence': 1.0, 'OccurrenceOffsets': [232], 'SurfaceForms': ['Walmart']}, {'Label': 'Target Corporation', 'Type': 'O', 'WikidataId': 'Q1046951', 'Confidence': 1.0, 'OccurrenceOffsets': [241], 'SurfaceForms': ['Target']}, {'Label': 'Rite Aid', 'Type': 'O', 'WikidataId': 'Q3433273', 'Confidence': 1.0, 'OccurrenceOffsets': [222], 'SurfaceForms': ['Rite Aid']}, {'Label': 'Walgreens', 'Type': 'O', 'WikidataId': 'Q1591889', 'Confidence': 1.0, 'OccurrenceOffsets': [206], 'SurfaceForms': ['Walgreens']}, {'Label': 'Kroger', 'Type': 'O', 'WikidataId': 'Q153417', 'Confidence': 1.0, 'OccurrenceOffsets': [249], 'SurfaceForms': ['Kroger']}, {'Label': 'Centers for Disease Control and Prevention', 'Type': 'O', 'WikidataId': 'Q583725', 'Confidence': 0.994, 'OccurrenceOffsets': [4], 'SurfaceForms': ['Centers for Disease Control and Prevention']}]\n",
      "N82836\n",
      "health\n",
      "medical\n",
      "Where to Get a Cheap Flu Shot: Walmart, CVS, Costco, and More\n",
      "The Centers for Disease Control and Prevention recommends that everyone over the age of 6 months get a dose of the influenza vaccine. To find out where to get cheap flu shots in 2019, we compared prices at Walgreens, CVS, Rite Aid, Walmart, Target, Kroger, Safeway, Meijer, Costco, and Sam's Club.\n",
      "https://assets.msn.com/labs/mind/AAI3QbY.html\n",
      "[{\"Label\": \"Costco\", \"Type\": \"O\", \"WikidataId\": \"Q715583\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [45], \"SurfaceForms\": [\"Costco\"]}, {\"Label\": \"CVS Pharmacy\", \"Type\": \"O\", \"WikidataId\": \"Q2078880\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [40], \"SurfaceForms\": [\"CVS\"]}, {\"Label\": \"Walmart\", \"Type\": \"O\", \"WikidataId\": \"Q483551\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [31], \"SurfaceForms\": [\"Walmart\"]}]\n",
      "[{\"Label\": \"Costco\", \"Type\": \"O\", \"WikidataId\": \"Q715583\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [274], \"SurfaceForms\": [\"Costco\"]}, {\"Label\": \"Meijer\", \"Type\": \"O\", \"WikidataId\": \"Q1917753\", \"Confidence\": 0.998, \"OccurrenceOffsets\": [266], \"SurfaceForms\": [\"Meijer\"]}, {\"Label\": \"CVS Pharmacy\", \"Type\": \"O\", \"WikidataId\": \"Q2078880\", \"Confidence\": 0.971, \"OccurrenceOffsets\": [217], \"SurfaceForms\": [\"CVS\"]}, {\"Label\": \"Sam's Club\", \"Type\": \"O\", \"WikidataId\": \"Q1972120\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [286], \"SurfaceForms\": [\"Sam's Club\"]}, {\"Label\": \"Safeway Inc.\", \"Type\": \"O\", \"WikidataId\": \"Q1508234\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [257], \"SurfaceForms\": [\"Safeway\"]}, {\"Label\": \"Walmart\", \"Type\": \"O\", \"WikidataId\": \"Q483551\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [232], \"SurfaceForms\": [\"Walmart\"]}, {\"Label\": \"Target Corporation\", \"Type\": \"O\", \"WikidataId\": \"Q1046951\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [241], \"SurfaceForms\": [\"Target\"]}, {\"Label\": \"Rite Aid\", \"Type\": \"O\", \"WikidataId\": \"Q3433273\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [222], \"SurfaceForms\": [\"Rite Aid\"]}, {\"Label\": \"Walgreens\", \"Type\": \"O\", \"WikidataId\": \"Q1591889\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [206], \"SurfaceForms\": [\"Walgreens\"]}, {\"Label\": \"Kroger\", \"Type\": \"O\", \"WikidataId\": \"Q153417\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [249], \"SurfaceForms\": [\"Kroger\"]}, {\"Label\": \"Centers for Disease Control and Prevention\", \"Type\": \"O\", \"WikidataId\": \"Q583725\", \"Confidence\": 0.994, \"OccurrenceOffsets\": [4], \"SurfaceForms\": [\"Centers for Disease Control and Prevention\"]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "# emb_size = 2\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "# emb = [empty] * emb_height\n",
    "\n",
    "title_entity_lens = []\n",
    "abstract_entity_lens = []\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title_entities, abstract_entities = l[0], l[-2], l[-1]\n",
    "    title_entities = json.loads(title_entities)\n",
    "    abstract_entities = json.loads(abstract_entities)\n",
    "    title_entity_lens +=  [len(title_entities)]\n",
    "    abstract_entity_lens += [len(abstract_entities)]\n",
    "    if len(abstract_entities) > 10:\n",
    "      print(line)\n",
    "      print(title_entities)\n",
    "      print(abstract_entities)\n",
    "      print('\\n'.join(l))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1927917839529372"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(title_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(title_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(title_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9756095690256867"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abstract_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(abstract_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(abstract_entity_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130379/130379 [00:03<00:00, 35555.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1    0    0 ...    0    0    0]\n",
      " [   1    0    0 ...    0    0    0]\n",
      " [   1    0    0 ...    0    0    0]\n",
      " ...\n",
      " [   1    0    0 ...    0    0    0]\n",
      " [   1    0    0 ...    5    0    0]\n",
      " [1840 9673    0 ...    0    0    0]] (130381, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "          '../input/news/news.tsv',\n",
    "        ]\n",
    "\n",
    "# emb_size = 2\n",
    "vocab = gezi.Vocab('../input/did.txt')\n",
    "emb_height = vocab.size()\n",
    "print(emb_height)\n",
    "\n",
    "entity_vocab = gezi.Vocab('../input/entity.txt')\n",
    "entity_type_vocab = gezi.Vocab('../input/entity_type.txt')\n",
    "\n",
    "# [title_entities, abstract_entities, title_etypes, abstract_entities]\n",
    "# 9 and 30\n",
    "MAX_TITLE_ENTITIES = 5\n",
    "MAX_ABSTRACT_ENTITIES = 5\n",
    "\n",
    "ENTITY_LEN = MAX_TITLE_ENTITIES + MAX_ABSTRACT_ENTITIES\n",
    "emb_size = ENTITY_LEN * 2\n",
    "\n",
    "empty = [0] * emb_size\n",
    "empty[0] = 1 \n",
    "empty[MAX_TITLE_ENTITIES] = 1\n",
    "emb = [empty.copy() for _ in range(emb_height)] \n",
    "\n",
    "for file_ in files:\n",
    "  total = len(open(file_).readlines())\n",
    "  for line in tqdm(open(file_), total=total):\n",
    "    l = line.strip().split('\\t')\n",
    "    did, title_entities, abstract_entities = l[0], l[-2], l[-1]\n",
    "    did = vocab.id(did)\n",
    "    title_entities = json.loads(title_entities)\n",
    "    abstract_entities = json.loads(abstract_entities)\n",
    "    for i, m in enumerate(title_entities):\n",
    "      if i >= MAX_TITLE_ENTITIES:\n",
    "        continue\n",
    "      emb[did][i] = entity_vocab.id(m['WikidataId'])\n",
    "      emb[did][ENTITY_LEN + i] = entity_type_vocab.id(m['Type'])\n",
    "    for i, m in enumerate(abstract_entities):\n",
    "      if i >= MAX_ABSTRACT_ENTITIES:\n",
    "        continue\n",
    "      emb[did][MAX_TITLE_ENTITIES + i] = entity_vocab.id(m['WikidataId'])\n",
    "      emb[did][ENTITY_LEN + MAX_TITLE_ENTITIES + i] = entity_type_vocab.id(m['Type'])     \n",
    "emb = np.asarray(emb)\n",
    "print(emb, emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../input/entity_lookup.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30629923071613196"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(emb > 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([424,   0,   0,   0,   0, 254, 713, 120, 260, 532,   2,   0,   0,\n",
       "         0,   0,   2,   2,   2,   2,   2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'sub_cat', 'title', 'abstract', 'body', 'title_uncased', 'abstract_uncased', 'body_uncased']\n",
      "[1, 1, 30, 50, 100, 30, 50, 100]\n",
      "362\n",
      "(130381, 2)\n",
      "(130381, 30)\n",
      "(130381, 50)\n",
      "(130381, 100)\n",
      "(130381, 30)\n",
      "(130381, 50)\n",
      "(130381, 100)\n"
     ]
    }
   ],
   "source": [
    "# keys = ['cat', 'title', 'abstract', 'body', 'title_uncased']\n",
    "keys = ['cat', 'title', 'abstract', 'body', 'title_uncased', 'abstract_uncased', 'body_uncased']\n",
    "feats_ = [ ]\n",
    "flens_ = []\n",
    "\n",
    "for key in keys:\n",
    "  feats_ += fnames[key]\n",
    "  flens_ += flens[key]\n",
    "  \n",
    "print(feats_)\n",
    "print(flens_)\n",
    "print(sum(flens_))\n",
    "\n",
    "embs = []\n",
    "for key in keys:\n",
    "  emb = np.load(f'../input/{key}_lookup.npy')\n",
    "  print(emb.shape)\n",
    "  embs += [emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     1,     1, ...,     0,     0,     0],\n",
       "       [    1,     1,     1, ...,     0,     0,     0],\n",
       "       [    3,    40,  1109, ..., 10969,  4180,  4034],\n",
       "       ...,\n",
       "       [   10,   104,  9743, ...,  2130,  2029, 11033],\n",
       "       [    2,     3,  2289, ...,  1024,  2418,  1010],\n",
       "       [    2,    19, 21166, ...,  8183, 20685,  1010]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = np.concatenate(embs, 1)\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130381, 362)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../input/doc_lookup.npy', emb)\n",
    "np.save('../input/doc_fnames.npy', np.asarray(feats_))\n",
    "np.save('../input/doc_flens.npy', np.asarray(flens_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entity emb from wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50927 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[[ 0.04096048 -0.03556133 -0.04408021 ... -0.0022821  -0.02009994\n",
      "   0.02849957]\n",
      " [-0.04931467 -0.02747755 -0.01376152 ... -0.04337468 -0.04510694\n",
      "  -0.03300745]\n",
      " [-0.00224096 -0.0357698  -0.00430866 ... -0.03357329 -0.03051395\n",
      "   0.04724857]\n",
      " ...\n",
      " [-0.04139754  0.00033464 -0.01594218 ... -0.0262714   0.04772429\n",
      "   0.01051264]\n",
      " [ 0.00095603  0.01616765 -0.03987193 ...  0.03548088  0.0228728\n",
      "   0.00683941]\n",
      " [-0.03737675 -0.02940915  0.03667745 ...  0.00616488  0.00748581\n",
      "  -0.04651056]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 42007/50927 [00:02<00:00, 18099.00it/s]\n",
      " 65%|██████▌   | 33310/50927 [00:00<00:00, 75491.09it/s]\n",
      " 92%|█████████▏| 46807/50927 [00:00<00:00, 56981.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04096048 -0.03556133 -0.04408021 ... -0.0022821  -0.02009994\n",
      "   0.02849957]\n",
      " [-0.04931467 -0.02747755 -0.01376152 ... -0.04337468 -0.04510694\n",
      "  -0.03300745]\n",
      " [ 0.001677   -0.069251    0.00074    ... -0.05406     0.003242\n",
      "  -0.006887  ]\n",
      " ...\n",
      " [ 0.02741     0.019489    0.005972   ... -0.041304    0.007908\n",
      "   0.009783  ]\n",
      " [ 0.037157   -0.016108   -0.01267    ... -0.004157   -0.008539\n",
      "   0.050533  ]\n",
      " [ 0.040347    0.054959    0.084386   ... -0.066457   -0.03859\n",
      "   0.001724  ]]\n",
      "[[ 0.14022623 -0.12174249 -0.15090647 ... -0.00781266 -0.06881118\n",
      "   0.09756688]\n",
      " [-0.17931151 -0.09991027 -0.05003782 ... -0.15771332 -0.16401193\n",
      "  -0.12001736]\n",
      " [ 0.00335602 -0.13858546  0.00148089 ... -0.10818515  0.00648791\n",
      "  -0.0137823 ]\n",
      " ...\n",
      " [ 0.09532903  0.06778065  0.02076997 ... -0.14365087  0.02750317\n",
      "   0.03402422]\n",
      " [ 0.09275387 -0.0402099  -0.03162773 ... -0.01037699 -0.02131564\n",
      "   0.12614397]\n",
      " [ 0.12996376  0.17703121  0.27182    ... -0.214068   -0.1243042\n",
      "   0.00555326]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "vocab_file = '../input/entity.txt'\n",
    "vocab = gezi.Vocab(vocab_file)\n",
    "emb_height = vocab.size()\n",
    "\n",
    "emb_size = len(open('../input/train/entity_embedding.vec').readline().strip().split()) - 1\n",
    "print(emb_size)\n",
    "\n",
    "emb = np.random.uniform(-0.05, 0.05,(emb_height, emb_size))\n",
    "print(emb)\n",
    "\n",
    "emb = list(emb)\n",
    "\n",
    "files = [\n",
    "  '../input/train/entity_embedding.vec', \n",
    "  '../input/dev/entity_embedding.vec', \n",
    "  '../input/test/entity_embedding.vec'\n",
    "]\n",
    "\n",
    "entities = set()\n",
    "for file_ in files:\n",
    "  for line in tqdm(open(file_), total=emb_height):\n",
    "    l = line.strip().split()\n",
    "    entity, vals = l[0], l[1:]\n",
    "    if entity in entities:\n",
    "      continue\n",
    "    entities.add(entity)\n",
    "    vals = np.asarray(list(map(float, vals)))\n",
    "    #vals = normalize(np.reshape(vals, (1,-1)))\n",
    "    #vals /= np.sqrt(emb_size)\n",
    "    vals = np.reshape(vals, (-1,))\n",
    "    emb[vocab.id(entity)] = vals\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "\n",
    "#emb = normalize(emb)\n",
    "\n",
    "np.save('../input/entity_emb.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50927 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[[-0.00154117  0.03035518 -0.00589259 ... -0.04094373  0.04299281\n",
      "  -0.02494033]\n",
      " [ 0.01657999 -0.04411747  0.04258725 ... -0.0143552  -0.04014027\n",
      "  -0.02701598]\n",
      " [-0.01651694  0.01509018 -0.04305561 ...  0.00110249  0.02015254\n",
      "   0.0299422 ]\n",
      " ...\n",
      " [ 0.00573689 -0.02462167  0.00071024 ...  0.03420342  0.03679753\n",
      "   0.01625067]\n",
      " [-0.02460548 -0.04921551 -0.02206481 ...  0.04289599 -0.01137452\n",
      "  -0.02312832]\n",
      " [ 0.01571464 -0.01823292  0.04533747 ...  0.00403212  0.03232957\n",
      "   0.03336391]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 42007/50927 [00:05<00:01, 7064.90it/s]\n",
      " 65%|██████▌   | 33310/50927 [00:00<00:00, 62116.54it/s]\n",
      " 92%|█████████▏| 46807/50927 [00:01<00:00, 28778.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00154117  0.03035518 -0.00589259 ... -0.04094373  0.04299281\n",
      "  -0.02494033]\n",
      " [ 0.01657999 -0.04411747  0.04258725 ... -0.0143552  -0.04014027\n",
      "  -0.02701598]\n",
      " [ 0.00335602 -0.13858546  0.00148089 ... -0.10818515  0.00648791\n",
      "  -0.0137823 ]\n",
      " ...\n",
      " [ 0.09532903  0.06778065  0.02076997 ... -0.14365087  0.02750317\n",
      "   0.03402422]\n",
      " [ 0.09275387 -0.0402099  -0.03162773 ... -0.01037699 -0.02131564\n",
      "   0.12614397]\n",
      " [ 0.12996376  0.17703121  0.27182    ... -0.214068   -0.1243042\n",
      "   0.00555326]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "vocab_file = '../input/entity.txt'\n",
    "vocab = gezi.Vocab(vocab_file)\n",
    "emb_height = vocab.size()\n",
    "\n",
    "emb_size = len(open('../input/train/entity_embedding.vec').readline().strip().split()) - 1\n",
    "print(emb_size)\n",
    "\n",
    "emb = np.random.uniform(-0.05, 0.05,(emb_height, emb_size))\n",
    "print(emb)\n",
    "\n",
    "emb = list(emb)\n",
    "\n",
    "files = [\n",
    "  '../input/train/entity_embedding.vec', \n",
    "  '../input/dev/entity_embedding.vec', \n",
    "  '../input/test/entity_embedding.vec'\n",
    "]\n",
    "\n",
    "entities = set()\n",
    "for file_ in files:\n",
    "  for line in tqdm(open(file_), total=emb_height):\n",
    "    l = line.strip().split()\n",
    "    entity, vals = l[0], l[1:]\n",
    "    if entity in entities:\n",
    "      continue\n",
    "    entities.add(entity)\n",
    "    vals = np.asarray(list(map(float, vals)))\n",
    "    vals = normalize(np.reshape(vals, (1,-1)))\n",
    "    #vals /= np.sqrt(emb_size)\n",
    "    vals = np.reshape(vals, (-1,))\n",
    "    emb[vocab.id(entity)] = vals\n",
    "\n",
    "emb = np.asarray(emb)\n",
    "print(emb)\n",
    "\n",
    "#emb = normalize(emb)\n",
    "\n",
    "np.save('../input/entity_emb2.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
