{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow import keras\n",
    "\n",
    "def swish_activation(x):\n",
    "  return (K.sigmoid(x) * x)\n",
    "\n",
    "class FixedDropout(tf.keras.layers.Dropout):\n",
    "  def _get_noise_shape(self, inputs):\n",
    "    if self.noise_shape is None:\n",
    "      return self.noise_shape\n",
    "\n",
    "    symbolic_shape = K.shape(inputs)\n",
    "    noise_shape = [symbolic_shape[axis] if shape is None else shape\n",
    "                    for axis, shape in enumerate(self.noise_shape)]\n",
    "    return tuple(noise_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "class wBiFPNAdd(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-4, **kwargs):\n",
    "        super(wBiFPNAdd, self).__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        num_in = len(input_shape)\n",
    "        self.w = self.add_weight(name=self.name,\n",
    "                                 shape=(num_in,),\n",
    "                                 initializer=keras.initializers.constant(1 / num_in),\n",
    "                                 trainable=True,\n",
    "                                 dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        w = keras.activations.relu(self.w)\n",
    "        # print(w)\n",
    "        # print(inputs)\n",
    "        x = tf.reduce_sum([w[i] * inputs[i] for i in range(len(inputs))], axis=0)\n",
    "        x = x / (tf.reduce_sum(w) + self.epsilon)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(wBiFPNAdd, self).get_config()\n",
    "        config.update({\n",
    "            'epsilon': self.epsilon\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(tf.keras.layers.BatchNormalization):\n",
    "  def __init__(self, **kwargs):\n",
    "    if not kwargs.get('name', None):\n",
    "      kwargs['name'] = 'tpu_batch_normalization'\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "  def call(self, inputs, training=None):\n",
    "    outputs = super().call(inputs, training)\n",
    "    for u in self.updates:\n",
    "      tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, u)\n",
    "    return outputs\n",
    "  \n",
    "def batch_norm_class(is_training, strategy=None):\n",
    "    return BatchNormalization\n",
    "  \n",
    "def build_batch_norm(is_training_bn,\n",
    "                     beta_initializer='zeros',\n",
    "                     gamma_initializer='ones',\n",
    "                     data_format='channels_last',\n",
    "                     momentum=0.99,\n",
    "                     epsilon=1e-3,\n",
    "                     strategy=None,\n",
    "                     name='tpu_batch_normalization'):\n",
    "  axis = 1 if data_format == 'channels_first' else -1\n",
    "  batch_norm_class_ = batch_norm_class(is_training_bn, strategy)\n",
    "\n",
    "  bn_layer = batch_norm_class_(\n",
    "      axis=axis,\n",
    "      momentum=momentum,\n",
    "      epsilon=epsilon,\n",
    "      center=True,\n",
    "      scale=True,\n",
    "      beta_initializer=beta_initializer,\n",
    "      gamma_initializer=gamma_initializer,\n",
    "      name=name)\n",
    "\n",
    "  return bn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "class SegmentationHead(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               num_classes,\n",
    "               num_filters,\n",
    "               min_level,\n",
    "               max_level,\n",
    "               data_format,\n",
    "               is_training_bn,\n",
    "               act_type,\n",
    "               strategy,\n",
    "               head_strategy='transpose',\n",
    "               upsampling_last=False,\n",
    "               **kwargs):\n",
    "\n",
    "    super(SegmentationHead, self).__init__(**kwargs)\n",
    "\n",
    "    self.num_classes = num_classes\n",
    "    self.num_filters = num_filters\n",
    "    self.min_level = min_level\n",
    "    self.max_level = max_level\n",
    "    self.data_format = data_format\n",
    "    self.is_training_bn = is_training_bn\n",
    "    self.act_type = act_type\n",
    "    self.strategy = strategy\n",
    "    self.head_strategy = head_strategy\n",
    "    self.upsampling_last = upsampling_last\n",
    "\n",
    "    self.act_type = act_type\n",
    "    self.con2d_ts = []\n",
    "    self.con2d_t_bns = []\n",
    "    max_level += 1  # chg 20201205 add just like unet\n",
    "    ks = 3\n",
    "    head_stride = 2\n",
    "    for i in range(max_level - min_level):\n",
    "      self.con2d_ts.append(\n",
    "          tf.keras.layers.Conv2DTranspose(\n",
    "              num_filters,\n",
    "              ks,\n",
    "              strides=2,\n",
    "              padding='same',\n",
    "              data_format=data_format,\n",
    "              use_bias=False,\n",
    "              name=f'cond2d_ts_{i}'))\n",
    "      self.con2d_t_bns.append(\n",
    "          build_batch_norm(\n",
    "              is_training_bn=is_training_bn,\n",
    "              data_format=data_format,\n",
    "              strategy=strategy,\n",
    "              name=f'bn_{i}'))\n",
    "\n",
    "    # TODO 封装一个upsampling 传递参数 upsampling,resize,dconv/transpose\n",
    "    self.head_process = None\n",
    "    head_strategy = self.head_strategy\n",
    "    if head_strategy == 'transpose':\n",
    "        self.head_process  = tf.keras.layers.Conv2DTranspose(\n",
    "            num_classes, ks, strides=head_stride, padding='same', name='head_transpose')\n",
    "        self.head_conv = None\n",
    "    elif head_strategy == 'upsampling':\n",
    "        scale_size = 4\n",
    "        self.head_process = tf.keras.layers.UpSampling2D(size=scale_size, interpolation='bilinear')\n",
    "        self.head_conv = tf.keras.layers.Conv2D(num_classes, kernel_size=(1,1), padding='same')\n",
    "    elif head_strategy == 'resize':\n",
    "        image_size = gezi.get('image_size')\n",
    "        assert image_size\n",
    "        self.head_process = tf.keras.layers.Lambda(lambda x: mt.image.resize(x, image_size))\n",
    "        self.head_conv = tf.keras.layers.Conv2D(num_classes, kernel_size=(1,1), padding='same')\n",
    "    else:\n",
    "        raise ValueError(head_strategy)\n",
    "\n",
    "#     dropout = gezi.get('effdet_dropout', flags=True)\n",
    "    dropout = 0\n",
    "    if dropout:\n",
    "        self.dropout = tf.keras.layers.Dropout(float(dropout))\n",
    "    else:\n",
    "        self.dropout = None\n",
    "\n",
    "  # NotImplementedError: Layer SegmentationHead has arguments in `__init__` and therefore must override `get_config`. 如果没有get_config\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'num_classes': self.num_classes,\n",
    "        'num_filters': self.num_filters,\n",
    "        'min_level': self.min_level,\n",
    "        'max_level': self.max_level,\n",
    "        'data_format': self.data_format,\n",
    "        'is_training_bn': self.is_training_bn,\n",
    "        'act_type': self.act_type,\n",
    "        'strategy': self.strategy,\n",
    "        'head_strategy': self.head_strategy,\n",
    "        'upsampling_last': self.upsampling_last,\n",
    "        }\n",
    "    base_config = super(SegmentationHead, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "  def call(self, feats, training):\n",
    "    # print('---------', len(feats))\n",
    "    # print([(x.shape, x.name) for x in feats])\n",
    "    # print(feats)\n",
    "    x = feats[-1] # TODO dropout here like unet ?\n",
    "\n",
    "    if self.dropout is not None:\n",
    "        x = self.dropout(x)\n",
    "\n",
    "    skips = list(reversed(feats[:-1]))\n",
    "    # print([(x.shape, x.name) for x in skips])\n",
    "\n",
    "    if len(skips) < len(self.con2d_t_bns):\n",
    "      skips.append(None)\n",
    "\n",
    "    for con2d_t, con2d_t_bn, skip in zip(self.con2d_ts, self.con2d_t_bns,\n",
    "                                         skips):\n",
    "      #  print('-----input', x.shape, skip.shape if skip is not None else None)\n",
    "      if skip is not None and x.shape[1] * 2 != skip.shape[1]:\n",
    "        x = mt.image.resize(x, skip.shape[1:3])\n",
    "      else:\n",
    "        x = con2d_t(x)\n",
    "      x = con2d_t_bn(x, training)\n",
    "#       from  gseg.third.automl.efficientdet import utils\n",
    "      ## TODO下面写法save graph有点问题 其实最终实际等价标准tf.nn.swish\n",
    "      #   x = utils.activation_fn(x, self.act_type)\n",
    "      x = tf.keras.layers.Activation(self.act_type)(x)\n",
    "      if skip is not None:\n",
    "        x = tf.concat([x, skip], axis=-1)\n",
    "    #   print('------output', x.shape)\n",
    "\n",
    "    # This is the last layer of the model\n",
    "    # 如果是transpose注意 最后到128 后面还需要resize一次，如果是upsampling或者resize 这里直接resize到输出大小 *4\n",
    "\n",
    "    notop = x\n",
    "    if self.head_conv is not None and not self.upsampling_last:\n",
    "      x = self.head_conv(x)\n",
    "\n",
    "    x = self.head_process(x)  # 64x64 -> 128x128\n",
    "\n",
    "    if self.head_conv is not None and self.upsampling_last:\n",
    "      x = self.head_conv(x)\n",
    "\n",
    "    # print('------------final shape', x.shape)\n",
    "    return x, notop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-2afd27ae98fd>:9: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    }
   ],
   "source": [
    "model_path = '../working/v6/effdet/bak/model.h5'\n",
    "custom_objects = {\n",
    "                  'tf': tf, \n",
    "                  'swish_activation': swish_activation,\n",
    "                  'FixedDropout': FixedDropout,\n",
    "                  'wBiFPNAdd': wBiFPNAdd,\n",
    "                  'SegmentationHead': SegmentationHead\n",
    "                  }\n",
    "import pickle\n",
    "m = pickle.load(open(model_path, 'rb'))\n",
    "model = tf.keras.models.model_from_json(m['net'], custom_objects=custom_objects)\n",
    "model.set_weights(m['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root = '/home/featurize/data'\n",
    "sys.path.append(f'{root}/pikachu/utils')\n",
    "sys.path.append(f'{root}/pikachu/third')\n",
    "sys.path.append(f'{root}/pikachu')\n",
    "sys.path.append(f'{root}/pikachu/projects/ai/naic_seg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import gezi\n",
    "from gezi import tqdm\n",
    "from gezi.metrics.image.semantic_seg import Evaluator\n",
    "import melt as mt\n",
    "from gseg.dataset import Dataset\n",
    "from gseg.loss import get_loss_fn\n",
    "from gseg.metrics import get_metrics\n",
    "from gseg import util\n",
    "from gseg.util import *\n",
    "gezi.set_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['water', 'track_road', 'build', 'track_airport', 'other_park', 'other_playground', 'arable_natural', 'arable_greenhouse',\n",
    "           'grass_natural', 'grass_greenbelt', 'forest_natural', 'forest_planted', 'bare_natural', 'bare_planted', 'other_other']\n",
    "NUM_CLASSES = len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../input/quarter/tfrecords/train/1/1.3334.tfrec', '../input/quarter/tfrecords/train/1/11.3333.tfrec', '../input/quarter/tfrecords/train/1/21.3333.tfrec']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='get_num_records'), FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='get_num_records'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mt.init_flags()\n",
    "FLAGS = mt.get_flags()\n",
    "FLAGS.batch_parse = False\n",
    "batch_size = 32\n",
    "batch_size = 16\n",
    "# batch_size = 8\n",
    "mt.set_global('batch_size', batch_size) # loss fn used / mt.batch_size()\n",
    "eval_files = gezi.list_files('../input/quarter/tfrecords/train/1/*')\n",
    "# eval_files = gezi.list_files('../input/quarter/tfrecords/train/1/21.*')\n",
    "print(eval_files)\n",
    "eval_dataset = Dataset('valid').make_batch(batch_size, eval_files)\n",
    "train_files = gezi.list_files('../input/quarter/tfrecords/train/*/*')\n",
    "train_files = [x for x in train_files if not x in eval_files]\n",
    "train_dataset = Dataset('train').make_batch(batch_size, train_files)\n",
    "train_steps = -(-mt.get_num_records(train_files) // batch_size)\n",
    "steps = -(-mt.get_num_records(eval_files) // batch_size)\n",
    "FLAGS.NUM_CLASSES = NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model):\n",
    "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "  metrics = get_metrics()\n",
    "  model.compile('sgd', loss_fn, metrics=metrics)\n",
    "  res = model.evaluate(eval_dataset, steps=steps, return_dict=True)\n",
    "#   gezi.pprint_dict(res)\n",
    "  cm = mt.distributed.sum_merge(metrics[0].get_cm())\n",
    "  infos = util.get_infos_from_cm(cm, CLASSES)\n",
    "  res.update(infos)\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 21s 34ms/step - loss: 2.4594 - FWIoU: 0.1953 - MIoU: 0.0692 - ACC/pixel: 0.3730 - ACC/class: 0.1411\n",
      "water                |true:0.0273 pred:0.0000 acc:nan recall:0.0000 iou:0.0000 fwiou:0.5907|\n",
      "track_road           |true:0.0767 pred:0.0000 acc:0.1784 recall:0.0000 iou:0.0000 fwiou:-0.1507|\n",
      "build                |true:0.1212 pred:0.0004 acc:0.9179 recall:0.0031 iou:0.0031 fwiou:-0.8129|\n",
      "track_airport        |true:0.0001 pred:0.0000 acc:nan recall:0.0000 iou:0.0000 fwiou:0.9991|\n",
      "other_park           |true:0.0665 pred:0.0000 acc:nan recall:0.0000 iou:0.0000 fwiou:0.0029|\n",
      "other_playground     |true:0.0033 pred:0.0000 acc:nan recall:0.0000 iou:0.0000 fwiou:0.9511|\n",
      "arable_natural       |true:0.2266 pred:0.3518 acc:0.5787 recall:0.8986 iou:0.5433 fwiou:-0.5523|\n",
      "arable_greenhouse    |true:0.0129 pred:0.0000 acc:nan recall:0.0000 iou:0.0000 fwiou:0.8064|\n",
      "grass_natural        |true:0.1275 pred:0.4832 acc:0.1911 recall:0.7242 iou:0.1781 fwiou:-0.5718|\n",
      "grass_greenbelt      |true:0.0245 pred:0.0000 acc:nan recall:0.0000 iou:0.0000 fwiou:0.6324|\n",
      "forest_natural       |true:0.1564 pred:0.1645 acc:0.4660 recall:0.4902 iou:0.3139 fwiou:-0.6097|\n",
      "forest_planted       |true:0.0726 pred:0.0000 acc:nan recall:0.0000 iou:0.0000 fwiou:-0.0896|\n",
      "bare_natural         |true:0.0497 pred:0.0000 acc:nan recall:0.0000 iou:0.0000 fwiou:0.2538|\n",
      "bare_planted         |true:0.0113 pred:0.0000 acc:nan recall:0.0000 iou:0.0000 fwiou:0.8301|\n",
      "other_other          |true:0.0233 pred:0.0000 acc:nan recall:0.0000 iou:0.0000 fwiou:0.6498|\n"
     ]
    }
   ],
   "source": [
    "res = eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
