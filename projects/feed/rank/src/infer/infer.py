#!/usr/bin/env python 
# -*- coding: utf-8 -*-
# ==============================================================================
#          \file   get-all-uers.py
#        \author   chenghuige  
#          \date   2019-08-18 11:06:39.496266
#   \Description  
# ==============================================================================

  
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import sys 
import os
import collections
from collections import defaultdict
from tqdm import tqdm
import numpy as np
import random
import traceback
from multiprocessing import Manager
import pandas as pd

import melt
import gezi
logging = gezi.logging
import tensorflow as tf

# try:
#   ROOT_DIR = '/home/gezi/mine/pikachu/projects/feed/rank/src'
#   time_module = tf.load_op_library('%s/ops/time.so' % ROOT_DIR)
# except Exception:
#   pass

from absl import app, flags
FLAGS = flags.FLAGS

flags.DEFINE_bool('out_min', False, '')
flags.DEFINE_integer('batch_size_', 512, '')
flags.DEFINE_bool('loop_only', False, '')
flags.DEFINE_bool('faster', False, '')
flags.DEFINE_string('ofile', None, '')
flags.DEFINE_bool('force', False, '')

from projects.feed.rank.src.tfrecord_dataset import Dataset
from projects.feed.rank.src import util

## below nc h8 all under chgenv, using local storage not CloudS both tfrecord and model, using p100 gpu
# use cpu 1 process infer, 7it/s, 10min10s to finish
# nc infer.py tfrecords2/2020010522/ models2/15/2020010521/ --ofile=/tmp/hscores
# use 1 gpu infer, 54it/s, 1min47s
# infer.py tfrecords2/2020010522/ models2/15/2020010521/ --ofile=/tmp/hscores
# gpu infer using horovod 8 gpu, 1min18s 
# h8 infer.py tfrecords2/2020010522/ models2/15/2020010521/ --ofile=/tmp/hscores 
# cpu infer using horovod 32 processes (notice this will consume more mem), 4min30s 
# nc horovodrun -np=32 infer.py tfrecords2/2020010522/ models2/15/2020010521/ --ofile=/tmp/hscores
# for using mirrored mode non horovod but just tensorflow native distribute api, 
# seems hard and now now work(TODO just try keras api)
## you can find more log info from /tmp/melt/log.html

# Do not add any other infos anymore, for infer actaully only need mid,docid,dur,pred,pred_click,pred_dur all other infos can be found in infos/common generated by loop.py

def main(_):    
  # input tfrecords stored on Clouds need absolute path 
  in_dir = os.path.realpath(sys.argv[1])
  hour = os.path.basename(in_dir)
  files = gezi.list_files(in_dir)
  ## this is dangerous as for multi processor you need to make sure each process with same rand seed...
  # random.shuffle(files)
  num_records_file = os.path.join(in_dir, "num_records.txt")
  total = melt.get_num_records(files) 
  print('total', total, in_dir,  file=sys.stderr)

  if not total:
    exit(1)

  assert FLAGS.ofile
  df = None
  if FLAGS.ofile and gezi.non_empty(os.path.realpath(FLAGS.ofile)):
    df = pd.read_csv(FLAGS.ofile)

  FLAGS.batch_size = FLAGS.batch_size_
  batch_size = FLAGS.batch_size
  batch_size_= batch_size

  melt.init(tf.Graph())
  sess = melt.get_session()

  use_horovod = FLAGS.use_horovod
  if use_horovod:
    import horovod.tensorflow as hvd
    import mpi4py
    from mpi4py import MPI
    comm = MPI.COMM_WORLD
    batch_size_ *= hvd.size()

  predictor = None

  with sess.graph.as_default():
    # notice strategy here is just for future usage compat, now actually not need it to use horovod distribute mode
    strategy = melt.distributed.get_strategy(FLAGS.distribute_strategy, FLAGS.num_gpus)
    with strategy.scope():    
      dataset = Dataset('valid')
      num_processes = 1 if not use_horovod else hvd.size()
      rank = 0 if not use_horovod else hvd.rank()
      # if num_processes > 1:
      res = []

      iter = dataset.make_batch(batch_size=FLAGS.batch_size, filenames=files, repeat=False)
      op = iter.get_next()

      print('---batch_size', dataset.batch_size, FLAGS.batch_size, melt.batch_size(), file=sys.stderr)  

      num_steps = -int(-total // batch_size_)
      print('----num_steps', num_steps, file=sys.stderr) 
  
      desc = 'infer' if not FLAGS.loop_only else 'loop'
      m = defaultdict(list)
      for i, _ in tqdm(enumerate(range(num_steps)), total=num_steps, ascii=True, desc=desc):
        x, y = sess.run(op)
        
        if 'product' in x:
          product = x['product']
          x['product_data'] = x['product']
          del x['product']
        else:
          product = np.asarray(['sgsapp'] * len(mids))
          x['product_data'] = product
          
        x2 = {}
        bs = len(x['id'])
        keys = list(x.keys())
        for key in keys:
          if not len(x[key]):
            continue
          if key in ["tw_history","tw_history_topic","tw_history_rec","tw_history_kw","vd_history","vd_history_topic","doc_keyword","doc_topic"]:
            continue
          # mkyuwen
          if "mktest" in key and "_kw_feed" in key:
            continue
          if x[key].shape == (bs, 1):
            x[key] = gezi.squeeze(x[key])
          if x[key].shape != (bs,):
            continue
          if x[key].dtype == np.object:
            x[key] = gezi.decode(x[key])
          x2[key] = x[key]
          m[key] += [x2[key]]
        
        if i == 0:
          if not FLAGS.force and df is not None and len(df) == total \
              and set(list(m.keys()) + ['hour', 'pred', 'pred_click', 'pred_dur']) == set(list(df.columns)):
            print(f'infos file {FLAGS.ofile} exits do nothing', file=sys.stderr)
            exit(0)
          if not FLAGS.loop_only:
            try:
              model_path = os.path.realpath(sys.argv[2])
              # TODO using pb py and c++ not ok if using hash table, but since we will change to use checkpoint will not test anymore
              # using checkpoint py ok for hash table, have not tested c++ yet
              predictor = melt.Predictor(model_path, sess=sess)   
              assert predictor.graph.get_collection('index_feed') 
            except Exception:
              print(traceback.format_exc(), file=sys.stderr)
              print('Turn to use loop only mode, with out prediction', file=sys.stderr)
              predictor = None
              FLAGS.loop_only = True
        
        index = x['index']
        value = x['value']
        field = x['field']
        mids = x['mid']
        docids = x['docid']
        uid = x['uid']
        did = x['did']
        history = x['history']
        product = x['product_data']
        def to_product_id(x):
          if x == 'sgsapp':
            return 0
          elif x == 'newmse':
            return 1 
          elif x == 'shida':
            return 2
          else:
            return 0
        product_ = np.asarray([to_product_id(x) for x in product])

        if not FLAGS.loop_only:
          index, value, field = x['index'], x['value'], x['field']
          assert len(predictor.graph.get_collection('index_feed')) == 1
          feed_dict = {
                        predictor.graph.get_collection('index_feed')[-1]: index,
                        predictor.graph.get_collection('value_feed')[-1]: value,
                        predictor.graph.get_collection('field_feed')[-1]: field,
                        predictor.graph.get_collection('uid_feed')[-1]: uid.reshape(-1, 1),
                        predictor.graph.get_collection('did_feed')[-1]: did.reshape(-1, 1),
                        #predictor.graph.get_collection('doc_idx_feed')[-1]: history,
                      } 

          try:
            feed_dict.update({
                        predictor.graph.get_collection('time_interval_feed')[-1]: x['time_interval'].reshape(-1, 1),
                        predictor.graph.get_collection('time_weekday_feed')[-1]: x['time_weekday'].reshape(-1, 1),
                        predictor.graph.get_collection('timespan_interval_feed')[-1]: x['timespan_interval'].reshape(-1, 1),             
                        predictor.graph.get_collection('product_feed')[-1]: product_.reshape(-1, 1),
                        predictor.graph.get_collection('doc_kw_idx_feed')[-1]: x['doc_keyword'],
                        predictor.graph.get_collection('doc_topic_idx_feed')[-1]: x['doc_topic'].reshape(-1,1),
                        predictor.graph.get_collection('tw_history_feed')[-1]: x['tw_history'],
                        predictor.graph.get_collection('tw_history_topic_feed')[-1]: x['tw_history_topic'],
                        predictor.graph.get_collection('tw_history_rec_feed')[-1]: x['tw_history_rec'],
                        predictor.graph.get_collection('tw_history_kw_feed')[-1]: x['tw_history_kw'],
                        predictor.graph.get_collection('vd_history_feed')[-1]: x['vd_history'],
                        predictor.graph.get_collection('vd_history_topic_feed')[-1]: x['vd_history_topic'],
                        predictor.graph.get_collection('user_active_feed')[-1]: x['user_active'].reshape(-1,1),
                        predictor.graph.get_collection('rea_feed')[-1]: x['rea'].astype(int).reshape(-1,1)
              })
          except Exception:
            print("mktest infer error1111111", traceback.format_exc(), file=sys.stderr)
            # print(traceback.format_exc(), file=sys.stderr)
            pass

          try:
            feed_dict.update({
                        predictor.graph.get_collection('mktest_distribution_id_feed')[-1]: x['mktest_distribution_id_feed'].reshape(-1, 1),

                        predictor.graph.get_collection('mktest_tw_history_kw_feed')[-1]: x['mktest_tw_history_kw_feed'],
                        predictor.graph.get_collection('mktest_vd_history_kw_feed')[-1]: x['mktest_vd_history_kw_feed'],
                        predictor.graph.get_collection('mktest_rel_vd_history_kw_feed')[-1]: x['mktest_rel_vd_history_kw_feed'],
                        predictor.graph.get_collection('mktest_tw_long_term_kw_feed')[-1]: x['mktest_tw_long_term_kw_feed'],
                        predictor.graph.get_collection('mktest_vd_long_term_kw_feed')[-1]: x['mktest_vd_long_term_kw_feed'],
                        predictor.graph.get_collection('mktest_long_search_kw_feed')[-1]: x['mktest_long_search_kw_feed'],
                        predictor.graph.get_collection('mktest_new_search_kw_feed')[-1]: x['mktest_new_search_kw_feed'],
                        predictor.graph.get_collection('mktest_user_kw_feed')[-1]: x['mktest_user_kw_feed'],

                        predictor.graph.get_collection('mktest_doc_kw_feed')[-1]: x['mktest_doc_kw_feed'],
                        predictor.graph.get_collection('mktest_doc_kw_secondary_feed')[-1]: x['mktest_doc_kw_secondary_feed'],
              })
          except Exception:
            print("mktest infer error", traceback.format_exc(), file=sys.stderr)
            pass
          
          # gezi.sprint(feed_dict)
          
          try:
            preds, preds_click, preds_dur = predictor.predict(['pred', 'pred_click', 'pred_dur'], feed_dict)
          except Exception:
            print("mktest error 3 preds not all in predictor",traceback.format_exc(), file=sys.stderr)
            preds = predictor.predict('pred', feed_dict)
            preds_click, preds_dur = preds, preds
        else:
          preds, preds_click, preds_dur = np.zeros_like(lr_scores), np.zeros_like(lr_scores), np.zeros_like(lr_scores)
        
        m['pred'] += [gezi.squeeze(preds)]
        m['pred_click'] += [gezi.squeeze(preds_click)]
        m['pred_dur'] += [gezi.squeeze(preds_dur)]
        # print(predictor.predict(['dummy', 'index_feed', 'pred'], feed_dict, return_dict=True))

      if num_processes > 1:
        comm.barrier()
        res = np.asarray(list(res))
        res = comm.allgather(res)
        res = np.concatenate(res)
        comm.barrier()
      
      if rank == 0:
        for key in m.keys():
          m[key] = np.concatenate(m[key], 0)
        
        keys = list(m.keys())
        for key in keys:
          if len(m[key]) != len(m['id']):
            del m[key]
    
        df = pd.DataFrame(m)
        df['hour'] = hour
        print(len(df), len(set(df.id.values)))
        print('write csv', os.path.realpath(FLAGS.ofile), file=sys.stderr)
        df.to_csv(os.path.realpath(FLAGS.ofile), index=False)

if __name__ == '__main__':
  app.run(main)
  
