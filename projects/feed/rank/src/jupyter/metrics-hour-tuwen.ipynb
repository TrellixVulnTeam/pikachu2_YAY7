{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from plotly.graph_objs import Scatter,Layout\n",
    "import plotly\n",
    "import plotly.offline as py\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "import gezi\n",
    "from gezi import tqdm, line\n",
    "import pymp\n",
    "from multiprocessing import Pool, Manager, cpu_count \n",
    "import qgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_wrapper, .output {\n",
       "    height:auto !important;\n",
       "    max-height:50000px;  /* your desired max-height here */\n",
       "}\n",
       ".output_scroll {\n",
       "    box-shadow:none !important;\n",
       "    webkit-box-shadow:none !important;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:50000px;  /* your desired max-height here */\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark = 'tuwen'\n",
    "root = f'/home/gezi/tmp/rank/exps/tuwen/v26'\n",
    "root = f'/home/gezi/tmp/rank/exps/tuwen/v27'\n",
    "# root = f'/home/gezi/tmp/rank/exps/{mark}/monitor'\n",
    "base_dir = f'/search/odin/publicData/CloudS/libowei/rank_online/infos/{mark}/8'\n",
    "# base_dir = f'/search/odin/publicData/CloudS/rank/infos/{mark}/16.new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_df(mark):\n",
    "  dfs_offline = Manager().list()\n",
    "  dfs_online = Manager().list()\n",
    "  files = glob.glob(f'{base_dir}/*/sgsapp_metrics_offline_impression.csv')\n",
    "#   files = sorted(files, key=lambda x: os.path.getmtime(x))[:100]\n",
    "  ps = min(len(files), cpu_count())\n",
    "  with pymp.Parallel(ps) as p:\n",
    "    for i in tqdm(p.range(len(files)),desc='offline'):\n",
    "      file = files[i]\n",
    "      if not gezi.non_empty(file):\n",
    "        continue\n",
    "      df = pd.read_csv(file)\n",
    "      dfs_offline.append(df)\n",
    "  files = glob.glob(f'{base_dir}/*/sgsapp_metrics_online_impression.csv')\n",
    "  files = sorted(files, key=lambda x: os.path.getmtime(x))[:100]\n",
    "  ps = min(len(files), cpu_count())\n",
    "  with pymp.Parallel(ps) as p:\n",
    "    for i in tqdm(p.range(len(files)), desc='online'):\n",
    "      file = files[i]\n",
    "      if not gezi.non_empty(file):\n",
    "        continue\n",
    "      df = pd.read_csv(file)\n",
    "      dfs_online.append(df)\n",
    "  df_off = pd.concat(list(dfs_offline))\n",
    "  df_on = pd.concat(list(dfs_online))\n",
    "  return df_off, df_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "offline: 100%|██████████| 19/19 [00:03<00:00,  5.44it/s]\n",
      "offline: 100%|██████████| 19/19 [00:03<00:00,  5.15it/s]\n",
      "offline:  84%|████████▍ | 16/19 [00:02<00:00,  4.39it/s]\n",
      "offline:  84%|████████▍ | 16/19 [00:02<00:00,  5.27it/s]\n",
      "offline: 100%|██████████| 19/19 [00:03<00:00,  5.24it/s]\n",
      "offline:  89%|████████▉ | 17/19 [00:02<00:00,  5.12it/s]\n",
      "offline: 100%|██████████| 18/18 [00:02<00:00,  5.57it/s]\n",
      "offline:  84%|████████▍ | 16/19 [00:03<00:00,  4.86it/s]\n",
      "offline:  95%|█████████▍| 18/19 [00:03<00:00,  5.80it/s]\n",
      "offline: 100%|██████████| 19/19 [00:03<00:00,  5.55it/s]\n",
      "offline: 100%|██████████| 19/19 [00:03<00:00,  4.37it/s]\n",
      "\n",
      "offline: 100%|██████████| 19/19 [00:03<00:00,  5.12it/s]\n",
      "offline: 100%|██████████| 19/19 [00:03<00:00,  4.57it/s]\n",
      "offline: 100%|██████████| 19/19 [00:03<00:00,  5.50it/s]\n",
      "\n",
      "\n",
      "offline: 100%|██████████| 19/19 [00:03<00:00,  5.35it/s]\n",
      "offline: 100%|██████████| 19/19 [00:03<00:00,  5.85it/s]\n",
      "offline: 100%|██████████| 19/19 [00:03<00:00,  5.47it/s]\n",
      "offline:  47%|████▋     | 9/19 [00:03<00:02,  4.06it/s]]\n",
      "offline: 100%|██████████| 19/19 [00:03<00:00,  4.50it/s]\n",
      "offline: 100%|██████████| 19/19 [00:03<00:00,  4.72it/s]\n",
      "offline:  83%|████████▎ | 15/18 [00:03<00:00,  4.18it/s]"
     ]
    }
   ],
   "source": [
    "df_base_off, df_base_on = gen_df(mark)\n",
    "df_base_off = df_base_off[df_base_off.abtest==45600]\n",
    "df_base_off = df_base_off.groupby(df_base_off.hour, as_index=False).last()\n",
    "# df_base_on = df_base_on[df_base_on.abtest==456]\n",
    "# df_base_on = df_base_on.groupby(df_base_on.hour, as_index=False).last()\n",
    "# df_base_off['hour'] = df_base_off['hour'].astype(int)\n",
    "# df_base_on['hour'] = df_base_on['hour'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_off.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "m = {}\n",
    "start_hour = 1e32\n",
    "end_hour = -1\n",
    "base_records = []\n",
    "for dir in tqdm(glob.glob(f'{root}/*')):\n",
    "  if os.path.isdir(dir):\n",
    "    try:\n",
    "      df = pd.read_csv(f'{dir}/metric_hours.csv')\n",
    "      df = df[~df.hour.isnull()]\n",
    "      df['hour'] = df['hour'].astype(int)\n",
    "      df = df.groupby(df.hour, as_index=False).last().sort_values(by=['hour'])\n",
    "      df = df.replace(0., np.NaN)\n",
    "      name = os.path.basename(dir)\n",
    "      name = name.replace('deeponly-', '')\n",
    "      df['model'] = name\n",
    "      df.name = name\n",
    "      m[name] = df\n",
    "      if 'timestamp' in df.columns:\n",
    "        timestamps = df['timestamp'].values\n",
    "        df['duration'] = [0, *((timestamps[1:] - timestamps[:-1]) / 60)]\n",
    "      if len(df):\n",
    "        dfs.append(df)\n",
    "      else:\n",
    "        print('empty df:', f'{dir}/metric_hours.csv')\n",
    "      if df.hour.max() > end_hour:\n",
    "        end_hour = df.hour.max()\n",
    "      if df.hour.min() < start_hour:\n",
    "        start_hour = df.hour.min()\n",
    "      try:\n",
    "        df_record = pd.read_csv(f'{dir}/base_metric_hours.csv')\n",
    "        df_record = df_record.replace(0., np.NaN)\n",
    "        base_records.append(df_record)\n",
    "      except Exception:\n",
    "        pass\n",
    "    except Exception:\n",
    "#       print(traceback.format_exc())\n",
    "      continue\n",
    "         \n",
    "if len(base_records):\n",
    "  df_base_records = pd.concat(base_records)\n",
    "  df_base_records = df_base_records.groupby(df_base_records.hour, as_index=False).last().sort_values(by=['hour'])\n",
    "  name = 'base'\n",
    "  df_base_records['model'] = name\n",
    "  df_base_records.name = name\n",
    "  dfs.append(df_base_records)\n",
    "  m[name] = df_base_records\n",
    "    \n",
    "df_base_off['hour'] = df_base_off['hour'].astype(int)\n",
    "# df_base_on['hour'] = df_base_on['hour'].astype(int)\n",
    "\n",
    "# df_base_off_ = df_base_off[df_base_off.hour >= int(start_hour)][df_base_off.hour <= int(end_hour)]\n",
    "\n",
    "# df_base_ = df_base[df_base.hour >= start_hour]\n",
    "if len(df_base_off):\n",
    "  name = 'baseline.off'\n",
    "  df_base_off['model'] = name\n",
    "  df_base_off.name = name\n",
    "  dfs.append(df_base_off)\n",
    "  m[name] = df_base_off\n",
    "  \n",
    "# df_base_on_ = df_base_on[df_base_on.hour >= int(start_hour)][df_base_on.hour <= int(end_hour)]\n",
    "# # df_base_ = df_base[df_base.hour >= start_hour]\n",
    "# if len(df_base_on_):\n",
    "#   name = 'baseline.on'\n",
    "#   df_base_on['model'] = name\n",
    "#   df_base_on.name = name\n",
    "#   dfs.append(df_base_on)\n",
    "#   m[name] = df_base_on\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "  df = dfs[i]\n",
    "  name = df.name\n",
    "#   print(i, name)\n",
    "  df['sqrt/auc'] = (df['auc'] * df['click/time_auc']) ** 0.5\n",
    "  df['group/sqrt/auc'] =  (df['group/auc'] * df['group/click/time_auc']) ** 0.5\n",
    "  try:\n",
    "    df['rmse'] = 1. - df['mse'] ** 0.5\n",
    "    df['click/rmse'] = 1. - df['click/mse'] ** 0.5\n",
    "    df['Dur/click/rmse'] = 1. - df['Dur/click/mse'] ** 0.5\n",
    "  except Exception:\n",
    "    pass\n",
    "  dfs[i] = df\n",
    "  m[name] = df\n",
    "  \n",
    "models = m.keys()\n",
    "\n",
    "# models = [\n",
    "#         'dlrm2', 'baseline', 'baseline.off'\n",
    "#          ]\n",
    "\n",
    "# print(models)\n",
    "\n",
    "def is_ok(key):\n",
    "#   if 'baseline' in key or 'trans' in key or key == 'dlrm' or key == 'concat':\n",
    "#     return True\n",
    "#   return False\n",
    "  return True\n",
    "\n",
    "dfs_ = [m[key][m[key].hour>=2020050100] for key in models if key in m.keys() if is_ok(key)]\n",
    "\n",
    "# x = ['fm.finetune', 'old']\n",
    "# dfs_ = [m[key] for key in x] + [m[key] for key in models if key not in x]\n",
    "# # print(m.keys())\n",
    "\n",
    "# group/auc is gauc using impression as weight, group/auc2 not using weight, just mean auc with / num_users\n",
    "# so group/auc2 might more realted to read_ratio as it treat each user equally, hight group/auc2 means better\n",
    "# performance on users with less impressions(and views)\n",
    "metric_names = [\n",
    "  'stats/num_instances',  \n",
    "  'Click/group/auc',\n",
    "#   'Click/group/auc2',\n",
    "  'Dur/group/click/time_auc',\n",
    "  'group/auc',\n",
    "  'group/click/time_auc',\n",
    "  'Click/cold/group/auc'\n",
    "#   'Dur/group/click/ndcg_dur',\n",
    "#   'Dur/click/rmse',\n",
    "#   'group/sqrt/auc',\n",
    "#    'group/auc2', 'cold/group/auc', 'cold/group/auc2', 'quality/group/auc', 'quality/group/auc2',\n",
    "#   'group/click/time_auc', 'cold/group/click/time_auc', 'quality/group/click/time_auc', 'click/rmse', 'rmse',\n",
    "#   #   'sqrt/auc', 'auc', 'clickmids/auc', 'cold/auc', 'quality/auc',  \n",
    "#   #   'click/time_auc', 'cold/click/time_auc', 'quality/click/time_auc',\n",
    "#   'rcr_auc',\n",
    "#   'ptime_auc',\n",
    "# #   'Dur/click/time_auc',\n",
    "#   'loss/click',\n",
    "#   'loss/dur',\n",
    "#   'inv_rate',\n",
    "#   'stats/num_instances',\n",
    "]\n",
    "# metric_names += [x for x in dfs_[0].columns if 'loss' in x or 'mse' in x or 'mae' in x]\n",
    "# show(dfs_, metric_names, smoothing=0.5, limit=168, exclude='Click,Dur')\n",
    "\n",
    "focus = True # focus means focus by the first model\n",
    "# focus = False\n",
    "# focus_name = 'transformer'\n",
    "focus_name = 'baseline'\n",
    "smoothing= 0.5\n",
    "line(dfs_, x='hour', y=metric_names, smoothing=smoothing, focus=focus, focus_name=focus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    "  'group/top1_click', #precision@1\n",
    "  'group/top3_click', #precision@3\n",
    "  'group/top1_dur', #top1 position mean duration\n",
    "  'group/top3_dur', #top3 position mean duration\n",
    "  'group/click/top1_dur', \n",
    "  'group/click/top3_dur',\n",
    "]\n",
    "line(dfs_, 'hour', metrics, smoothing=smoothing, focus=focus, focus_name=focus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    "  'group/first_click_position',\n",
    "  'group/last_click_position',\n",
    "  'group/ndcg_click',   # ndcg@all for click\n",
    "  'group/ndcg3_click',  # ndcg@3 for click (using 0-1 label)\n",
    "  'group/ndcg3_dur',    # ndcg@3 using duration as label\n",
    "  'group/ndcg_dur',\n",
    "#   'group/ndcg7_click',\n",
    "#   'group/ndcg14_click',\n",
    "  'group/click/ndcg_dur',\n",
    "  'group/click/ndcg3_dur',\n",
    "#   'group/click/ndcg7_dur',\n",
    "#   'group/click/ndcg14_dur',\n",
    "  'quality/group/ndcg_dur',\n",
    "  'quality/group/click/ndcg_dur',\n",
    "  'quality/group/click/ndcg3_dur',\n",
    "]\n",
    "line(dfs_, 'hour', metrics, smoothing=smoothing, focus=focus, focus_name=focus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = [\n",
    "#   'duration',\n",
    "#   'perf/total_time',\n",
    "#   'perf/train_time',\n",
    "#   'perf/valid_time',\n",
    "# ]\n",
    "# dfs_ = [m[key] for key in models if key in m.keys() if is_ok(key) and 'records' not in key]\n",
    "# line(dfs_, 'hour', metrics, smoothing=smoothing, focus=focus, focus_name=focus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
